<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="author" content="Bugra Akyildiz">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">
        <title>8th NYAS Machine Learning Symposium 2014 | Bugra Akyildiz</title>

        <link rel="alternate" type="application/atom+xml" title="Bugra Akyildiz blog atom feed" href="/feeds/all.atom.xml" />
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

    </head>

  <body>
      <div id="logo"><a href="/" class="at"></a></div>
      <div id="mln-io"><a href="http://www.mln.io/">Machine Learning Newsletter</a></div>

  <div id="content">
    <div class="post">
      <div class="post-content">
<div id="fb-root"></div>
<script>(function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
      if (d.getElementById(id)) return;
        js = d.createElement(s); js.id = id;
          js.src = "//connect.facebook.net/en_US/all.js#xfbml=1";
            fjs.parentNode.insertBefore(js, fjs);
            }(document, 'script', 'facebook-jssdk'));
</script>
<div class="post">
    <article>
        <header>
            <h1>8th NYAS Machine Learning Symposium 2014</h1>
        </header>
        <div class='post-content'>
            <p>I attended to <a href="http://www.nyas.org/Events/Detail.aspx?cid=2cc3521e-408a-460e-b159-e774734bcbea">NYAS 8th Machine Learning Symposium</a> and here are the notes
that I took from the event. It may contain errors and mistakes. If you
find any, please let me  know.<br />
On personal view, it was worse than the previous machine learning
symposium(7th) in both posters and also talks. Last year, the posters and
talks were much more interesting to me. That being said, I could not
visit all of the posters so take my word with a grain of salt.  <br />
The <a href="http://www.nyas.org/asset.axd?id=defb6b86-f7ad-4a8d-af2a-3a5ef979c143&amp;t=635314462602300000">abstracts in pdf</a> in here. </p>
<h2>Machine Learning for Powers of Good</h2>
<p>by <a href="http://www.rayidghani.com/">Rayid Ghani</a></p>
<ul>
<li>Probability of optimization of limited resources for a campaign. </li>
<li>Another important thing: to influence the behavior of the voter and
  how to make them engage with the campaign.</li>
<li>Because prediction itself is not good enough. </li>
<li>Resource allocation based on who are likely to be influenced. Who are
  likely to change their mind? Definitely not Texas.</li>
<li>Not voting to Romney and not going to vote, too much work. Do not try
  to event attempt to do anything.</li>
<li>Focus on the ones either who are indecisive but likely to vote or
  indecisive about voting but weakly support Obama.</li>
<li><a href="http://dssg.io/">Data Science for Social Good</a></li>
</ul>
<h2>Following Spotlight Talks</h2>
<h3>Graph-Based Posterior Regularization for Semi-Supervised Structured Prediction:</h3>
<ul>
<li>Posterior labels for part of speech tags. Graph-based approach,
  using Laplacian of Graph</li>
<li>Structured Prediction =&gt;  CRF =&gt; local scope, features</li>
</ul>
<p>Graph-propagation and CRF estimation =&gt; Joint objective, then to
optimize and look at the KL divergence as well for both world parameters.</p>
<p>Relevant work is <a href="http://jmlr.org/papers/volume11/ganchev10a/ganchev10a.pdf">Posterior Regularization(PR) Linear Ganchev</a></p>
<ul>
<li>EM like algorithm =&gt; to converge to the local optimum </li>
</ul>
<p>She showed that it performs better than both CRF and Graph based approaches in her poster, but she did not compare speed of this approach with CRF or Graph based approaches. It is likely the method is slower than CRF but, I am not very familiar Graph based approaches and joint objective could be quite hard to optimize. So, the speed is could be much worse more than 2 times than CRF.</p>
<h3>Learning from Label Proportions(LLP):</h3>
<p>It attacks Binary learning problem with an extension of bag approach
where bags represent the ratio of the labels that are known but
individual labels are unknown. They try to solve the problem in a large
margin framework trying to model the instances belonging to a particular
label and try to increase margin with the other label(smv-like).
- Extension of supervised learning objective with Bag Proportion Loss
  with model parameters with a proportion loss.</p>
<h5>Generalization Error of LLP</h5>
<ul>
<li>Sample complexity of learning is proportional to bag proportion</li>
<li>Instance label prediction error =&gt; again depends on the prediction
  error</li>
<li>Not only the supervised learning objective but also the bag
  proportions for the labeling matters.</li>
</ul>
<h3>Generative Image Models For Visual Phenotype Modeling</h3>
<p>They have genome types of fish and they have features of the fish. In
order to learn which genome type has effect on which fish trait, they
propose an admixture model which tries to correlate the traits and
genome. </p>
<ul>
<li>Admixture model to correlate the shape variance between geneology of the
fishes.   </li>
<li>Annotated genome from the shape variance of the fish.</li>
<li>Genome annotates the features of the shape variance of the fish.</li>
<li>Unsupervised learning of the features and joint generative model from
  fish variance and genome change. =&gt; Seems quite novel.</li>
</ul>
<h3>Large Scale Learning - Scaling Graph-based semi supervised-learning</h3>
<ul>
<li>Replace label vectors =&gt; count-min-sketch? is a data
  structure(randomized) stores the counts of items. </li>
<li>MAD exact vs Mad-Sketch =&gt; comparison</li>
<li><a href="https://github.com/parthatalukdar/junto">Junto Toolkit @ Github</a> </li>
</ul>
<h2>Structured Classification Criteria for Deep Learning for Speech Recognition (Second Keynote)</h2>
<p>Before this talk, I knew that IBM is strong in deep learning(if I recall
correctly, they had a poster last year for speech recognition)  but
I did not know that they published also strong papers for speech
recognition last year. Google and Facebook get a lot of coverage for deep learning 
and that maybe rightly so, but IBM is also a strong player in the area.</p>
<h3>Talk Structure</h3>
<ul>
<li>Acoustic Modeling for speech recoognition:</li>
<li>Structured loss function: we do not care about the loss function per
  se but how audible it is in the speech. Therefore, the loss function
should be structured around audibility of the speech.</li>
<li>Optimization</li>
<li>Speeding for training</li>
</ul>
<h4>Bayesian Modeling for Speech Recognition</h4>
<p>Sequence of phones are nice because  if have a word to
classify and you did not have that sample in the training set, you could "guess" the word from the sequence of phones.</p>
<blockquote>
<p>Context affects the acoustic realization of a phone in the speech.</p>
</blockquote>
<h4>Context-dependent modeling</h4>
<ul>
<li>Condition on adjacent phones, infer the xcontext.</li>
<li>Parameter sharing needed to mitigate data sparsity
 sahring must generalize to unseen contexts.</li>
<li>Decision tree to get the AA-b, nasal, retroflex, fricative?, too much
  hand-engineered features.</li>
<li>Basic speech sounds =&gt; 1000 to 64K</li>
</ul>
<h4>Structured Loss Functions</h4>
<ul>
<li>Cross-entropy, for training criterion.</li>
<li>A neural network with training error with the cross-entropy.</li>
<li>We do not care about individual phones error but word error.</li>
<li><a href="http://en.wikipedia.org/wiki/Cross_entropy">Cross-Entropy</a></li>
<li><a href="http://mi.eng.cam.ac.uk/~wjb31/ppubs/hlt04_mbr_smt.pdf">Bayes-Risk Losses</a></li>
<li>Hamming distance is a <strong>forgiving</strong> distance measure for error between
  HMM sequences.</li>
<li>To represent reference space, lattices generated via constrained
  recognition.</li>
</ul>
<h5>Stochastic Gradient Optimization</h5>
<ul>
<li>Training GMM with is the fundamental idea.<a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=4960445&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4960445">Stochastic Gradient Optimization</a></li>
<li>Second Order optimization =&gt; well researched</li>
<li>Linear conjugate gradient minimizes a quadratic, which can be
  described by only matrix-vector products. Only linear time and memory are
necessary. </li>
<li>CG is not necessary, truncated Newton is good enough.</li>
<li><a href="http://www.researchgate.net/publication/228619939_The_IBM_Attila_speech_recognition_toolkit">Hessian Free Optimization</a></li>
<li><a href="http://research.microsoft.com/pubs/209355/NOW-Book-Revised-Feb2014-online.pdf">It has reference in this book,too.</a> =&gt; most probably, this may explain better.</li>
</ul>
<h3>Speeding Training</h3>
<ul>
<li><a href="https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester1201314/Sainath2013_lrdnn.pdf">Low-rank factorization of output weights</a></li>
<li>Word error rate generally drops as we increase the number of output
  targets =&gt; factorization to reduce dimension?</li>
<li>It gets faster.</li>
</ul>
<h4>Preconditioning in Sampling</h4>
<ul>
<li><a href="http://arxiv.org/pdf/1309.1508v3.pdf">Accelerating Hessian Free Optimization Implicit Preconditioning and Sampling</a></li>
<li>Geometric optimization reference is also in the above link.</li>
</ul>
<h3>Take-Home Mesages</h3>
<ul>
<li>A Structured loss function instead of cross-entropy</li>
<li>Stochastic gradient on a GPU is faster but distributed Hessian free
  optimization produces better models.</li>
<li>Low rank factorization of the output weights</li>
<li>Preconditioning and sampling</li>
</ul>
<p><a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_Martens10.pdf">Hessian-Free Optimization</a></p>
<h3>Learning Guarantees of the Optimization</h3>
<ul>
<li>Convex surrogate did not work</li>
<li>You could learn an interesting loss function.</li>
<li><a href="https://docs.google.com/presentation/d/1fZjE2aNMEoXprlvju_CrJETPylKrWZ5GzBYCj-GFSQc/edit#slide=id.g40599a40022e2fc114">Related
  Slides</a>,
I could not catch the presenter in the poster.</li>
</ul>
<h3>Large Scale Machine Learning(Accelerated)</h3>
<ul>
<li>Machine learning as an optimization problem.</li>
<li><a href="http://yaroslavvb.blogspot.com/2014/03/stochastic-gradient-methods-2014.html">Stochastic Gradient Methods</a></li>
<li><a href="http://mrtz.org/blog/the-zen-of-gradient-descent/">Gradient Descent Method Blog Post</a></li>
</ul>
<h3>Fast Scalable Comment Moderation on NYT</h3>
<h4>Active Learning at the New York Times</h4>
<ul>
<li>Comment is split into two: metadata and n-grams.</li>
<li>Hash the n-grams, score of the comment for human editor if it will be
  shown to her. Human moderators will work on whatever the algorithm
scores on high.</li>
<li>20% =&gt; workload reduction according to the plan</li>
</ul>
<h2>Role of Optimization in Machine Learning</h2>
<h3>Key Observations</h3>
<ul>
<li>Stochastic Gradient Descent in the origin</li>
<li>Batch Gradient Descent in one direction (between semi-stochatic
  approaches)</li>
<li>Stochastic Newton Method in the other direction (between second-order
  methods)</li>
<li>Other fields in Newton method</li>
<li>Coordinate descent is towards simpler methods</li>
</ul>
<blockquote>
<p>Optimal rate is achieved also for testing cos as long as each data
point is seen only once.</p>
</blockquote>
<h4>To learn More about Stochastic Gradient Descent</h4>
<ul>
<li>
<p><a href="http://www.eecs.berkeley.edu/~brecht/cs294docs/week1/09.Nemirovski.pdf">Nice paper for stochastic gradient methods</a></p>
</li>
<li>
<p>Stochastic Quasi-Newton Method performs quite well.</p>
</li>
</ul>
<p>Sorry about other spotlight talks, I am sure they were as interesting as
the ones above but I was only able to take notes and follow this much. </p>
        </div>
    </article>
          <ul class="social-media-links" style="text-align: center;">
            <li class="twitter" style="display:inline;">
            <a href="https://twitter.com/share" i
              class="twitter-share-button"
              data-url="http://bugra.github.io/work/notes/2014-03-28/nyas-machine-learning-symposium-2014/"
              data-via="bugraa">Tweet</a>
              <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
              </li>
           <li class="fb-like" style="display:inline;">
              <div class="fb-share-button" data-href="http://developers.facebook.com/docs/plugins/" data-width="32" data-type="button_count"></div>
           </li>
           <li class="google-plus" style="display:inline;">
             <!-- Place this tag where you want the +1 button to render. -->
             <div class="g-plusone" data-size="medium"></div>

             <!-- Place this tag after the last +1 button tag. -->
             <script type="text/javascript">
                 (function() {
                       var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
                           po.src = 'https://apis.google.com/js/platform.js';
                               var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
                                 })();
             </script>
           </li>
             <li class="pinterest" style="display:inline;">
               <a href="//www.pinterest.com/pin/create/button/" data-pin-do="buttonBookmark"  data-pin-shape="round" data-pin-height="32"><img src="//assets.pinterest.com/images/pidgets/pinit_fg_en_round_red_32.png" /></a><!-- Please call pinit.js only once per page --><script type="text/javascript" async src="//assets.pinterest.com/js/pinit.js"></script>
               </a>
             </li>
           </li>
         </ul>
    <div class="metadata">

        <div class="left">
            <ul>
                <li>
                    <strong>Published: </strong>
                    <i>
                        <time datetime="2014-03-28T00:00:00">
                            March 28, 2014
                        </time>
                    </i>
                </li>
                </li>
            </ul>
        </div>

        <div class="right">
            <ul>
                <li><a href="/">Home</a></li>
            </ul>
        </div>

    </div>

</div>

    <div id="disqus_thread"></div>
      <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'bugra'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
      </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

      </div>
    </div>
</div>

    <link href='http://fonts.googleapis.com/css?family=Merriweather:400,300,700' rel='stylesheet' type='text/css'>

        <link rel="stylesheet" href="http://yui.yahooapis.com/pure/0.3.0/base-min.css">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" href="/theme/css/pygments.css">
        <link rel="stylesheet" href="/theme/css/main.css">
        <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/favicon.ico" type="image/x-icon">

      <!-- Timesheet JS and CSS -->
      <script src="/theme/js/timesheet.js"></script>
      <script src="/theme/js/timesheet.bubble.js"></script>
      <script type='text/javascript'>new Timesheet('timesheet', 2008, 2020, [
  ['09/2006', '05/2011', 'B.S Electrical Engineering at Bilkent University', 'dolor'],
  ['06/2009', '09/2009', 'Software Developer Intern at Optisis Inc.', 'dolor'],
  ['06/2010', '09/2010', 'Software Developer Intern at Tubitak R&D', 'dolor'],
  ['06/2011', '09/2011', 'Research Internship at Koc University', 'dolor'],
  ['09/2011', '05/2013', 'M.S Electrical and Computer Engineering at NYU', 'lorem'],
  ['12/2012', '05/2013', 'Part-Time Software Developer at Viacom', 'lorem'],
  ['05/2013', '12/2013', 'Algorithms Engineer at dMetrics Inc.', 'ipsum'],
  ['12/2013', '12/2014', 'Machine Learning Engineer at CB Insights', 'ipsum'],
  ['12/2014', '2/2015', 'Data Scientist at Axial', 'ipsum'],
  ['03/2014', '2/2015', 'Consultant in Text and Time Series Analysis', 'default']
]);</script>
      <!-- Timesheet End -->
      <script src="/theme/js/logo.js"></script>
      <script src="http://d3js.org/d3.v3.min.js"></script>
      <script src="/theme/js/scramble.js"></script>
      <script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
      <script type="text/javascript">init();</script>
      <script>
        emailScramble = new scrambledString(document.getElementById('email'), 'emailScramble', 'dun@euuygbar.', [12, 9, 7, 6, 11, 13, 2, 8, 3, 1, 5, 4, 10]);
      </script>

<script type="text/javascript">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-32814563-1']);
_gaq.push(['_trackPageview']);

(function() {
var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>

    </body>
</html>