<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="author" content="Bugra Akyildiz">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">
        <title>Topic Modeling for the Uninitiated | Bugra Akyildiz</title>

        <link rel="alternate" type="application/atom+xml" title="Bugra Akyildiz blog atom feed" href="/feeds/all.atom.xml" />
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

    </head>

  <body>
      <div id="logo"><a href="/" class="at"></a></div>
      <div id="mln-io"><a href="http://www.mln.io/">Machine Learning Newsletter</a></div>

  <div id="content">
    <div class="post">
      <div class="post-content">
<div id="fb-root"></div>
<script>(function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
      if (d.getElementById(id)) return;
        js = d.createElement(s); js.id = id;
          js.src = "//connect.facebook.net/en_US/all.js#xfbml=1";
            fjs.parentNode.insertBefore(js, fjs);
            }(document, 'script', 'facebook-jssdk'));
</script>
<div class="post">
    <article>
        <header>
            <h1>Topic Modeling for the Uninitiated</h1>
        </header>
        <div class='post-content'>
            <h2>Motivation</h2>
<p>As more and more data are stored digitally and people have better and improved tools for publishing, we are witnessing more and more text data has been collected and published in various mediums.(e-books, blogs, newspaper websites, magazines and mobile applications) So-called big data era not only enable people to collect more and more data through different forms but also provide a set of tools to analyze, infer various structures in the data and interpret that knowledge in various forms. </p>
<p>Search became more and more important to find the needle in this haystack. 
Especially, if you need a very specific document, the only option you have is to depend on the search engine’s capabilities and hope for the best. </p>
<p>Search is good, it brings the exact document that has the specific keyword and maybe the most relevant document based on your history. It may even go further and do a link analysis in a link network if it really want the bring best in the class. However, it comes to short when you want to read a collection of documents that are about “Iraq War” or it comes to short when you want to analyze the documents that are about “literature in Renaissance”. One of the reasons why they came short is because they build indices on top of documents not themes or topics. Iraq War or literature in Renaissance implicitly suggests a theme rather than keyword or phrase. </p>
<p>In order to overcome this problem, topic models provide a nice way to explore a collection of documents that share a single common theme so-called “topic”.</p>
<h2>Definition</h2>
<p>Probabilistic topic models aka Topic Models are probabilistic generative models which uncovers hidden thematic structures in large collection of documents.</p>
<p>Its statistical nature does not need any information other than the text itself. It does not use metadata, labels or annotations to build up the topics from the text. One needs to define only the number of clusters(this is quite hard, albeit). There are various different inference algorithms that enables fast inference in the corpus and some of the them also work in an online fashion so that one does not have to load the data into the memory. </p>
<p>As most of the unsupervised learning algorithms, since there is no definitive number of clusters, the topics in the corpus are quite hard to evaluate. One could define various metrics either inter-topic(distance between different topics) or intra-topic(topic coherence). However, it generally depends on manual inspection and humans to decide the exact number of clusters in the dataset. While increasing the number of topics in the dataset may provide more granular information in the dataset, it could divide a very coherent topic into two parts that may be very close to each other. </p>
<p>Although Latent Dirichlet Allocation is one of the topic models, I somehow interchangeably used to mean topic model and vice versa.</p>
<h3>Word Distribution</h3>
<p>Since topic models treat corpus and documents as bag of words, occurrences rather than position of words play an important role. The probability distribution of a particular word not only changes the topic assignment of the topic assignment the document that it belongs to but also may change the word distribution in the topic as well. </p>
<p>One thing to be noted before applying Topic Modeling is that you need to care a lot about word distribution in both corpus and also in the topics. Not only the words themselves affect the probability but also they will affect the co-occurrence probability. One word probability both plays a role in assignment of a topic for that particular word but also on other words that they are likely to occur.</p>
<p>Due to that reason, one needs to remove very common words as they do not contribute the topic assignment and shadow the other words probabilities in the topics which results in incoherent topics. Similarly, if the word occurrences is very small in the corpus, those probabilities have very small probabilities and would likely to have small effect on the word distribution and word co-occurrences. They may not affect the topics as much as very common words, yet they may increase the convergence time in the Gibbs sampling.  Due to that reason, one should also remove the very infrequent words in the corpus as well.</p>
<h3>Learning Topics</h3>
<p>Using Gibbs Sampling, one could learn the topics in the following way:</p>
<ul>
<li>We first assign a random topic to each word in the document</li>
<li>For each word in the document<ul>
<li>Compute the topic likelihood given document (1)</li>
<li>Compute the word likelihood given a topic (2)</li>
<li>Reassign the topic to the word which is a multiplicative of (1) and (2)</li>
</ul>
</li>
</ul>
<p>Topic distribution is drawn from a Dirichlet Distribution so the Dirichlet in Latent Dirichlet Allocation. The hidden structure among words in topics is latent so the Latent in LDA. </p>
<h3>Experimentation</h3>
<p>I generally run the topic models for a number of different clusters and determine the optimal number based on manual inspection. Although it is not easily quantified information, since the topics is plausible to a human being, it is easy to interpret. Especially, if you are doing exploratory analysis. This is also a good way to get a feeling in the dataset </p>
<h3>Assumptions</h3>
<h4>First One</h4>
<p>As most of statical models, topic models also do a lot of assumptions in order to build its inference on top of the observations. One of the assumptions that it makes, there is a structure on the words that compose a topic gives its power. This hidden structure that the inference of topic models uncover is quite intuitive to the humans. By looking at the word distributions, one could immediately grasp the topic. </p>
<h4>Second One</h4>
<p>Second assumption is that all of the documents is composed of multiple topics and different documents share the topics in a different proportions. This is very much like a mixture model where every observation has mixed membership among many classes(in topic modeling, they are topics). This membership could also be used document retrieval as we could compute the maximum likelihood of documents given a topic: $\max P(*|t_0)$. By doing so, one could build a topic search engine as the most relevant documents are brought for a particular topic search. If one want to go further, she could assign topic memberships on individual words as the topics are word distributions themselves. </p>
<h3>LDA Limitations</h3>
<ul>
<li>It does not use metadata or any related  information about the documents.</li>
<li>It does not allow correlation among topics and does not have a good way  to model the correlations.</li>
<li>You cannot incorporate temporal information(date of the publication) to measure the change occurring in the topic distribution. </li>
<li>It assumes a bag of words model on top of words and phrases. You cannot use the word order information in the text.</li>
</ul>
<h4>LDA Extensions</h4>
<ul>
<li>Nearly all of the limitations of LDA are addressed in different variants of LDA.</li>
<li><a href="http://www.cs.columbia.edu/~blei/papers/BleiLafferty2006a.pdf">Dynamic Topic Models</a> use temporal information to detect changes in the topic frequency over time.</li>
<li>There is <a href="https://www.cs.princeton.edu/~blei/papers/BleiLafferty2006.pdf">Correlated Topic Model</a> which allows correlation between different topics. </li>
<li><a href="http://mimno.infosci.cornell.edu/info6150/readings/398.pdf">Author-Topic Models</a> allows you to incorporate various author related information into the topic models.</li>
<li><a href="http://cs.nyu.edu/~dsontag/papers/AroraEtAl_icml13.pdf">Online Topic Models</a> allow you to infer the topics in an online manner without having to load the text data into the memory.</li>
</ul>
<h4>Tools</h4>
<ul>
<li><a href="http://nlp.stanford.edu/software/tmt/tmt-0.4/">Stanford Topic Modeling Toolbox</a> is quite good and if you want to look at the end results in Excel, it has a bunch of nice helper functions to use.  You could programmatically use Scala library as well.</li>
<li><a href="http://mallet.cs.umass.edu/">Mallet</a> is somehow outdated, yet it is mature and has a bunch of nice evaluation methods and nice default parameters. It could be also used via terminal without using its Java interface as well.</li>
<li><a href="http://factorie.cs.umass.edu/">Factorie</a> is not for topic modeling per se, it is much more comprehensive but has a suite of algorithms and implementation that could be used for topic modeling as well.</li>
<li><a href="http://cran.r-project.org/web/packages/lda/">lda</a> is for people who like R and has a number of variants of LDA as well(correlated topic model is one of them).</li>
<li>Also check out the <a href="http://www.cs.princeton.edu/~blei/topicmodeling.html">topic modeling page of David Blei</a>. He is the main author of highly cited <a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">LDA paper</a> as well.</li>
</ul>
        </div>
    </article>
          <ul class="social-media-links" style="text-align: center;">
            <li class="twitter" style="display:inline;">
            <a href="https://twitter.com/share" i
              class="twitter-share-button"
              data-url="http://bugra.github.io/work/notes/2015-02-21/topic-modeling-for-the-uninitiated/"
              data-via="bugraa">Tweet</a>
              <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
              </li>
           <li class="fb-like" style="display:inline;">
              <div class="fb-share-button" data-href="http://developers.facebook.com/docs/plugins/" data-width="32" data-type="button_count"></div>
           </li>
           <li class="google-plus" style="display:inline;">
             <!-- Place this tag where you want the +1 button to render. -->
             <div class="g-plusone" data-size="medium"></div>

             <!-- Place this tag after the last +1 button tag. -->
             <script type="text/javascript">
                 (function() {
                       var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
                           po.src = 'https://apis.google.com/js/platform.js';
                               var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
                                 })();
             </script>
           </li>
             <li class="pinterest" style="display:inline;">
               <a href="//www.pinterest.com/pin/create/button/" data-pin-do="buttonBookmark"  data-pin-shape="round" data-pin-height="32"><img src="//assets.pinterest.com/images/pidgets/pinit_fg_en_round_red_32.png" /></a><!-- Please call pinit.js only once per page --><script type="text/javascript" async src="//assets.pinterest.com/js/pinit.js"></script>
               </a>
             </li>
           </li>
         </ul>
    <div class="metadata">

        <div class="left">
            <ul>
                <li>
                    <strong>Published: </strong>
                    <i>
                        <time datetime="2015-02-21T00:00:00">
                            February 21, 2015
                        </time>
                    </i>
                </li>
                </li>
            </ul>
        </div>

        <div class="right">
            <ul>
                <li><a href="/">Home</a></li>
            </ul>
        </div>

    </div>

</div>

    <div id="disqus_thread"></div>
      <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'bugra'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
      </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

      </div>
    </div>
</div>

    <link href='http://fonts.googleapis.com/css?family=Merriweather:400,300,700' rel='stylesheet' type='text/css'>

        <link rel="stylesheet" href="http://yui.yahooapis.com/pure/0.3.0/base-min.css">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" href="/theme/css/pygments.css">
        <link rel="stylesheet" href="/theme/css/main.css">
        <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/favicon.ico" type="image/x-icon">

      <!-- Timesheet JS and CSS -->
      <script src="/theme/js/timesheet.js"></script>
      <script src="/theme/js/timesheet.bubble.js"></script>
      <script type='text/javascript'>new Timesheet('timesheet', 2008, 2020, [
  ['09/2006', '05/2011', 'B.S Electrical Engineering at Bilkent University', 'dolor'],
  ['06/2009', '09/2009', 'Software Developer Intern at Optisis Inc.', 'dolor'],
  ['06/2010', '09/2010', 'Software Developer Intern at Tubitak R&D', 'dolor'],
  ['06/2011', '09/2011', 'Research Internship at Koc University', 'dolor'],
  ['09/2011', '05/2013', 'M.S Electrical and Computer Engineering at NYU', 'lorem'],
  ['12/2012', '05/2013', 'Part-Time Software Developer at Viacom', 'lorem'],
  ['05/2013', '12/2013', 'Algorithms Engineer at dMetrics Inc.', 'ipsum'],
  ['12/2013', '12/2014', 'Machine Learning Engineer at CB Insights', 'ipsum'],
  ['12/2014', '2/2015', 'Data Scientist at Axial', 'ipsum'],
  ['03/2014', '2/2015', 'Consultant in Text and Time Series Analysis', 'default']
]);</script>
      <!-- Timesheet End -->
      <script src="/theme/js/logo.js"></script>
      <script src="http://d3js.org/d3.v3.min.js"></script>
      <script src="/theme/js/scramble.js"></script>
      <script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
      <script type="text/javascript">init();</script>
      <script>
        emailScramble = new scrambledString(document.getElementById('email'), 'emailScramble', 'dun@euuygbar.', [12, 9, 7, 6, 11, 13, 2, 8, 3, 1, 5, 4, 10]);
      </script>

<script type="text/javascript">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-32814563-1']);
_gaq.push(['_trackPageview']);

(function() {
var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>

    </body>
</html>