<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Asynchronous Web Scraping with Asyncio</title>

		<meta name="description" content="Asynchronous Web Scraping with Asyncio">
		<meta name="author" content="Bugra Akyildiz">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.min.css">
		<link rel="stylesheet" href="css/theme/solarized.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="lib/css/tomorrow-night-bright.css">

		<!-- If the query includes 'print-pdf', use the PDF print sheet -->
		<script>
			document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">


<!-- HOME PAGE -->
<div class="slides">

    <section>
        <h2>Asynchronous Web Scraping with Asyncio</h2>
        <a href="https://twitter.com/bugraa">@bugraa</a><br>
        <a href="https://bugra.github.io">bugra.github.io</a><br><br>
        <b>Bugra Akyildiz</b><br>
        <a style="color:#57068C;">bugra@nyu.edu</a><br><br>
        <a href="http://bit.ly/pygotham-2014"> http://bit.ly/pygotham-2014</a> | Repo<br>


    </section>


<!-- Word Origin --- Greek -->
<section data-markdown>
## What is it?
### Greek
- asyn => not with
- chronos => time

> not with time?
</section>

<!-- Asynchronous I/O -->
<section data-markdown>
### Asynchronous I/O

- It is a programming paradigm for performing I/O operations so that the process that issued an I/O request is not blocked until the data is available.
- After an I/O request is submitted, the process continues to execute the program and can later check the status of the submitted request.

> Asynchronous programming increases the task execution throughput where it minimizes time wasted for a blocking I/O operation.
</section>

<!-- Images for execution -->
<section>
<h2>Synchronous</h2>
<img src="static/img/sync.png" alt="Synchronous" style="width: 256px; height: 512px;">
</section>

<section>
<h2> Problem in Syncronous Execution</h2>
<img src="static/img/blocking.png" alt="Blocking" style="width: 256px; height: 512px;">
</section>

<section>
<h2>Threaded</h2>
<img src="static/img/threaded.png" alt="Threaded" style="width: 512px; height: 256px;">
</section>

<section>
<h2>Asnychronous </h2>
<img src="static/img/async.png" alt="Asynchronous" style="width: 256px; height: 512px;">
<blockquote> Interleaved Execution
</blockquote>
</section>


<section >
<img src="http://i.imgur.com/IodHyXu.jpg" alt="Asynchronous Inception" style="width: 512px; height: 1024px;">
</section>

<!-- Asynchronous Programming Advantages -->
<section data-markdown>
### Asynchronous Programming Advantages
- If there is a blocking operation for one task, you could execute others.
- It is more performant where you need to do a lot of I/O operations.
- Blocking operations do not matter as much.

</section>


<!-- Asyncio Programming Advantages over Multithreading Programming -->
<section data-markdown>
### Advantages over Multithreading Programming
- One thread is easier than multithread.
- Low overhead:
    - Easy to share data.
    - Easier to manage and control the processes in a single thread.
    - No need for communication between threads.
    - You have control all over the data, variables and whole namespace.
- Full control(instead of OS to decide which treads to be used) over the program.

</section>

<section>
<img src="http://i.imgur.com/KT1LBlg.jpg" alt="Asynchronous Seriousness" style="width: 1280px; height: 1024px;">


</section>

<section data-markdown>
### Global Interpreter Lock (GIL)
- CPython interpreter uses GIL and only one thread could execute.
- Due to GIL, the overhead Python pays for multithreading is quite large even if your machine has multiprocessors.
    - In the same time, GIL makes it more efficient for single thread.
- Due to GIL, execution speed does not increase linearly with the number of threads you are using.
- You could always run multiple interpreters to execute the job.
</section>

<!-- Historical Background -->
<section data-markdown>
## Where do we stand in Python?
- Before Python 3.x, there is no good asynchronous module in the standard library.(`asyncore` and `asynchat`, both deprecated in Python 3.x)
- First Asynchronous Programming Effort targeted to Python 3.x: [Tulip](https://code.google.com/p/tulip/) (by Guido van Rossum)
- __Tulip__ became `asyncio` module starting from Python 3.4
- If you want to use it in Python 2.x, there is [Trollius](http://trollius.readthedocs.org/en/latest/index.html).(works on Python 3.x, too)
- We will use Python 3.4 for notebooks and the code in the slides.
    - It is _the_ __Future__. (We will come back to this later)
</section>

<section>
<blockquote class="twitter-tweet" lang="en" ><p>those who donâ€™t There are two types of people in the world: those who understand asynchronous jokes and</p>&mdash; Zach Holman (@holman) <a href="https://twitter.com/holman/statuses/485883220697694209">July 6, 2014</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</section>
<!-- Asyncio Module -->
<section data-markdown>
## Asyncio Module
- New in Python 3.4!
- Single-threaded concurrent code using coroutines
- Multiplexing I/O access over sockets and other resources
- Running network clients and servers
- Other asynchronous execution related primitives
- Many more ...
</section>

<section>
<h2><b>Asyncio</b></h2>
<blockquote style="color: #268bd2;">I'm not trying to reinvent the wheel. I'm trying to build a good one. Guido von Rossum</blockquote>
</section>


<!-- Asyncio Module -->
<section data-markdown>
## Asyncio Module A Closer Look
- Concurrent Primitives
- Coroutines
- Event Loop
- Tasks
- Futures
- Transports
- Protocols
- and many more ...

</section>

<section>
    <img src="http://i.imgur.com/iHk18gH.gif" alt="Here we go" style="width: 1024px; height: 512px;">
</section>


<!-- Event Loop -->
<section data-markdown>
### Event loop
- Pluggable. (works with Twisted, Gevent and others)
- Multiplexes I/O
- Optimizes the best method for I/O mechanism.
    - No need to worry about when to trigger and call the function.
- Scheduler for __all__ of coroutines, tasks and Futures.
</section>

<!-- Event Loop -->
<section>
<h2>Reactor Pattern</h2>
<img src="static/img/reactor-pattern.png" alt="Reactor Pattern" style="width: 512px; height: 512px;">
</section>

<section data-markdown>
### Event Loop
- Schedulers
- Where coroutines exist
```python
loop = asyncio.get_event_loop() # create a loop
f = asyncio.wait([inform(link_blob) for link_blob in links]) # List of corotine functions
loop.run_until_complete(f) # Run them
```
> Coroutines cannot be run outside of event loops
</section>

<!-- Concurrent Primitives -->
<section data-markdown>
### Concurrent Primitives (Data Structures)
- Locks
- Semaphores
- Queues

</section>


<!-- Locks -->
<section data-markdown>
### Lock
- Synchronization primitive.
- It has two states `locked` and `unlocked`.
- When it is locked, it cannot be used any coroutine.
- It could be considered as a _blocking_ mechanism towards the execution
  of the tasks.
</section>

<section>
<h2>Semaphore</h2>
<img src="static/img/semaphore.png" alt="Semaphore" style="width: 1024; height: 768;">
</section>

<!-- Semaphores -->
<section data-markdown>
### Semaphore
- Manages the shared resources through locks.
- It tracks the sources by counting the times that `acquire()` and
  `release()` get called.
- If there are not sources available for `acquire()` call, it blocks the
  execution.(like in a synchronous procedure)
- Very useful if you want to limit the coroutines that run at a single
  time.
- We will revisit this concept in Responsive Web Scraping
</section>

<section>
<img src="static/img/queue.png" alt="Queue" style="width: 400; height: 400px;">
</section>

<!-- Queues -->
<section data-markdown>
### Queue
- Asynchronous Queue primitive.
- Manages the producer and consumer coroutines.
- If you do not want to use coroutines, they also support `get_nowait()`
  and `put_nowait()` functions to remove the asynchronous nature of
calls.
</section>

<!-- Coroutines -->
<section data-markdown>
### Coroutines
- They need to have `yield from` in their function body.
- `yield from awesome_coroutine()`.
- `awesome_coroutine()` returns only a generator.
- For readability purposes, they should be decorated with `@asyncio.coroutine`.
- `asyncio.iscoroutinefunction()`
- `asyncio.iscoroutine()`


> They are using `introspect` module under the hood.
</section>

<section data-markdown>
```python
sleep_time = 1.
def i_am_not_a_coroutine():
    for ii in range(1, 31):
        if ii % 10 == 0:
            print('I am not a coroutine :(')
        time.sleep(sleep_time)
        print('I  slept {} seconds'.format(sleep_time))
start = time.perf_counter()
i_am_not_a_coroutine()
print('It has been {}'.format(time.perf_counter() - start))
```
</section>


<section data-markdown>
```python
@asyncio.coroutine
def i_am_a_coroutine():
    for ii in range(1, 31):
        print('I  slept {} seconds'.format(sleep_time))
        if ii % 10 == 0:
            print('I am a coroutine!')
        yield from asyncio.sleep(sleep_time)
start = time.perf_counter()
new_loop = asyncio.get_event_loop()
f = asyncio.wait([i_am_a_coroutine() for _ in range(10)]) # Put 10 coroutines in a sequence
new_loop.run_until_complete(f) # Run
print('It has been {}'.format(time.perf_counter() - start))
```
</section>


<section data-markdown>
### Web Scraping
- HTTP Library
- HTML Parser

</section>

<section data-markdown>
### HTTP Library
- [urrlib](https://docs.python.org/2/library/urllib.html), [urrlib2](https://docs.python.org/2/library/urllib2.html) and their API is somehow broken in Python 2.
- New [`urllib`](https://docs.python.org/3/library/urllib.html) library is more sane in Python 3 and provides a more coherent namespace.
- Yet, its API is still not as good as [Requests](http://docs.python-requests.org/en/latest/)

> Requests and most of the HTTP libraries are synchronous.
> HTTP protocol is also synchronous, no surprise there

</section>

<section>
<h3> HTML Parser</h3>
<ul>
    <li>Do not even attempt to use regex to parse HTML. (If you are not Chuck Norris)</li>
</ul>
<img src="static/img/chuck-norris.png" alt="Paris Hilton Regex" style="width: 512px; height: 100px;">
<img src="static/img/paris-hilton-regex.png" alt="Paris Hilton Regex" style="width: 768px; height: 128px;">
</section>

<section>
<h3>One of the Most Famous Answers in SO</h3>
<img src="static/img/famous-regex-comment.png" alt="Famous Regex" style="width: 1280px; height: 768px;">
</section>

<section data-markdown>
### HTML Parser
- There are tons of libraries in Python for parsing HTML
    - [lxml](http://lxml.de/) => very fast
    - [BeautifulSoup](http://en.wikipedia.org/wiki/Beautiful_Soup) => most adopted and used
    - [PyQuery](https://pypi.python.org/pypi/pyquery) => on top of lxml, fast
    - [Scrapy](http://scrapy.org/) => full featured
    - [Mechanize](http://wwwsearch.sourceforge.net/mechanize/) => full featured(includes logins and forms)
    - [HTML Parser](https://docs.python.org/3/library/html.parser.html) => barebone, not very good.
        - But A for the effort putting in the standard library
</section>


<section data-markdown>
```python
import requests
from pyquery import PyQuery as pq
schedule_url = 'http://pygotham.org/talks/schedule'
raw_html = requests.get(schedule_url).text
talks = []
for ii in pq(raw_html)('.slot-talk'):
    title, _, presenter = str(ii.text_content()).strip().split('\n')
    talks.append((title.strip(), presenter.strip().title()))
```

```python
# Talks - First 16
[('Python Apps and Docker', 'Matt Wright'),
 ('Insight into the anatomy of a huge Python project', 'Thomas Hatch'),
 ('Getting Rich with Comparison Methods', 'Matt Story'),
 ('Games with Pygame', 'Patrick Thunstrom'),
 ('Finding your "Teaching Stack": tools for teaching Python from beginners to experts',
  'Elliott Hauser'),
 ('Customizable and SaaSy REST APIs', 'Juan Gutierrez'),
 ('Bend Postgres to Your Pythonic Will', 'Wes Chow'),
 ('High Technology for High Needs Students: A year of programming in the Bronx',
  'Meg Winston Ray'),
 ('How I Taught a 10-year-old to Love Python and Programming', 'Eric Matzner'),
 ('TSAR (the TimeSeries AggregatoR), - how to count tens of billions of daily events in real time using open source technologies',
  'Anirudh Todi'),
 ('Python Wats: Uncovering Odd Behavior', 'Amy Hanlon'),
 ('Python as uniting programming language across computer graphics packages and 3D automated manufacturing',
  'Rainer Schmidt'),
 ('How I use Python to Fight human trafficking', 'Eric Schles'),
 ('Helping Python Play Chess', 'Jarret Petrillo'),
 ('Canonical sectors and evolutions of US stocks: an application of machine learning in Python',
  'Ricky Chachra'),
 ('Sparkling Pandas - using Apache Spark to scale Pandas', 'Holden Karau'),]
```
</section>

<section data-markdown>
### Single-Page Application (SPA)
- Content is retrieved via AJAX request when user lands on page.
- When you make a `GET` request via scraper, you do not necessarily get the content that an user sees.
- Do not despair!
- With Python, we have great tools to deal with this problem as well.

</section>

<section data-markdown>

### Selenium and PhantomJS to rescue
- [Selenium](http://selenium-python.readthedocs.org/en/latest/index.html)
- [PhantomJS](http://phantomjs.org/)
- Install Selenium and PhantomJS
- Use the PhantomJS driver

```
# On Mac
## After installing brew and pip
brew update && brew install phantomjs
pip install selenium
```

```python
from selenium import webdriver
driver = webdriver.PhantomJS()
driver.get(SINGLE_PAGE_APPLICATION_URL)
# With content retrieved via AJAX!
raw_html =  driver.page_source
driver.quit()
```
</section>

<section data-markdown>
### Splinter
- Based on Selenium
- Nice, high level API
- Immature
- [Documentation](http://splinter.readthedocs.org/en/latest/) is good

```
pip install splinter
```

```python
from splinter import Browser
with Browser() as browser:
    browser.visit(SINGLE_PAGE_APPLICATION_URL)
    raw_html = browser.html
```
</section>

<section>
<h3>Responsive Web Scraping</h3>
<ul>
    <li>Do not open too many connections to same base url!</li>
    <li>Do not scrape if `robots.txt` disallows scraping.</li>
    <li>Do not crawl if they have an API.</li>
    <li>Crawl once, process multiple times. (Preferably offline)</li>
    <li>Avoid XML if you could.</li>
    <li>If you have to, there is a <a href="https://github.com/martinblech/xmltodict"><b>xmltodict</b></a> (fast enough)</li>
</ul>
<blockquote> <a href="https://www.google.com/killer-robots.txt">Google Killer Robots</a></blockquote>
<img src="static/img/google-killer-robots.png" alt="Reactor Pattern" style="width: 512px; height: 256px;">
</section>

<section data-markdown>
### Semaphores for Responsive Web Scraping
- Use __Semaphores__ to limit the connection numbers or limit the resources.
    (If we are crawling the same web server, use this one in order
not to damage and make the load of the server too much!)

```python
sem = asyncio.Semaphore(10) # Permit only 10 connections at a time
with (yield from sem):
    page = yield from get_body(url)
```
instead of
```python
page = yield from get_body(url
```
</section>

<section data-markdown>
### What next?
- Try to think the problems in an asynchronous manner.
- Try to use `asyncio` in your next project which depends on I/O.
- Use Python 3, it breaks incompatibility, but it is simply __better__. (at least your side projects)
- Try to learn more advanced and low-level `asyncio` primitives; transports and protocols.

> Syncronous is not cool... You know what is cool? __Asynchronous__.
</section>

<section data-markdown>
### Materials on Coroutine
- [Coroutines by David Beazley](http://www.dabeaz.com/coroutines/) (For core concepts, it is good start, outdated ~2008 though)
- [Difference between `yield` and `yield from` by Guido van Rossum](https://groups.google.com/forum/#!msg/python-tulip/bmphRrryuFk/aB45sEJUomYJ)
-

</section>


<section data-markdown>
### Third Party Scraping Sources
- [Yahoo Query Language](https://developer.yahoo.com/yql/)
- [import.io](https://import.io/)
- [Kimono Labs](https://www.kimonolabs.com/)
- [ScraperWiki](https://scraperwiki.com/)

### Other Useful Libraries
- [Pattern](http://www.clips.ua.ac.be/pages/pattern)
- [Bixo](http://bixo.101tec.com/)
- [Upton in Ruby](https://github.com/propublica/upton)
- [Mechanize in Ruby](https://github.com/sparklemotion/mechanize)
- [Portia - Visual Scraping](https://github.com/scrapinghub/portia)

</section>
<!-- Credits -->
<section data-markdown>
### Credits
- http://krondo.com/?p=1209
    - (Most of images are taken from here)

</section>


<section>
<ul>
    <li>For notebooks and code: </li>
    <li><a href="http://bit.ly/pygotham-2014"> http://bit.ly/pygotham-2014</a></li>
    <li>For everything else:</li>
</ul><br><br>
<a style="color:#57068C;">bugra@nyu.edu</a><br>
Thanks!
</section>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>
    <script>
    // Full list of configuration options available here:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        width: 1024,
        height: 768,
        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'fade', // default/cube/page/concave/zoom/linear/fade/none
        math: {
                mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
                config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
            },
        // Optional libraries used to extend on reveal.js
        dependencies: [
            { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
            { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
            { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
            { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
            { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
            { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
            { src: 'plugin/math/math.js', async: true }
            // { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
            // { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
        ]
    });

        </script>

	</body>
</html>
