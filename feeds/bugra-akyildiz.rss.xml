<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bugra Akyildiz</title><link>http://bugra.github.io/</link><description></description><atom:link href="http://bugra.github.io/feeds/bugra-akyildiz.rss.xml" rel="self"></atom:link><lastBuildDate>Sun, 20 Jul 2014 00:00:00 +0000</lastBuildDate><item><title>Pydata Silicon Valley 2014 Part 2</title><link>http://bugra.github.io/work/notes/2014-07-20/pydata-silicon-valley-2014-part-2/</link><description>&lt;p&gt;&lt;img alt="Pydata Silicon Valley 2014" src="http://i.imgur.com/tInmSM1.png" /&gt;
The Pydata Silicon Valley videos are put into Youtube, thanks to
Facebook, Continuum and Parse.ly. Check it &lt;a href="https://www.youtube.com/user/PyDataTV"&gt;out&lt;/a&gt;.
They are pretty great. &lt;/p&gt;
&lt;p&gt;I already wrote about(kind of wrap-up) the presentations that I attended &lt;a href="http://bugra.github.io/work/notes/2014-05-12/pydata-silicon-valley-2014/"&gt;here&lt;/a&gt;.
This one will be similar to the ones that I could not attend but watch the videos.&lt;/p&gt;
&lt;p&gt;I somehow managed to miss some of the most interesting talks but could follow the presentation
much better through their videos on Youtube than real-life so it kind of worked better
than I expected.&lt;/p&gt;
&lt;h3&gt;ggplot&lt;/h3&gt;
&lt;p&gt;Presenter: Greg Lamb  &lt;/p&gt;
&lt;p&gt;As I already mentioned in the first part, this tutorial introduces &lt;code&gt;ggplot&lt;/code&gt; visualization 
library which is based on Grammar of Graphics. &lt;/p&gt;
&lt;h4&gt;What is ggplot?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Based on Grammar of Graphics&lt;/li&gt;
&lt;li&gt;Implementation in Python(from R implementation, ggplot2)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Matplotlib&lt;/h4&gt;
&lt;h5&gt;Advantages&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Mature, it has been around for some time&lt;/li&gt;
&lt;li&gt;Integration with Ipython Notebook&lt;/li&gt;
&lt;li&gt;Customizable&lt;/li&gt;
&lt;li&gt;Community&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Disadvantages&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;API, learning curve, syntax, default themes&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;d3.js is like a Mona Lisa painting where &lt;code&gt;ggplot&lt;/code&gt; is like 
a camera. You buy the painting to show off where camera is 
more like a utility.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This comparison quite good, generally the plots(either ggplot or matplotlib) 
are either ad-hoc analysises or for internal usage. If you are producing something
consumer facing, you should be using much more suited library for the job. From
the conference, I actually used &lt;code&gt;ggplot&lt;/code&gt; for couple of projects and I liked its syntax
and api as well as the integration with pandas. From a 2-3 years matplotlib user, I think
it will be great library when it reaches maturity. However, I think  &lt;code&gt;ggplot&lt;/code&gt; is more a
competitor to &lt;code&gt;seaborn&lt;/code&gt; rather than matplotlib. Matlotlib is a plotting library 
in a much broader sense(it has support for maps, images, 3D visualizations as well) 
where &lt;code&gt;ggplot&lt;/code&gt; and &lt;code&gt;seaborn&lt;/code&gt; are more targeted to a specific subset of visualizations.
The subset is quite important, though.&lt;/p&gt;
&lt;p&gt;That being said, it attacks the disadvantages of matplotlib quite well. It has great API,
concise syntax and much better default themes. &lt;/p&gt;
&lt;h4&gt;Syntax&lt;/h4&gt;
&lt;p&gt;Especially, syntax of &lt;code&gt;ggplot&lt;/code&gt; is great.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mtcars&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt;&lt;span class="n"&gt;wt&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt;&lt;span class="n"&gt;mpg&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is &lt;em&gt;what&lt;/em&gt; step. This is where you say I want to bind 
&lt;code&gt;wt&lt;/code&gt; to x axis and &lt;code&gt;mpg&lt;/code&gt; to y axis. Then, in the &lt;em&gt;how&lt;/em&gt; step, you
will say how&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;geom_point&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This produces a scatterplot. If you want to produce a density
graph, you could change it &lt;code&gt;p + geom_density()&lt;/code&gt; without changing
&lt;em&gt;what&lt;/em&gt; step. This is pretty great. If you want to put also an 
averaging window over the variable, &lt;code&gt;p + geom_point() + stat_smooth()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This layered approach provides couple of benefits. All of the layers
can be treated independently as visualization is mainly defined in 
&lt;em&gt;what&lt;/em&gt; step rather than &lt;em&gt;how&lt;/em&gt;. Therefore, you could easily experiment and
play with different layers on your data visualization. Icing on the 
cake, faceting becomes very easy with &lt;code&gt;ggplot&lt;/code&gt; as it has a faceting 
utility out of the box. I am big fan of faceting, in order to understand
some categorical variable on the parameters and to be able to see 
the effect, so it is a great tool to do exploratory data analysis. 
If you want to learn more, check out the &lt;a href="http://yhat.github.io/ggplot/"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Functional Performance with Core Data Structures&lt;/h4&gt;
&lt;p&gt;Presenter: Matthew Rocklin&lt;/p&gt;
&lt;p&gt;The premise of this talk is the following two:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pure Python is not slow&lt;/li&gt;
&lt;li&gt;Python is a decent language.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to prove the first one, he provides benchmarks to show that Python is not “that” slow
comparing to other languages. For second one, he introduces a functional programming library &lt;a href="https://github.com/pytoolz/toolz/"&gt;Toolz&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The library is actually pretty great. He was influenced by Clojure which I wrote how great it is &lt;a href="http://bugra.github.io/work/notes/2014-07-09/pigpen-hadoop-pig-clojure-cascading/"&gt;here.&lt;/a&gt;
The library promotes laziness, functions, pure functions, composability; all of the good stuff 
that are common in functional programming languages. Surprisingly, some of the functions are actually same with standard 
library function like; &lt;code&gt;groupby&lt;/code&gt; in &lt;code&gt;itertools&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt; and he claimed functions are actually faster than standard library
equivalents. Some of the functions are not in the Python like &lt;code&gt;take&lt;/code&gt;, &lt;code&gt;interleave&lt;/code&gt;, or &lt;code&gt;pipe&lt;/code&gt;. &lt;code&gt;pipe&lt;/code&gt; is pretty good, it provides 
a similar mechanism with &lt;code&gt;-&amp;gt;&amp;gt;&lt;/code&gt; clojure where you could chain a bunch of functions and does not sacrifice readability and execution order.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;pipe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;to get the string representation of 3.0. This is a great way to compose functions. &lt;/p&gt;
&lt;p&gt;Another example he gave is much more impressive:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;pipe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;tale&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;of&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;two&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;cities&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                    &lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                    &lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                    &lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                    &lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stem&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                    &lt;span class="n"&gt;frequencies&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is pretty great.&lt;/p&gt;
&lt;p&gt;He also gave another library direction which implements the same functionality of Toolz 
in C, which is more efficient, called unsurprisingly &lt;a href="https://github.com/pytoolz/cytoolz"&gt;Cytoolz&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I am quite happy to see there are great libraries like these two to make at least a dent for functional programming 
in Python.&lt;/p&gt;
&lt;p&gt;He then summarizes why Functional programming is becoming more relevant as following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Good software practices&lt;/li&gt;
&lt;li&gt;Composition promotes scalable software systems&lt;/li&gt;
&lt;li&gt;Purity, composition promotes testing&lt;/li&gt;
&lt;li&gt;Sequence abstractions support streaming computation(if you have low memory, you have no other choice anyway)&lt;/li&gt;
&lt;li&gt;Purity, serializability promotes parallel and concurrent processing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Querying your database in natural languages&lt;/h3&gt;
&lt;p&gt;Presenter: Daniel Moisset  &lt;/p&gt;
&lt;p&gt;This is one of the talk that I regretted most that I did not attend. Not only it is quite
interesting on its own, one of previous project that I did(querying yelp restaurants
with natural language) is directly related. &lt;/p&gt;
&lt;p&gt;Generally, speaking or natural language in writing provides a much easy interface
to user. However, the systems that process these inputs are in their early periods
so they cannot really process the inputs very effectively. Therefore, you’d see all
those radiobuttons, sliders, checkboxes and text input area for user to enter 
structured information rather than user typing “show me the closest five restaurants
that serve Turkish food and accepts credit card”. Question types vary quite a lot
for different people and even the order, verbs and nouns are not something most
people agree on. Therefore, most companies provide “intuitive” interfaces to get
structuresd data, to make it easier sometimes they help, too.(autocomplete)
&lt;a href="http://quepy.machinalis.com/"&gt;Quepy&lt;/a&gt; attacks the problem in a somehow different angle.
First, it tries to find &lt;strong&gt;what&lt;/strong&gt; is being filtered and then, try to return the 
query based on the filtering parameters. As I mentioned later, tackling the question
variation in a regular expression way is somehow limited but since building different
of versions of questions is easy, it kind of compensates its limitation by providing
ease of use. Other than that, installing and integrating the library in your application
is quite easy.(some of the commands resemble Django as well). It is a very nice library.&lt;/p&gt;
&lt;h4&gt;Introduction&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Data is everywhere&lt;/li&gt;
&lt;li&gt;Collecting data is not the problem but what to do with is the real problem&lt;/li&gt;
&lt;li&gt;Any operation starts with selecting and filtering data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Natural Language Queries&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Very accessible, trivial to learn&lt;/li&gt;
&lt;li&gt;Weak is mos applications as it has limited list of queries that it can handle&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a href="http://quepy.machinalis.com/"&gt;Quepy&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;They propose &lt;strong&gt;Quepy&lt;/strong&gt; which is an &lt;a href="https://github.com/machinalis/quepy"&gt;open-sourced&lt;/a&gt; 
framework to translate natural language questions into a database query langauge. &lt;/li&gt;
&lt;li&gt;Support for database query language is somehow limited(SparQL and MQL).&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Approach&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;First it parses the query&lt;/li&gt;
&lt;li&gt;Matches and creates an intermediate representation&lt;/li&gt;
&lt;li&gt;Then, based on representation, it generates query and Domain Specific Language&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;Parsing&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;It is done in a word level&lt;ul&gt;
&lt;li&gt;is: is/be/VBZ&lt;/li&gt;
&lt;li&gt;swallows: swallows/swallow/NNS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Question Rule =&amp;gt; Regular expressions&lt;ul&gt;
&lt;li&gt;Token(“what”) + Lemma (“be”) + Question Pos(“DT”) + Pos(“NN”)
=&amp;gt; The word “what” followed by any verb “of to be” veryb, optionally followed 
by a word determines the count(“all”, “every”), followed by one or more nouns.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;In my opinion, the biggest disadvantage is question rule. As the regular expressions
are very limited in terms of structure, and it seems only could “answer” &lt;em&gt;Wh&lt;/em&gt; questions
for specific forms. “Give me”, “Find me” order forms, yes-no questions, 
subordinate clauses(this corresponds to “where” in sql langauge) do not exist. Otherwise, 
library especially in terms of usage seems very good. However, various question forms
could be easily added to the question types using &lt;code&gt;QuestionTemplate&lt;/code&gt;. However, I was
still expecting, very common question type support out of the box for a natural langauge
translator to database query.(Only this one is quite hard problem to be fair)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5&gt;Intermediate Representation&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Graph-like, with some known values and some holes(x0, x1, x2). Always has a “root”.&lt;/li&gt;
&lt;li&gt;Similar to knowledge databases(like freebase)&lt;/li&gt;
&lt;li&gt;Easy to build from python code.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;DSL&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Single concepts with fixed relations or entities.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;WhatIs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;QuestionTemplate&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;regex&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Lemma&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;what&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;Lemma&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;be&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; \
        &lt;span class="n"&gt;Question&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;POS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;DT&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;Thing&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;Question&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;POS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;interpret&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;match&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;DefinitionOf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;match&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;thing&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This Python code is great in terms of several aspects. First is very easy to 
construct question types in this from. Second, it is very expressive and readable
what type of question types that you cover if you want to see what types 
of questions you already covered. For fixed type DSLs, it resembles a lot like
freebase.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;IsPerson&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FixedType&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;fixed_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;people&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;person&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;
    &lt;span class="n"&gt;fixed_type_relation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Apps: Gluing it all together&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;You build a Python package with&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;quepy&lt;/span&gt; &lt;span class="n"&gt;startapp&lt;/span&gt; &lt;span class="n"&gt;myapp&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;There you add dsl and questions templates&lt;/li&gt;
&lt;li&gt;Then configure it editing &lt;code&gt;myapp/settings.py&lt;/code&gt;(output query language, data encoding)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Resembles a lot like Django.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;quepy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;install&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;myapp&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;question&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;what&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="n"&gt;love&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;
&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;metadata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;question&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;execute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;The Good Things&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Effort to add questions template is small and the benefit is linear
with respect to effort.&lt;/li&gt;
&lt;li&gt;Good for industry applications.&lt;/li&gt;
&lt;li&gt;Low specialization requireed to extend&lt;/li&gt;
&lt;li&gt;Human work is very parallelizable&lt;/li&gt;
&lt;li&gt;Easy to get many people to work on questions.&lt;/li&gt;
&lt;li&gt;Better for domain specific databases.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Future Directions&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Testing this under databases.&lt;/li&gt;
&lt;li&gt;Improving performance&lt;/li&gt;
&lt;li&gt;Collecting uncovered questions, add machine learning to learn 
new patterns(Yes, especially trying to cover all of the forms from a 
set of question forms would be very valuable)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;PyAlgoViz: Python Algorithm Visualization in the browser&lt;/h3&gt;
&lt;p&gt;Presenter: Chriss Laffra&lt;/p&gt;
&lt;p&gt;He presents a very nice visualization environment &lt;a href="http://pyalgoviz.appspot.com/"&gt;PyAlgoViz&lt;/a&gt;
where any developer could visualize execution steps of the algorithm or 
algorithm in general in a visualization rich way. This kind of opens up the algorithms black box
and makes them more memorable through visualizations as well as nice way to learn
what they are in the first place. Debugging would be also easier in this
type of environment if your algorithm does not produce the output that you are expecting.&lt;/p&gt;
&lt;h3&gt;Building an Army of Data Collecting Posts in Python&lt;/h3&gt;
&lt;p&gt;Presenter: Freedom Dumlao  &lt;/p&gt;
&lt;p&gt;This presentation’s focus is more on architecture than anything else. He took a 
real-world problem and try to tackle it. Architecturing the whole framework bit by
bit was a nice approach to build the system. &lt;/p&gt;
&lt;p&gt;I really like the presentation style(along
with James Horey) where they present the solutions in a story.(his friend and a 
beginner Python developer)
This makes somehow easier to both understand the problem, domain, contextand how 
the proposed method solves the given problem.&lt;/p&gt;
&lt;h4&gt;OpenGraph Protocol&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;This protocol enables any webpage to become a rich object in a social graph.&lt;/li&gt;
&lt;li&gt;The reason when you share something, open graph title, url and image
is used in the the social media that you are sharing with.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;RQ&lt;/h4&gt;
&lt;p&gt;They used RQ to handle job queues with Redis to store the files.&lt;/p&gt;
&lt;h4&gt;3 Services&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;ElasticSearch (it is used like a database, ), qbox.io&lt;/li&gt;
&lt;li&gt;Task Queue (RQ &amp;amp; Redis) =&amp;gt; Redis to Go&lt;/li&gt;
&lt;li&gt;Compute (Heroku)&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Rules to keep in mind&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Don’t do DOS to your neighbors&lt;/li&gt;
&lt;li&gt;Look at robots.txt if you are permitted&lt;/li&gt;
&lt;li&gt;Don’t be a jerk.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Ferry Share and Deploy Big Data Applications with Docker&lt;/h3&gt;
&lt;p&gt;Presenter: James Horey  &lt;/p&gt;
&lt;p&gt;I had the chance to talk with &lt;a href="http://jhorey.github.io/"&gt;him&lt;/a&gt; on Saturday, his 
presentation was also like him, quite awesome. If you want to understand what Docker
or why it is necessary/beneficial, definitely watch his talk. It is a beginner talk
for infrastructure. This is also one of the most organized presentation that I saw 
as well so it is very easy to follow as well. He eventually introduces his library
managing containers &lt;a href="http://ferry.opencore.io/"&gt;Ferry&lt;/a&gt;, which makes it easy to 
manage the containers. &lt;/p&gt;
&lt;h4&gt;Docker&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Encapsulates applications in isolated containers.&lt;/li&gt;
&lt;li&gt;Makes it easy and safe to distribute applications.&lt;/li&gt;
&lt;li&gt;Easy to get started.&lt;/li&gt;
&lt;li&gt;Nice way to isolate the runtime environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Docker allows public images to be shared freely.&lt;/p&gt;
&lt;p&gt;Using docker gives isolated environments advantages and very
low overhead and disadvantages.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Cassandra&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Highly scalable and fault-tolerant.&lt;/li&gt;
&lt;li&gt;Great for storing streaming data(sensors, messages)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Ferry&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Specify the containers that constitue your application in YAML.&lt;/li&gt;
&lt;li&gt;Support for Hadoop, Cassandra, GlusterFS, and OpenMPI.&lt;/li&gt;
&lt;li&gt;It’s a little bit like &lt;code&gt;pip&lt;/code&gt; for your Docker-based runtime
environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Advantages of using Ferry&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Even simple applications can be complicated to install and run.&lt;/li&gt;
&lt;li&gt;Docker solves this by encapsulating the entire runtime environment
in lightweight containers.&lt;/li&gt;
&lt;li&gt;Ferry enables plugging in external big data dependencies.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Crushing the Head of the Snake&lt;/h3&gt;
&lt;p&gt;Presenter: Robert Brewer  &lt;/p&gt;
&lt;p&gt;It was an optimization talks where he walks through what to do improving the 
speed of the code. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;timeit&lt;/code&gt;’s default argument is 1000000, nobody wants to wait that long
to profile. It has also overhead, so for benchmarking, that needs to be
subtracted from the end result.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Timer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repeat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this run, take the minimum of three as the others may have some other background
or some other overhead processes are added.&lt;/p&gt;
&lt;h4&gt;Some Rule of Thumbs&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Use Python builtins. As they are written in C, the ones that you will be using
from external libraries would be slower(written in Python).&lt;/li&gt;
&lt;li&gt;Reduce function calls. There is some overhead in calling functions in Python.&lt;/li&gt;
&lt;li&gt;Vector operations can be done in Numpy.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Parallelization&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;multiprocessin&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Pool&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Pool&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;run_one&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;segments&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Introducing x-ray: extended arrays for scientific datasets&lt;/h3&gt;
&lt;p&gt;Presenter: Stephan Hoyer  &lt;/p&gt;
&lt;p&gt;&lt;a href="http://xray.readthedocs.org/en/stable/"&gt;X-ray&lt;/a&gt; is a library that provides a rich
datastructure for weather data which handles multidimensional data.&lt;/p&gt;
&lt;h4&gt;Why it is necessary?&lt;/h4&gt;
&lt;p&gt;5 dimensional weather forecasts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;labeled&lt;/li&gt;
&lt;li&gt;big&lt;/li&gt;
&lt;li&gt;binary&lt;/li&gt;
&lt;li&gt;high-dimensioanl&lt;/li&gt;
&lt;li&gt;not very natural to put in a pandas dataframe&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;What we want?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Like &lt;code&gt;numpy.ndarray&lt;/code&gt;, but better&lt;/li&gt;
&lt;li&gt;N-dimensional&lt;/li&gt;
&lt;li&gt;Handles missing data&lt;/li&gt;
&lt;li&gt;Labeled dimensions(axes)&lt;ul&gt;
&lt;li&gt;“time”, “latitude”, “longitude”&lt;/li&gt;
&lt;li&gt;not “index” and “columns”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Labeled coordinates(ticks)&lt;/li&gt;
&lt;li&gt;Lazy array acess(out of memory data)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Xray Design Goals&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Labeld, N-dimensional arrays for scientific data&lt;/li&gt;
&lt;li&gt;Don’t rebuild the wheel&lt;ul&gt;
&lt;li&gt;reuse pandas indexing&lt;/li&gt;
&lt;li&gt;copy the pandas API&lt;/li&gt;
&lt;li&gt;Interoperate with the rest of the scientific task.&lt;/li&gt;
&lt;li&gt;Implement a proven data model.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Xray Data Model&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;DataArray&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N-dimensional&lt;/li&gt;
&lt;li&gt;labeled dimensions (axes)&lt;/li&gt;
&lt;li&gt;labeled coordinates (indices)&lt;/li&gt;
&lt;li&gt;homogeneous dtype&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dataset&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a dict-like container of DataArrays&lt;/li&gt;
&lt;li&gt;shared dimensions and coordinates&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Both keep track of arbitrary metadata (attributes)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Pointers&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://xray.readthedocs.org/en/stable/"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/xray/xray"&gt;Source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Generators Will Free Your Mind&lt;/h3&gt;
&lt;p&gt;Presenter: James Powell  &lt;/p&gt;
&lt;p&gt;James shows us why usage of generators may change our programming style or at least
how we approach problems. I think a better title for this talk “Functional Programming
Will Free Your Mind” as most of the concepts and some portion of talk is about why 
functional programming is better than imperative style. Some of the code he writes,
especially how he tackles the formatting problem is quite mind-opening. Especially,
I come to like adding predicates instead of &lt;code&gt;if else&lt;/code&gt; into code whenever I want to add
some feature to the existing code. This not only makes less assumptions on the 
data that you are getting but also makes it easier to read and more maintainable code.
Generators, lazy evaluations part that he stresses on is also great. Instead of 
giving me the &lt;strong&gt;all&lt;/strong&gt; data give me as much as I want. If I want to process the data 
in a batch fashion, let me handle that for myself.(less assumption, memory 
efficiency). It was a quite great talk.&lt;/p&gt;
&lt;h4&gt;Themes&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Generators are a very useful way of modeling problems in Python&lt;/li&gt;
&lt;li&gt;The ways we model problems using generators is fundamentally
different&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Functional Programming&lt;/h4&gt;
&lt;p&gt;Modalities could be &lt;strong&gt;injected&lt;/strong&gt; through functional programming.&lt;/p&gt;
&lt;h5&gt;Template Usage&lt;/h5&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;template&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;region&lt;/span&gt;&lt;span class="p"&gt;}:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;align&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;profit&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;
&lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;template&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;region&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;region&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;profit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;profit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;align&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;align&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;What is generator &amp;amp; Coroutine?&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;
        &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;islice&lt;/span&gt;
&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;islice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Nice Util Functions for Functional Programming&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;islice&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;izip&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tee&lt;/span&gt;
&lt;span class="n"&gt;nwise&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;izip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;islice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tee&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;ratio&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="n"&gt;fro&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nwise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;islice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ratio&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;takewhile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropwhile&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;islice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;takewhile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;Pointers&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/urls/gist.githubusercontent.com/dutc/4eb5124bda88eedd476e/raw/a06c32ab62dd9ca941ef8537dab796d212348201/generators-free-your-mind.ipynb"&gt;Ipython Notebook-Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;How to Spy with Python&lt;/h3&gt;
&lt;p&gt;Presenter: Lynn Root  &lt;/p&gt;
&lt;p&gt;This talk was about how to sniff packages around Python, at least for most part of it.
Otherwise, she presented a great historical context what an NSA is, how it may be doing
the surveillance, using metadata, and whatnot. This not only provides a much more richer
context than what is available, but also provides where it started and where it is 
growing/going. &lt;/p&gt;
&lt;h4&gt;Prism&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Planning tool for Resource Integration, Synchronization and Management&lt;/li&gt;
&lt;li&gt;Mines electronic data for the purpose of mass surveillance.&lt;/li&gt;
&lt;li&gt;Collects intelligence that passes through US servers&lt;/li&gt;
&lt;li&gt;Targets foreigners, but is elusive about data on US citizens.&lt;/li&gt;
&lt;li&gt;Only collects metadata(supposedly).&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;What is XKeyScore?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Digital Network Intelligence Exploitation System&lt;/li&gt;
&lt;li&gt;Federated Query System of completely unfiltered data&lt;/li&gt;
&lt;li&gt;500-700 servers, as of 2008&lt;/li&gt;
&lt;li&gt;Gives users ability to query for email addresses, a target’s activity, phone numbers,
HTTP traffic, extract file attachments, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Technologies&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.secdev.org/projects/scapy/"&gt;scapy&lt;/a&gt;: packet sniffing &amp;amp; manipulation&lt;/li&gt;
&lt;li&gt;&lt;a href="https://code.google.com/p/pygeoip/"&gt;pygeoip&lt;/a&gt;: API for GEOIP databases&lt;/li&gt;
&lt;li&gt;&lt;a href="http://geojson.org/"&gt;geojson&lt;/a&gt;: bindings &amp;amp; utilities for GeoJSON&lt;/li&gt;
&lt;li&gt;&lt;a href="http://xael.org/norman/python/python-nmap/"&gt;python-nmap&lt;/a&gt;: wrapper around nmap port scanner&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Wrap-Up&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Long historical precedence for mass surveillance with very little oversight or restriction.&lt;/li&gt;
&lt;li&gt;Safe to assume they spy everyone.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Pointers&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://rogue.ly/spy"&gt;The presentation material&lt;/a&gt;. &lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/urls/gist.github.com/econchick/7610860/raw/bfd06d3b326ee019425f2a46e9a088d3dbba1ca6/pycones.ipynb"&gt;The notebook here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://rogue.ly/spy-quick"&gt;Introduction to Scapy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/econchick/spy"&gt;Spy Github Repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Hustle: A column Oriented, Distributed Event Database&lt;/h3&gt;
&lt;p&gt;Presenter: Tim Spurway  &lt;/p&gt;
&lt;p&gt;Hustle is another distributed database. It is relational and claims to do super
fast queries and distributed writes(inserts). It is an interesting concept and
its domain query language is Python based. Seeing Python to be different DSL’s of 
data processing frameworks, it was not a big suprise for me seeing a database
solely depends on Python for all querying tasks. &lt;/p&gt;
&lt;h4&gt;Hustle Features&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Open Source&lt;/li&gt;
&lt;li&gt;Column Oriented, pipelined execution = Fast queries&lt;/li&gt;
&lt;li&gt;Embarrasingly distributed =&amp;gt; Disco MIR pipelines, DDFS&lt;/li&gt;
&lt;li&gt;Embarrasingly fast = LMDB bt Tree&lt;/li&gt;
&lt;li&gt;Relational = Join Large Tables&lt;/li&gt;
&lt;li&gt;Partitioned = Smart, Automatic Sharding&lt;/li&gt;
&lt;li&gt;Query Language: Python DSL&lt;/li&gt;
&lt;li&gt;Advanced Compression; store even more&lt;/li&gt;
&lt;li&gt;Command Line Interface: Ad-hoc queries&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Column Orientation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;As opposed to traditional row oriented DBMSes&lt;/li&gt;
&lt;li&gt;Typical use-case is extremely large datasets.&lt;/li&gt;
&lt;li&gt;Very efficient column aggreggation&lt;/li&gt;
&lt;li&gt;Excellent compression characteristics.&lt;/li&gt;
&lt;li&gt;Works well with Bitmap indices&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Insert into Hustle&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Distributed inserts - support massive write overload(millions recors/sec)&lt;/li&gt;
&lt;li&gt;Two steps:&lt;ul&gt;
&lt;li&gt;Create standalone Marble file (LMDB)&lt;/li&gt;
&lt;li&gt;Push Marble to DDFS (cheap, cluster-to-local)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;An insert can create and push any number of partitioned Marbles.&lt;/li&gt;
&lt;li&gt;CSV, JSON, fixed field supported&lt;/li&gt;
&lt;li&gt;Bulk append semantics - pay careful attention to ETL.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Query Hustle&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Python DSL - operator overloads in where clause&lt;/li&gt;
&lt;li&gt;Aggregating Functions/ Grouping (h-sum, h-count, h-avg)&lt;/li&gt;
&lt;li&gt;Efficient, distributed join&lt;/li&gt;
&lt;li&gt;Order, limit, desc, distinct&lt;/li&gt;
&lt;li&gt;Nested queries&lt;/li&gt;
&lt;li&gt;Extensible query language&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Hustle Query Execution&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Hustle queries are distributed Disco pipeline jobs&lt;/li&gt;
&lt;li&gt;Query optimizer and replication minimize data movement in cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Pointers&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/chango/hustle"&gt;Source Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Socialite Python Integrated Query Langauge for Big Data Analysis&lt;/h3&gt;
&lt;p&gt;Presenter: Jiwon Seo  &lt;/p&gt;
&lt;p&gt;Another big data processing framework and its query language could be integrated into Python
seamlessly as you could use the variables of Python in the query language or vice versa.
It claims to be faster than Graphlab and Not necessarily than Spark I guess,
it is memory-in computation, so in that aspect it differs from Hadoop-based frameworks.&lt;/p&gt;
&lt;h4&gt;Why Another Big Data Platform?&lt;/h4&gt;
&lt;p&gt;Problems in existing platforms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Too difficult(low-level primitives)&lt;/li&gt;
&lt;li&gt;Inefficient(not network bound)&lt;/li&gt;
&lt;li&gt;Too many (sub) framework&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Introducing Socialite&lt;/h4&gt;
&lt;p&gt;Socialite is a high level language:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Easy and efficient&lt;/li&gt;
&lt;li&gt;Compiled to optimized code&lt;/li&gt;
&lt;li&gt;Python integration(Jython)&lt;/li&gt;
&lt;li&gt;Good for&lt;ul&gt;
&lt;li&gt;Graph Analysis&lt;/li&gt;
&lt;li&gt;Data Mining &lt;/li&gt;
&lt;li&gt;Relational Queries&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Outline&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Concepts in Socialite&lt;ul&gt;
&lt;li&gt;Distributed Tables&lt;/li&gt;
&lt;li&gt;Python Integration (Jython and Python)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;System overview&lt;/li&gt;
&lt;li&gt;Analysis Algorithms&lt;ul&gt;
&lt;li&gt;Shortest paths, PageRank&lt;/li&gt;
&lt;li&gt;K-Means, Logistic Regression&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Evaluation&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Distributed In-Memory Tables&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Primary data structure in Socialite&lt;/li&gt;
&lt;li&gt;Column oriented storage&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Python Integration&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Socialite queries in Python code&lt;ul&gt;
&lt;li&gt;Queries are quoted in backtick&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Python functions, variables are accessible in Socialite queries.&lt;/li&gt;
&lt;li&gt;Socialite tables are readable from Python.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;This&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="n"&gt;Python&lt;/span&gt; &lt;span class="n"&gt;code&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;Foo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Foo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;41&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;answer&lt;/span&gt;&lt;span class="err"&gt;”`&lt;/span&gt;
&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="n"&gt;Python&lt;/span&gt; &lt;span class="n"&gt;variable&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;
&lt;span class="sb"&gt;`Foo[i](s) :- i=44,s=$func`&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="sb"&gt;`Foo[i](s)`&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;System Optimizations&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Custom Memory Allocator (temporary table)&lt;/li&gt;
&lt;li&gt;Optimized Serialization&lt;/li&gt;
&lt;li&gt;Direct ByteBuffer (network Buffer)&lt;/li&gt;
&lt;li&gt;Multiple network channels among workers&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Summary&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Distributed query language&lt;/li&gt;
&lt;li&gt;Integration with Python&lt;/li&gt;
&lt;li&gt;Easy and efficient&lt;/li&gt;
&lt;li&gt;Algorithms in Socialite&lt;/li&gt;
&lt;li&gt;Competitive Performance&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Pointers&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://socialite-lang.github.io/"&gt;Project page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Sun, 20 Jul 2014 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2014-07-20:work/notes/2014-07-20/pydata-silicon-valley-2014-part-2/</guid></item><item><title>PigPen, Hadoop, Pig, Clojure and All That</title><link>http://bugra.github.io/work/notes/2014-07-09/pigpen-hadoop-pig-clojure-cascading/</link><description>&lt;p&gt;&lt;img alt="Big Data vs me" src="http://i.imgur.com/dJ5nhHd.jpg" title="Me, When I see “big data” phrase" /&gt;
My humble contribution to &lt;a href="http://bigdatapix.tumblr.com/"&gt;ridiculous images on big data&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This post is about &lt;a href="https://github.com/Netflix/PigPen"&gt;PigPen&lt;/a&gt;, a library that Netflix
open sourced in the beginning of this year. Yet, in order to introduce the library, 
I covered some background namely Hadoop, Pig and Clojure which PigPen builds on top of 
those. If you want to jump right away to my rant on PigPen right away, scroll a bit.&lt;/p&gt;
&lt;h2&gt;Hadoop&lt;/h2&gt;
&lt;p&gt;Hadoop (a.k.a distributed data processing framework for counting words) built on 
top of idea of map-reduce, famous Google 
&lt;a href="http://static.googleusercontent.com/media/research.google.com/en/us/archive/mapreduce-osdi04.pdf"&gt;paper&lt;/a&gt; 
not only changed how we process data(it is a data processing framework after
all) but also create new perspectives on how we might think about data, what type of 
opportunities that it could provide, how can we use the data we collect. The latter
one, the implicit role on collected data usage has a quite profound effect on 
different type of skills(data wrangling,data analysis, scraping and so on), job 
types(suddenly you started to hear data engineers, data scientists ..) and also 
company types(data companies that just collect data and provide in a nice interface 
possibly through an API, data analytics company that creates nice visualizations 
and interfaces to interact with and learn from the data, big data consulting 
companies that provide consulting for big data technologies and so on). Do not get 
me wrong, I am not saying Hadoop is the &lt;em&gt;only&lt;/em&gt; cause for this change but it plays 
a strong role as it contributed to the change on how we might perceive the data.&lt;/p&gt;
&lt;p&gt;Hadoop adoption was great at first thanks to “big data” and its tireless marketers,
early adopters, media that repeatedly overloads the phrase, or maybe it was all 
because developers who want to learn another new cool technology. I am not sure at 
this point(I bet on “big data” gods, though). After some time, some researchers 
found out that &lt;a href="http://research.microsoft.com/pubs/163083/hotcbp12%20final.pdf"&gt;Noboy got ever fired for using Hadoop on a cluster&lt;/a&gt;
which is a skeptical view of Hadoop and tries to advocate the usage of memory as it
becomes cheaper and cheaper. If you are following big data space, they were almost
suggesting using Spark. However, even if the Hadoop popularity suffered a little bit,
its adoption did not seem to decrease over time. People who have big data, they 
still try to use Hadoop for their data processing system.&lt;/p&gt;
&lt;p&gt;However, one thing became clear after Hadoop became mainstream that nobody
really wanted to write vanilla map-reduce programs. Not only they were very low level
but also you need to write a lot of boiler plate for the things that are very common
in a data processing systems. They wanted more abstraction than what Hadoop 
provides. They wanted other language interfaces, too. Because writing Java has 
been one of the most fulfilling development experience said an experienced nobody 
developer.(By the way, Java is a fine programming language if not great if you ask
me.)&lt;/p&gt;
&lt;h3&gt;Abstractions, Abstractions, and more Abstractions&lt;/h3&gt;
&lt;p&gt;We need moar abstractions said one junior developer who spent two days to 
write a buggy combiner. Not only more abstractions, we need functional
programming language interfaces said another junior developer who loves Haskell. 
We need better workflows for common tasks said another developer. We already know 
SQL and hate it, why learning another new domain specific language and hate it 
said a senior developer. Later this developer would happily announce that he hates 
Hive as much as he hates SQL.&lt;/p&gt;
&lt;h3&gt;Software Abstraction Projects on Top of Hadoop&lt;/h3&gt;
&lt;p&gt;Software abstraction projects then developed in order to satisfy some of these needs.
The abstraction does not refer mere software abstractions and the concepts that 
Hadoop provides but also it refers how common tasks(counting words) are abstracted
in a way that minimal boilerplate code requires.(if a project requires one-liner for
counting words, then it wins.)&lt;/p&gt;
&lt;p&gt;Pig and Hive came into the play first. Pig provides a nice language and data processing 
flow for data processing tasks through User Defined Functions(UDFs). I write a little
bit about on Pig in &lt;a href="http://bugra.github.io/work/notes/2014-02-08/pig-advantages-and-disadvantages/"&gt;here&lt;/a&gt;
if you want to learn more.&lt;/p&gt;
&lt;p&gt;Hive’s adoption is surprisingly good, whose query language is very similar to SQL. 
The data analysts that write SQL queries start to write HiveQL queries to analyze
the data so the effort that requires for another new language became minimal. Instead
of learning a new language, it may make more sense for the companies that want to 
minimize their efforts and want to maximize the outcome of their employees. &lt;/p&gt;
&lt;p&gt;If I compare these two, I would much prefer Pig just because its language is better
than SQL. In general, I strongly believe that declarative style is not very good 
how you might want to interact with your data. Pig’s transformation based approach, 
especially the transformations as a first class citizen approach is very good in
this regard.&lt;/p&gt;
&lt;h2&gt;Cascading&lt;/h2&gt;
&lt;p&gt;What if you do not want to learn any new technology, but you will write code
that you write everyday, but magically it will process big data out of the box.
Cascading is a project that wants to abstract whole map-reduce flow flow and creates 
the pipeline for you out of the box. Cascading is the framework name and it is for
Java but it has quite a lot of interfaces for different languages as well, the most 
prominent is Scalding for Scala. It has also Clojure interface which is Cascalog and
PigPen could be considered as a competitor for Cascalog.&lt;/p&gt;
&lt;p&gt;It has a bunch of cool projects on top of it, I will mention in here briefly, but
if you are interested in check it on their website. Especially, Pattern seems quite
nice for Machine Learning models as it supports PMML and you could dploy your models 
tested on smaller data on big data seamlessly.&lt;/p&gt;
&lt;h3&gt;Pattern&lt;/h3&gt;
&lt;p&gt;It also has Pattern which combines Predictive Model Markup
Language(PMML) with Cascading to make the Machine learning deployment
much easier. This is very nice for a number of reasons but most
important reason is that if you evaluate your machine learning algorithm
for small dataset, then to apply the model into the production is quite
seamless. &lt;/p&gt;
&lt;h3&gt;Lingual&lt;/h3&gt;
&lt;p&gt;SQL for Cascading would be a good term to explain what Lingual is. It
provides a SQL interface for Hadoop, which makes it easier to interact
the data. This also aims to interoperability as you could make SQL
queries for big data in the same way you do in a much smaller scale. If
you are using any third party application on top of your data, you could
also connect through this interface as your big data supports for big
data as well. &lt;/p&gt;
&lt;h3&gt;Driven&lt;/h3&gt;
&lt;p&gt;It tries to visualize your data processing system on top of Cascading;
dependencies and other components. Similar to &lt;a href="https://github.com/Netflix/Lipstick"&gt;Lipstick&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Netflix approach&lt;/h4&gt;
&lt;p&gt;Instead of Cascading, Netlix seems to choose Pig as a data processing
backbone for processing data. Therefore, they seem to build their stack
on top of Pig. They put up Lipstick previously which is very similar to
Driven. Now, they are putting PigPen which is very similar to Cascalog
in spirit. Main(maybe not only) difference is the underlying data
processing environments.&lt;/p&gt;
&lt;p&gt;Cascading generally differs from Pig and Hive where you do not want to write quick-dirty
scripts but rather a production environment which you could deploy. Otherwise, if 
you want to do data ad-hoc data analysis Pig or even Hive might give better
solutions than what Cascading provides. Even for this reason, PigPen is a nice 
competitor for Cascalog in Clojure league. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are also Scoobi, Scrunch and Spark that you may want to consider for your
needs. Especially, if you have data that could be fit a good computer, Spark may be
more suitable. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Pig&lt;/h2&gt;
&lt;h3&gt;Twitter&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://www.umiacs.umd.edu/~jimmylin/publications/Lin_Kolcz_SIGMOD2012.pdf"&gt;Large Scale Machine Learning at Twitter paper&lt;/a&gt;
gives a good overview why Pig is good for Machine learning and data processing in
general.(If you are interested in the paper, its presentation is &lt;a href="http://videolectures.net/eswc2012_kolcz_twitter/"&gt;here&lt;/a&gt;) 
It basically says that Pig is the backbone of the processing framework where
you could use mighty Python libraries for scientific computing, data processing and
machine learning. As you have a general domain programming language that you could 
depend on via User Defined Functions(UDF’s), you could pretty much process you data 
however you want independent from the big data as the platform built on Pig already
handles query planning as well as Hadoop aspects. Only job is to write UDF’s and 
combine the processing steps in the Pig. Seems quite straightforward. Netflix also 
seems to adopt this approach and I find particularly very good for machine learning 
and data processing in general. I also wrote  my rants on these issues as well;
&lt;a href="http://bugra.github.io/work/notes/2014-02-08/pig-advantages-and-disadvantages/"&gt;previously on Pig&lt;/a&gt;
and on the &lt;a href="http://bugra.github.io/work/notes/2014-02-09/pig-not-so-foreign-language-paper-notes/"&gt;original paper&lt;/a&gt; 
that explains the rationale behind Pig and some of its advantages and disadvantages
over vanilla map-reduce.&lt;/p&gt;
&lt;p&gt;I really like Pig and its approach in data processing as I wrote previously. However,
big data is a field that sees a surprisingly good adoption and growing community. 
Therefore, it is not unexpected that people actually try to push the field 
quite a lot, trying to come up better solutions for data processing problems. 
Pig is still being developed and see improvements, features(&lt;code&gt;cube&lt;/code&gt; and &lt;code&gt;roll&lt;/code&gt; 
functions) and bug fixes, which is great but what about its shortcomings?&lt;/p&gt;
&lt;h3&gt;Disadvantages of Pig&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Even very simple tasks(i/o) requires a lot of boilerplate code. Importing jar
files, registering macros, setting the parameters and so on.&lt;/li&gt;
&lt;li&gt;You need to maintain two and sometimes three different codebase(Java, Pig) or if
you are using Jython for UDF’s, add Python as well.&lt;/li&gt;
&lt;li&gt;All of the scripts that are wrapped in Pig is specific to processing jobs. Therefore,
even though UDF’s are reusable across different jobs, the component of jobs cannot 
be reusable. (Macros provide a limited mechanism to do so to be fair, though).&lt;/li&gt;
&lt;li&gt;Due to partially above reason, programs becomes scripts rather than full fledge
production workfklows. &lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Functional Programming&lt;/h1&gt;
&lt;p&gt;Functional programming is a paradigm where it puts an emphasis on functions which
are pure and evaluated as mathematical functions. That means for same input, they 
always yield same result. This is one of the reason why it is popular, as the 
functions do not have side effects, this leads cleaner, readable and more bug-free
programs. There are a lot of functional programming languages: Common Lisp, Clojure, 
Erlang, Haskell, Ocaml, F# to name a few. However, these languages also differ
quite drastically as some of the languages give more importance to types(Haskell)
whereas some others adopt more dynamic approach(Clojure). Some programming 
languages are not pure functional programming languages but provide various 
structures and methods to enable developers to write functional programming style: 
Scala, Python and C# could be examples of this category. &lt;/p&gt;
&lt;h2&gt;Clojure&lt;/h2&gt;
&lt;p&gt;Clojure is a modern a dialect of List programming language specifically targeted to
Java Virtual Machine(JVM). Rich Hikey who is the inventor and main developer of Clojure
explains the rationale why there is a need for clojure in &lt;a href="http://clojure.org/rationale"&gt;here&lt;/a&gt;
which I would like to replicate some of the core aspects in here as well:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Functional Programming Language&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Designed for Concurrency&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Immutable data structures + first-class functions&lt;/li&gt;
&lt;li&gt;Dynamic emphasis instead of strongly types(unlike Haskell)&lt;/li&gt;
&lt;li&gt;JVM is good platform and Java community and libraries could be useful.&lt;/li&gt;
&lt;li&gt;Write Java wherever you have to, extend it with Clojure wherever you can.&lt;/li&gt;
&lt;li&gt;Object oriented programming language is overrated.&lt;/li&gt;
&lt;li&gt;Inheritance is not only way to do polymorphism.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Also, some of the design principles and features are listed in 
&lt;a href="http://clojure.org/features"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Syntax&lt;/h4&gt;
&lt;p&gt;Since it is a dialectic of Lisp, and the language is itself homoiconic, the syntax
and also the layout of the programs tend to very similar. Most people hate parentheses
but coming from Python, even if it is not readable as Python, it is much more pleasant
than say Java or Perl where they exploit a lot of different non-characters to form
their syntax. This makes it also learn the syntax of Clojure piece of cake.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;def &lt;/span&gt;&lt;span class="nv"&gt;fibonacci&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;lazy-cat &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map + &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;rest &lt;/span&gt;&lt;span class="nv"&gt;fibonacci&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;fibonacci&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;take &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="nv"&gt;fibonacci&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;; Get first 10 fibonacci numbers&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;S expressions are good but they become quite hard to unread if you are doing step 
by step data processing. Consider the next example;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map str &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map inc &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter pos? &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range &lt;/span&gt;&lt;span class="mi"&gt;-5&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are getting a vector, filter out the negative values, increment the rest of the 
numbers and return string representation. However, it is hard to read. Especially,
if clojure would be useful for data processing, this becomes quite cumbersome, as
we want to abstract each step(i/o, extract-tranformation-load(etl), preprocessing,
processing). Clojure provides threading macro mechanism to handle this type of 
sequence processing.&lt;/p&gt;
&lt;h5&gt;Macros to rescue&lt;/h5&gt;
&lt;div class="highlight"&gt;&lt;pre&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;-&amp;gt;&amp;gt;&lt;/span&gt;
   &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range &lt;/span&gt;&lt;span class="mi"&gt;-5&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter &lt;/span&gt;&lt;span class="nv"&gt;pos?&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map &lt;/span&gt;&lt;span class="nv"&gt;inc&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map &lt;/span&gt;&lt;span class="nv"&gt;str&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The same expression as previous, yet much readable. Since it is also reversed order,
you could follow the execution order where in the previous representation you need
to go from the inner expressions to the outer expressions. What would be equivalent
of this processing in Python.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ii&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;ii&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;ii&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Definitely, very readable but not perfect(Pythonic, though).
Python supports functional programming to some degree, so we could use functional programming approach to tackle the same
problem.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)))))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We could improve the &lt;code&gt;lambda&lt;/code&gt; expressions if we abstract into functions similar
to clojure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inc_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)))))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;More readable and improved, yet it is not Pythonic and still suffers reading from
the inner expressions to outer expressions, hard to read:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;pos_vals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;inc_vals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inc_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pos_vals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;str_vals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inc_vals&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Generally, Python programmers write programs like these, use intermediate variables,
as it is much easier to debug, also much easier to read. One-liners are cool but not
readable nor maintainable. I choose this example because of both syntax similarities
as well as keyword names’ similarities of Python and Clojure for this particular
example.&lt;/p&gt;
&lt;p&gt;Clojure with threading macro provides a great mechanism, other languages Javascript
provides also a method chaining mechanism where you want to process data by cascading
methods in a readable fashion. Scalding (Scala implementation of Cascading) also 
provides a very similar syntax and feeling, if you are familiar, check that out as
well.&lt;/p&gt;
&lt;h1&gt;PigPen&lt;/h1&gt;
&lt;p&gt;&lt;img alt="PigPen" src="http://i.imgur.com/mhil2yS.png" title="PigPen Logo" /&gt;
PigPen combines the Clojure awesomeness namely functional programming, Lisp and JVM 
on top of Pig. This is quite awesome for a number of reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You write clojure, no Pig at all. I know I said Pig is great language but with
either Python or Java UDF’s, it is harder to maintain two codebase rather than one.
And for data processing, however I like transformations first approach of Pig, I 
much prefer Clojure’s functional programming style to deal with data.&lt;/li&gt;
&lt;li&gt;You get to use Clojure wherever you want in the processing seamlessly. Either that
could be a preprocessing step or it could be something that you used UDF’s for, it 
does not matter, use Clojure. Therefore, you are not limited to Pig functions at
all. Similar to UDF’s, except you are writing the UDF’s in the overall flow.&lt;/li&gt;
&lt;li&gt;Clojure has not been around for a long time and its library support is not great.
Therefore, the libraries that you want to use may not exist in clojure. However, do
not despair, JVM to rescue! If there is a library in Java, which 
does not exist for Clojure, import the Java library inside of Clojure. Clojure core 
exploits a number of Java primitive functions like &lt;code&gt;Integer/valueOf&lt;/code&gt; to get integer
representation of a string. JVM works for Clojure, exploit it wherever you can without paying the burden of writing Java.&lt;/li&gt;
&lt;li&gt;You could write composable programs since Clojure provides much nicer abstractions
than Pig. You could write general purpose functions and reuse all the time, e.g 
loading data:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;input-data&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;input-path&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;pig/load-tsv&lt;/span&gt; &lt;span class="nv"&gt;input-path&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;Pig becomes a library that you could just import in the dependencies(with Leiningen)
this is straightforward. (If you are using Jython for Python UDF’s, jars, dependencies become a nightmare after some time.) To be able to use it, only the following lines are needed in 
&lt;code&gt;project.clj&lt;/code&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="ss"&gt;:dependencies&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="nv"&gt;org.clojure/clojure&lt;/span&gt; &lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="mf"&gt;1.6&lt;/span&gt;&lt;span class="nv"&gt;.0&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
               &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;com.netflix.pigpen/pigpen&lt;/span&gt; &lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="nv"&gt;.6&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Remaining is handled by mighty &lt;strong&gt;Leiningen&lt;/strong&gt; for you.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It just works. Just works.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yes, there are no jar files, no external binary files. Only one line states PigPen
dependency and that is it. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pig is actually another library that you import and you could use Pig functions
wherever you need to. Pig functions could be called after importing PigPen:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;require&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;pigpen.core&lt;/span&gt; &lt;span class="ss"&gt;:as&lt;/span&gt; &lt;span class="nv"&gt;pig&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Pig functions are good old Pig functions with very-to-no differences between actual
Pig functions. 
7. Scripts are not specific to jobs, they are regular functions that could use other
functions. See the example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;process-data&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;input-path&lt;/span&gt; &lt;span class="nv"&gt;output-path&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
     &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;input-data&lt;/span&gt; &lt;span class="nv"&gt;input-path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
     &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;process&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
     &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;pig/dump&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
     &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;spit&lt;/span&gt; &lt;span class="nv"&gt;output-path&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;where &lt;code&gt;process&lt;/code&gt; is your awesome logic that includes different transformations and 
does the heavy processing to your data.
Reuse in this example should be obvious, and all of the logic is in the process 
that could change independently from the i/o. Orthogonality is increased. You 
could further abstract input and output if you want. This could be done in Pig at 
some level using Macros to be fair. But not this clean and not this 
straightforward. This is pure beauty.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Wed, 09 Jul 2014 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2014-07-09:work/notes/2014-07-09/pigpen-hadoop-pig-clojure-cascading/</guid></item><item><title>PyData Silicon Valley 2014</title><link>http://bugra.github.io/work/notes/2014-05-12/pydata-silicon-valley-2014/</link><description>&lt;p&gt;&lt;img alt="Pydata" src="http://i.imgur.com/tInmSM1.png" title="PyData" /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;After every Python-related event, I keep telling myself that other language people have &lt;strong&gt;no idea&lt;/strong&gt; what they are missing. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yes, we have GIL(Global Interpreter Lock), yes unicode is painful especially if you are doing a lot text-related stuff, yes Python is slow, yes static typing may prevent most of the bugs I (people) have, yet Python community is increasing day by day. And last week, PyData SV 2014 was no exception.  &lt;/p&gt;
&lt;p&gt;I attended all three days(tutorials + conferences), maybe not surprisingly most talks were about tooling and data infrastructure except tutorials. Even if web development in Python will decrease over time(we have no reason to believe so), Python seems to be a big player in data in future. Even in "Big Data", people who want to write Python could get away with writing Python thanks to a bunch of ports for Python. This is also a huge gain for data processing in Python in general.  &lt;/p&gt;
&lt;p&gt;In Sunday talks, most of the talks especially before noon is about how to speed up the Python. Most people do not want to write Javascript but they needed it in the browser, so they came up with a bunch of languages that compiles to Javascript; Typescript, Coffeescript, Dart, Clojurescript ... For Python, most people want to write Python, and they came up with a bunch of interesting things to speed up Python; static compilers, JIT, numba, parakeet and so on. However, I think it is time to confess also one of the disadvantage of Python that it is slow. Otherwise, people &lt;strong&gt;should not&lt;/strong&gt; come up these solutions to overcome its limitation. Similarly, if Javascript were a nice language, people should not invent all those languages in order not to write Javascript to write Javascript.(this sentence makes sense when you read twice) Before provacating a language war which definitely Python will win, let's look at the tutorials and conferences notes that I took.  &lt;/p&gt;
&lt;h1&gt;Friday (Tutorials)&lt;/h1&gt;
&lt;h2&gt;IPython 2.0&lt;/h2&gt;
&lt;p&gt;Presenters: &lt;strong&gt;Brian Granger&lt;/strong&gt; and &lt;strong&gt;Jonathan Frederic&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;They introduced IPython and IPython notebook with a beginner level and then showed what changed 2.0 and whatnot. I was using IPython 1.0 prior to this event and now I am using IPython 2.0 which brings the modal structure to the table. If you are a Vim guy like me, that is just perfect. &lt;code&gt;Edit&lt;/code&gt; and &lt;code&gt;normal&lt;/code&gt; mode is very similar to Vim. Directories are also easy to change in the ipython notebook. If the modal environment is the first thing that they changed in IPython 2.0, the second thing is the interactivity. &lt;code&gt;@interactive&lt;/code&gt;  and &lt;code&gt;@interact&lt;/code&gt; decorators provide an interactive environment on top of Ipython notebook. &lt;/p&gt;
&lt;p&gt;Some of the things I did not like: adding cell on top of some cell is only done shortcut(&lt;code&gt;a&lt;/code&gt; where you are not in edit mode), you cannot do with UI. This is not a disadvantage per se for people who are already using notebook, but for beginners with this modal structure, I would expect some people have quite difficult time to figure out &lt;em&gt;hows&lt;/em&gt;. Considering Python 2 to 3, I wonder Python sacrifices easy entry in order to empower current users.(Some people end up learning Ruby because of this incompatibility problem).&lt;br /&gt;
On my point of view, if I did not attend this tutorial, my switch to Ipython notebook may get delayed even further. Now, quite happily using the most of the shortcuts, notebook feels quite like Vim. (if only the shortcuts were the same!, we should have &lt;code&gt;:w&lt;/code&gt; or at least &lt;code&gt;/&lt;/code&gt; seriously)&lt;/p&gt;
&lt;h3&gt;Ipython Notebook&lt;/h3&gt;
&lt;p&gt;If data science has anything to with other than software engineering, it is IPython notebook. Software engineers live on raw code whereas data scientists, social scientists and experimental people need richer representations to understand their experiments, conclusions and what data tell in general. Software engineers can get away with code and comments whereas other people need paragraphs, images, audios, videos and graphs for their experiments. They need to also communicate their findings to other people(not necessarily technical people). This gap is filled with Ipython notebook quite successfully. It is so successful that most of the presenters(either in tutorial or talks) opened an ipython notebook or references an ipython notebook in their talks in the conference. If an ecosystem needs a great environment to live, data people in Python have such environment thanks to IPython and notebook.&lt;/p&gt;
&lt;h2&gt;Gradient Boosted Regression Trees in scikit-learn&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Peter Prettenhofer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Peter showed how awesome gradient boosted regression trees are. Apart from their base function(piecewise functions) which I did not like one bit, they have quite nice properties such as robust to noise, handle categorical and scale different features with ease. His &lt;a href="http://nbviewer.ipython.org/github/pprett/pydata-gbrt-tutorial/blob/master/gbrt-tutorial.ipynb"&gt;IPython notebook&lt;/a&gt; covers not just regression trees but also low bias-high variance, high bias-low variance (deviance plot in the notebook) and parameter search to optimize the parameters. They are all in Scikit-learn. I took(read stole) one of his ideas around when a robust nonparametric tries to learn a function over the observations, one does not need to worry about the outliers and noise. This idea could be exploited in outlier detection as well. Instead of looking the problem as outlier detection, we could designa  robust regression model and exclude the ones(outliers) that model gives low probability. I used Gaussian processes in this &lt;a href="http://bugra.github.io/work/notes/2014-05-11/robust-regression-and-outlier-detection-via-gaussian-processes/"&gt;post&lt;/a&gt; to do so. 
He put up all of the tutorial material(slides+notebook) in &lt;a href="https://github.com/pprett/pydata-gbrt-tutorial"&gt;here&lt;/a&gt;. &lt;/p&gt;
&lt;h2&gt;Designing and Deploying Online Experiments with PlanOut&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Eytan Bakshy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recently, Facebook opensourced their online experiment framework &lt;a href="http://facebook.github.io/planout/"&gt;&lt;em&gt;Planout&lt;/em&gt;&lt;/a&gt;. If you read the &lt;a href="http://hci.stanford.edu/publications/2014/planout/planout-www2014.pdf"&gt;paper&lt;/a&gt; behind of the framework, this is a response of many of the shortcomings of the online experiments. &lt;/p&gt;
&lt;p&gt;Separation of the experiment from production code is the biggest advantage and also firing up different experiments in different timeline for different "segments" seems quite nice. With Python syntax and what it provides out of the box, it seemed quite easy to use. But need to experiment a little bit to see its advantages. Some of the features(GUI) are missing in the open-source release where paper shows also some implementation detail around that. &lt;/p&gt;
&lt;p&gt;He put up all of the ipython notebooks in &lt;a href="https://github.com/facebook/planout/tree/master/contrib/pydata14_tutorial"&gt;here&lt;/a&gt;, if you get the source code of the Planout, you would have this material as well.  &lt;/p&gt;
&lt;h2&gt;Know Thy Neighbor: An Introduction to Scikit-learn and K-NN&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Portia Burton&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;She gave an overview of what machine learning is and introduced Scikit-Learn. She also gave an overview of K Nearest Neighbor method and showed a demonstration on the Iris flower data set. &lt;/p&gt;
&lt;p&gt;She put up all of the material(slides+notebooks) &lt;a href="https://github.com/pkafei/Know_Thy_Neighbor"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;K-means Clustering with Scikit-Learn&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Sarah Guido&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;She put a strong emphasis on parameter selection on K-Means as parameter selection for unsupervised learning algorithms are ill-defined. She introduced &lt;a href="http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient"&gt;Silhouette Score&lt;/a&gt; which I did not know before. The score is somehow related to both intra-variance and inter-variance cluster. She put up &lt;a href="https://github.com/sarguido/k-means-clustering"&gt;IPython notebook&lt;/a&gt; and &lt;a href="http://www.slideshare.net/SarahGuido/kmeans-clustering-with-scikitlearn"&gt;slides&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These are the ones that I attended. &lt;/p&gt;
&lt;h3&gt;ggplot&lt;/h3&gt;
&lt;p&gt;I really also wanted to attend ggplot for Python but I attended Planout instead. However, yhat did an awesome job to put all of the notebooks in the &lt;a href="http://blog.yhathq.com/posts/facebook-ggplot-tutorial.html"&gt;web&lt;/a&gt; and also to &lt;a href="https://github.com/glamp/ggplot-tutorial/"&gt;github&lt;/a&gt;. Notebooks are quite amazing. They are not just for &lt;strong&gt;ggplot&lt;/strong&gt; per se but for general workflow for data analysis with an emphasis on explaratory data analysis. They are also putting really nice ipython notebooks on their blog, I highly recommend following their blog if that type of data analysis seems interesting to you.&lt;/p&gt;
&lt;p&gt;Now, the talks. First, on saturday talks, I was more sane and motivated. That motivation somehow decreased on Sunday. So Sunday talks, it is not you, it is me. Let's get started.&lt;/p&gt;
&lt;h1&gt;Saturday Talks&lt;/h1&gt;
&lt;h2&gt;Why Python is awesome for any scale data?&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Burc Alpat&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;This talk was about how and why Python should be used in the data processing environments and how Facebook adopts the Python as one of their main language. He also gave various pointers to other talks. Mainly focusing on the strengths of Python and whenever it feels it is not the best option, use something else and try to connect with Python.  &lt;/p&gt;
&lt;h3&gt;Why Python for Machine Learning?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;You want to get results quickly&lt;/li&gt;
&lt;li&gt;You will need lots of functionality&lt;/li&gt;
&lt;li&gt;C++&lt;/li&gt;
&lt;li&gt;DSL&lt;/li&gt;
&lt;li&gt;You cannot do it all by yourself.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Some pointers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://vimeo.com/63269736"&gt;Scaling Machine Learning in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://speakerdeck.com/ogrisel/scaling-machine-learning-in-python"&gt;Scaling Machine Learning in Pyton Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebook/sparts"&gt;Python Sparts by Facebook using Thrift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://prestodb.io/"&gt;PrestoDB by Facebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dropbox/PyHive"&gt;PyHive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://rocksdb.org/"&gt;RocksDB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://code.google.com/p/leveldb/"&gt;Leveldb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebook/tornado"&gt;Tornado from Facebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that Dataswarm is not open source.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;C/C++ and language interoperability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.boost.org/doc/libs/1_55_0/libs/python/doc/"&gt;Boost.Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://oicweave.org/"&gt;Weave&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.swig.org/"&gt;Swig&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Build Tools&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://facebook.github.io/buck/"&gt;Buck&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.facebook.com/notes/facebook-engineering/buck-how-we-build-android-apps-at-facebook/10151454619998920"&gt;Buck in Android&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Dependency Managements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://pantsbuild.github.io/howto_develop.html"&gt;Pants from Twitter&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are mostly dependency management systems where if you do not want to deal with dependency hell, you may want to use some sort of dependency management system.   &lt;/p&gt;
&lt;p&gt;&lt;a href="http://pantsbuild.github.io/index.html"&gt;Pants&lt;/a&gt; seems quite useful. If you are doing interoperation with C/C++, get a build system instead of depending on the &lt;code&gt;venv&lt;/code&gt; and &lt;code&gt;pip&lt;/code&gt; which seems not working quite well with other languages.&lt;/p&gt;
&lt;h3&gt;DSL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DSL's can be useful for speaking different languages.&lt;/li&gt;
&lt;li&gt;Dataswarm is a dependency graph description language.&lt;/li&gt;
&lt;li&gt;Python is good for DSL's as well.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;A Full Stack Approach to Data Visualization: Terabytes (and Beyond) at Facebook&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Jason Sundram&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This talk is a purely tooling talk where Jason introduced different visualization libraries and told how Facebook is using visualization for data. Their setup is quite interesting, they are using Tornado for framework and a bunch of different js libraries based on D3 that reveals multi aspects of data they have. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fresh Data&lt;/li&gt;
&lt;li&gt;More pixels&lt;/li&gt;
&lt;li&gt;Make the data smaller&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;CrossFilter&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://square.github.io/crossfilter/"&gt;CrossFilter&lt;/a&gt; in browsing D3 based visualization library.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.rusty.io/2012/09/17/crossfilter-tutorial/"&gt;CrossFilter Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://bl.ocks.org/milroc/raw/6316349/#0"&gt;CrossFilter Slides Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;In memory database&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://db.disi.unitn.eu/pages/VLDBProgram/pdf/industry/p767-wiener.pdf"&gt;Scuba Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Some other Pointers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Tornado + gzip + csv&lt;/li&gt;
&lt;li&gt;&lt;a href="http://square.github.io/cubism/"&gt;Cubism.js&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://bost.ocks.org/mike/cubism/intro/#0"&gt;Cubism.js Slides by Mike Bostock&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Different touchpoints require asynchronous api calls. Tornado provides asynchronicity out of the box, very convenient.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nickqizhu.github.io/dc.js/"&gt;DC.js&lt;/a&gt; is a multidimensional
  javascript library.&lt;/li&gt;
&lt;li&gt;These tools are mostly for multidimensional aspects of the data. Not necessarily a specific part of it, but rather for different aspects of it.&lt;/li&gt;
&lt;li&gt;For GeoVisualization: &lt;a href="http://leafletjs.com/"&gt;Leaflet.js&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://zeromq.org/"&gt;ZeroMQ&lt;/a&gt; for queing.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Thrift usage is all of the talks in Facebook, should be very important for internal api talks between different services. Its polyglot nature should provide a very nice common structure where different languages talk to between each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;To make a successful "big data" visualization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Big =&amp;gt; small&lt;/li&gt;
&lt;li&gt;Real-time&lt;/li&gt;
&lt;li&gt;More Pixels&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Dark Data: A Data Scientist's Exploration of the Unknown&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Rob Witoff&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This was one of those talks how we use Pyhon. Ipython notebook unsurprisingly all over the talk. They built a private notebook viewer where people could share their notebooks across NASA, sounds super useful.(Do first that SQL query, then process in such a way, then run the algorithm in the x directory, sounds familiar? Notebook solves that problem) &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Even NASA depends on AWS for their data infrastructure. Maybe, this is not &lt;em&gt;that&lt;/em&gt; surprising.&lt;/li&gt;
&lt;li&gt;Also the sharing data and visualizing data to understand is another aspect of his presentation. &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Visualization Tools and Pointers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/wrobstory/vincent"&gt;Vincent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/wrobstory/vincent/blob/master/examples/Vincent_Examples.ipynb"&gt;Vincent Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;They also use Rickshaw for some of other visualizations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Liberate your dark data&lt;/li&gt;
&lt;li&gt;Enable your engineers&lt;/li&gt;
&lt;li&gt;Grow data scientists&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If we cannot learn all of our data, we cannot imagine what we know beyond of our imagination.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;DataPad: Python-powered Business Intelligence&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Wes McKinney&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;He mainly talked about their product Datapad, browser based analytics tool. He also talked about some of the shortcomings of current data analysis tools. &lt;/p&gt;
&lt;h3&gt;How to speed things up?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Badger analytics engine. (it is not open source, though).&lt;/li&gt;
&lt;li&gt;Purpose-built in-memory analytical query processor&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;What they are using?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Query routing&lt;/li&gt;
&lt;li&gt;Load balancing&lt;/li&gt;
&lt;li&gt;comms: gevent + websockets&lt;/li&gt;
&lt;li&gt;comressed tabular data store&lt;/li&gt;
&lt;li&gt;external data source connectors&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Sentiment Classification Using scikit-learn&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Ryan Rosario&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In this talk Ryan gave an overview what sentiment classification means, gave pointers to the state of art which is unsurprisingly a recurrent neural network architecture (so-called "deep learning"). He did not go into this detail, in Facebook, they used Naive Bayes using the implementation of Scikit-learn. Their approach is gather a lot of labeled data, using bag of words as features and chi-squared feature selection to choose best features and Naive Bayes for classifier, they improved their sentiment classification on what they had previously. He also talked a little bit about their data infrastructure Dataswarm.&lt;/p&gt;
&lt;h3&gt;Sentiment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Some information around emotional state of the user.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Lexicon Based Approaches&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Wordnet&lt;/li&gt;
&lt;li&gt;Lexinet&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Machine Learning Based Approaches&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SVM and Naive Bayes are most frequenly used classifiers.&lt;/li&gt;
&lt;li&gt;Lexical features such as quantifiers and negtion&lt;/li&gt;
&lt;li&gt;Positive to negative words converted to a statistical measure.&lt;/li&gt;
&lt;li&gt;Sarcasm and humor cannot be captured from the beginning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Deep Learning (State of the Art)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stanford Stentiment Treebank&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial"&gt;Deep Learning Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf"&gt;Original Paper for Sentiment Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recursive Neural Tensor Network (RNTN). =&amp;gt; Recursive Neural Network&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Sentiwordnet Adaptation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://sentiwordnet.isti.cnr.it/"&gt;Opinion mining&lt;/a&gt; =&amp;gt; looks at the
  sentences in the form of positivity, negativity and objectivity&lt;/li&gt;
&lt;li&gt;Lexical analyiss is very slow, Part of Speech Taggin is slow&lt;/li&gt;
&lt;li&gt;Not very accurate either&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Feature Selection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Chi-Squared Feature selection, for version 1, they used 1000s and then
  extended to millions. &lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Roc Curve to visualize the true-positive and false-positive system&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Python Ecosystem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dataswarm, Presto and Hive&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;System Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dataswarm provides the training/test set divide&lt;/li&gt;
&lt;li&gt;Feature selection, training, model, testing, then store the data MySQL
  or dashboard. Pretty straightforward workflow.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;With a ton of self-labeled data and basic algorithm overperforms
  lexical parsing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Using Python to Find a Bayesian Network Describing Your Data&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Bartek Wilczynski&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;He introduced &lt;a href="https://pypi.python.org/pypi/BNfinder/2.0.4"&gt;Bayesian Network Module in Python&lt;/a&gt; and mainly talked about Bayesian networks. The module seems quite easy to use from the command line as well, but for most of the libraries, need to experiment.&lt;/p&gt;
&lt;h3&gt;What is a Bayesian Network?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DAG =&amp;gt; Directed Acyclic Graph, without cycles&lt;/li&gt;
&lt;li&gt;Nodes for representing random variables&lt;/li&gt;
&lt;li&gt;Edges for representing dependencies&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;How can we find the Best Bayesian Network?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bayesian statistics (BDe)&lt;/li&gt;
&lt;li&gt;Information theoretic (MDL)&lt;/li&gt;
&lt;li&gt;Hypothesis testing&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Dynamic Bayesian Network&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Describe also temporal dependencies&lt;/li&gt;
&lt;li&gt;Causal links only go forward in time&lt;/li&gt;
&lt;li&gt;The breaks the problem of cycles as we now have two versions "before"
  and "after" variables&lt;/li&gt;
&lt;li&gt;&lt;a href="http://bioputer.mimuw.edu.pl/software/bnf/methods.php"&gt;Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://bioputer.mimuw.edu.pl/software/bnf/"&gt;Homepage for the Project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cytoscape.github.io/cytoscape.js/index.html"&gt;Visualization Library for Graph Theory and
  Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Up and Down the Python Data and Web visualization&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Rob Story&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The most interesting IPython notebook is in this conference, hands down. He introduced a comprehensive comparison of visualization libraries in Python. See his notebook in below. Especially, mpld3 seems quite interesting. However, no matter how Pythonistas try to make it work, if visualization depends on browser or more importantly interactivity, Javascript is nearly only solution. So, in the presentation, I was thinking maybe we should not try so hard to make Python to be Javascript and made peace with Javascript. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://wrobstory.github.io/"&gt;His Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wrobstory/pydatasv2014"&gt;Presentation Material&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/gist/wrobstory/1eb8cb704a52d18b9ee8/Up%20and%20Down%20PyData%202014.ipynb"&gt;Notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Seaborn&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Jointplot is very good if you want to visualize two variable distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Interactivity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If you want to create an interactive visualization, use &lt;code&gt;@interactive&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Especially, switching to prior distributions in the MCMC models, this is gem.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Other Pointer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Leaflet is for all of the geospatial visualization, Jason told the same library as well. Should be the &lt;em&gt;lingua franca&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data Analysis with SciDB-Py&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Chris Beaumont&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;He talked about SciDB which is built-in support for matrix operations in database. 
SciDB seems interesting. Think about numpy arrays but in harddisk and you could manipulate however you like. Since you can do linear algebra operations, it seems to be a perfect fit for big data as its memory usage is small. It does not have to load the whole matrix into the memory. &lt;/p&gt;
&lt;h3&gt;Some notes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;There is no Mac OSX support, yet.&lt;/li&gt;
&lt;li&gt;You could write numpy expressions to the database except its syntax is a little different&lt;/li&gt;
&lt;li&gt;Able to display the data directly from db is quite nice.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://github.com/Paradigm4"&gt;Github Repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Paradigm4/SciDB-py"&gt;SciDB-Py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Sunday Talks&lt;/h1&gt;
&lt;h2&gt;Speed without drag&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Saul Diez-Guerra&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;He introduced a number of libraries(static compilers, JIT, LLVMs) to speed up the Python. We need to first of course write good code and then try to compensate the inherent slowness of Python with those libraries.&lt;/p&gt;
&lt;h3&gt;Pointer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/diezguerra/pydata-sv-speed-talk"&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Pythran: Static Compiler for High Performance&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Mehdi Amini&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This presentation was also about how to speed up the Python. He introduced his library and gave promising benchmarks and showed some under the hood stuff to explain how they speed up Python. Since the coverage is quite large(except introspection and couple of small number of modules), they could improve the execution speed. &lt;/p&gt;
&lt;h2&gt;Data Science at Berkeley&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Joshua Bloom&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Core principles&lt;/li&gt;
&lt;li&gt;Academic pursuit or a skillset to be trained? =&amp;gt; nice questions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nice headers (Not necessarily orthogonal to each other but rather
different aspects of tackling a problem):&lt;/p&gt;
&lt;h3&gt;Different Approaches&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data Driven or Theory Driven&lt;/li&gt;
&lt;li&gt;Bayesian vs Frequentist&lt;/li&gt;
&lt;li&gt;Parametric vs Nonparametric&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;They used &lt;code&gt;scipy.sparse&lt;/code&gt; package to speed the some of their computation up. They also used &lt;code&gt;PyMC&lt;/code&gt; for their research. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Pointers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.pythonbootcamp.info/schedule"&gt;Python Bootcamp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://mb3152.github.io/Graph-Growth/"&gt;Bayesian Network For Graphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1008.4686"&gt;Data Analysis Recipes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/pdf/1008.4686v1.pdf"&gt;Data Analysis Recipes Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/davidwhogg/DataAnalysisRecipes"&gt;Data Analysis Recipes Book&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Especiall Data Analysis Recipes paper is great, written in a sarcastic way and harshly criticizes the current/traditional methods in a Bayesian point of view. Need to definitely take a look at the book as well!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Real-time streams and logs with Storm and Kafka&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Andrew Montalenti&lt;/strong&gt;  and &lt;strong&gt;Keith Bourgoin&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;They talked about how they are parsing logs with Python and introduced a library they wrote (Streamparse) in house. They also explained how they architectured their system with Storm and Kafka. &lt;/p&gt;
&lt;h3&gt;Pointers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.parsely.com/slides/logs/notes/"&gt;Logs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.parsely.com/slides/logs/#9"&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.parsely.com/code/"&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.celeryproject.org/"&gt;Distributed Task Queue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://storm.incubator.apache.org/"&gt;Distributed Realtime Computation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://kafka.apache.org/"&gt;Distributed Messaging System&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/amontalenti/streams"&gt;Demo Repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Parsely/streamparse"&gt;StreamParse&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;How to build a SQL-based data warehouse for a trillion rows in Python&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Ville Tuulos&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;He talked about how they built a data warehouse for big data in Python. He also gave some pointers what they are using some of the visualization libraries. Some portion of the talk was about compression and efficiency of the compression using different methods.&lt;/p&gt;
&lt;h3&gt;Pointers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://tuulos.github.io/pydata-2014/#/"&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Low-latency, in-memory, fully-sql-compliant data warehouse&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/AdRoll/backbone.d3"&gt;Backbone.d3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;They used information theoretic approach to compress the signal. &lt;/li&gt;
&lt;li&gt;Efficient encoding brings the compression. Also, the data is somehow
sparse. &lt;/li&gt;
&lt;li&gt;It looks very much like numpy operation(very interesting how it overlaps with SciDB)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://archives.damiendebin.net/zzip/"&gt;Efficient compression library&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Run-length encoding&lt;/li&gt;
&lt;li&gt;Probabilistic Data Structures&lt;/li&gt;
&lt;li&gt;Huffman Encoding &lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;It seems very interesting to me that the methods that are used for video and image compression could be applicable. Not necessarily multimedia. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Outlier Detection in Time Series Signals&lt;/h2&gt;
&lt;p&gt;I gave this presentation and talked about outlier detection using Median Filtering, Fast Fourier Transform (read Discrete Fourier Transform) and Markov Chain Monte Carlo. &lt;/p&gt;
&lt;h3&gt;Pointers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://bit.ly/bugra-pydata-sv-2014"&gt;Presentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/bugra/pydata-sv-2014"&gt;Source of Presentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://bit.ly/outlier-fft"&gt;FFT and Median Filtering Blog Post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://bit.ly/outlier-mcmc"&gt;MCMC Blog Post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Dataswarm&lt;/h2&gt;
&lt;p&gt;Presenter: &lt;strong&gt;Mike Starr&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;He talked about Dataswarm framework and how they are using it. Mainly focusing on data infrastructure and communication between services.&lt;/p&gt;
&lt;h3&gt;Pipeline&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Actions&lt;/li&gt;
&lt;li&gt;Event Logging&lt;/li&gt;
&lt;li&gt;Data Pipelines&lt;/li&gt;
&lt;li&gt;Reports / Apps&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Dataswarm&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dependency graph description language(similar to &lt;a href="https://github.com/spotify/luigi"&gt;Luigi&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Dependency between tasks is much concise than Luigi. Luigi is quite verbose comparing to this tpe of dependency description.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Mon, 12 May 2014 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2014-05-12:work/notes/2014-05-12/pydata-silicon-valley-2014/</guid></item><item><title>Workflow Engine Comparison(First Impressions)</title><link>http://bugra.github.io/work/notes/2014-04-13/workflow-engine-comparison-first-impressions/</link><description>&lt;p&gt;I was looking at different options for workflow engines. I have
some experience in Oozie, little experience in Luigi and no experience
in Azkaban. In this post, I will try to give an overview of these
engines in terms of their advantages and disadvantages. Take my word
with a grain of salt(based on the experience I have with these tools),
though. &lt;/p&gt;
&lt;h3&gt;Crons do not scale(Surprise!)&lt;/h3&gt;
&lt;p&gt;If you have a lot of processes which manipulate, transform and write
data to database, you will sooner or later will face the limitations of
the cron jobs. You want to be able to handle failures, debug processes
and rerun the failed jobs. You want to have multiple scripts to run
based on data availability, data dependency and time-based scheduling.
You may want to also share the data workflow with many people where you 
cannot do any of the items with cron jobs. &lt;/p&gt;
&lt;h3&gt;What is sufficient?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Regular scheduling (depending on data availability and time-based)&lt;/li&gt;
&lt;li&gt;Workflow of multiple jobs&lt;/li&gt;
&lt;li&gt;You should be able to write workflows in the same way that you are
  writing programs&lt;/li&gt;
&lt;li&gt;You should handle the errors and failures gracefully&lt;/li&gt;
&lt;li&gt;Communication between cluster and client should be secure&lt;/li&gt;
&lt;li&gt;Community support should be very good &lt;/li&gt;
&lt;li&gt;Continuous integration(whenever you push to the master, it should
  adapt the changes(woohoo!))&lt;/li&gt;
&lt;li&gt;Testing should be supported out of the box&lt;/li&gt;
&lt;li&gt;Let's cut to the chase; pretty much anything that you want to expect from a  decent library or framework in terms of software quality and practices&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Let's write our own workflow engine&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Resources are limited(time, effort, human resources)&lt;/li&gt;
&lt;li&gt;Is your usage is too specific or could you make it work in one of the
  available tools?&lt;/li&gt;
&lt;li&gt;No need to reinvent the wheel&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;What do we want from the workflow engines?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;First and foremost: resilient to failures&lt;/li&gt;
&lt;li&gt;Debugging is necessary and important advantage over cron jobs&lt;/li&gt;
&lt;li&gt;If we have similar workflows, we should not write too much boilerplate
  code to make it all work&lt;/li&gt;
&lt;li&gt;Support for databases, HDFS and common file formats&lt;/li&gt;
&lt;li&gt;You should be able to write workflows in the same way that you are
writing programs&lt;/li&gt;
&lt;li&gt;You should handle the errors and failures gracefully&lt;/li&gt;
&lt;li&gt;Communication between cluster and client should be secure&lt;/li&gt;
&lt;li&gt;Community support should be very good&lt;/li&gt;
&lt;li&gt;Continuous integration(whenever you push to the master, it should
adapt the changes(woohoo!))&lt;/li&gt;
&lt;li&gt;Testing should be supported out of the box&lt;/li&gt;
&lt;li&gt;Default logging would be icing on the cake&lt;/li&gt;
&lt;li&gt;Let's cut to the chase; pretty much anything that you want to
expect from a decent library or framework in terms of software 
quality and practices&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Oozie&lt;/h2&gt;
&lt;h3&gt;Advantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Mature&lt;/li&gt;
&lt;li&gt;Support from Hadoop community is strong&lt;/li&gt;
&lt;li&gt;Documentation&lt;/li&gt;
&lt;li&gt;Default support for Pig, ssh, java, filesystem&lt;/li&gt;
&lt;li&gt;Coordinators: when data is available, do the computation. For
  recurring jobs, you do not need to explicitly configure the job flow.&lt;/li&gt;
&lt;li&gt;Security&lt;/li&gt;
&lt;li&gt;Built in authentication&lt;/li&gt;
&lt;li&gt;It has own testing suite(Mini Oozie) &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Disadvantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;XML(Verbose)&lt;/li&gt;
&lt;li&gt;Control flow is somehow restrictive&lt;/li&gt;
&lt;li&gt;Directed Acyclic Graph(Hard to rerun only a component after failure,
  perfectly goes along with Pig, though; Pig scripts also define DAG)&lt;/li&gt;
&lt;li&gt;User Interface&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Luigi&lt;/h2&gt;
&lt;h3&gt;Advantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python!&lt;/li&gt;
&lt;li&gt;Control flow is advanced as tasks are code&lt;/li&gt;
&lt;li&gt;Dependencies between flows&lt;/li&gt;
&lt;li&gt;Customization and code reuse through object-oriented programming&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Disadvantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Visualizer is not as good as Azkaban&lt;/li&gt;
&lt;li&gt;No default support for Pig, Hive &lt;/li&gt;
&lt;li&gt;No storage of history and generally persistent storage is lacking&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Azkaban&lt;/h2&gt;
&lt;h3&gt;Advantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If you are using Voldemort, it supports out of the box&lt;/li&gt;
&lt;li&gt;Visualizations for tasks (svg, interactive) is advanced&lt;/li&gt;
&lt;li&gt;Authentication and Authorization&lt;/li&gt;
&lt;li&gt;History of tasks(which are completed and which are not)&lt;/li&gt;
&lt;li&gt;Plugins for Pig, Hive and many more&lt;/li&gt;
&lt;li&gt;Web deployment&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Disadvantages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Support is not as good as Oozie.&lt;/li&gt;
&lt;li&gt;Scheduling is only time based. AFAIK, no data availability scheduling&lt;/li&gt;
&lt;li&gt;Workflow is somehow limited and restrictive comparing to Luigi and
  even Oozie. &lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Sun, 13 Apr 2014 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2014-04-13:work/notes/2014-04-13/workflow-engine-comparison-first-impressions/</guid></item><item><title>8th NYAS Machine Learning Symposium 2014</title><link>http://bugra.github.io/work/notes/2014-03-28/nyas-machine-learning-symposium-2014/</link><description>&lt;p&gt;I attended to &lt;a href="http://www.nyas.org/Events/Detail.aspx?cid=2cc3521e-408a-460e-b159-e774734bcbea"&gt;NYAS 8th Machine Learning Symposium&lt;/a&gt; and here are the notes
that I took from the event. It may contain errors and mistakes. If you
find any, please let me  know.&lt;br /&gt;
On personal view, it was worse than the previous machine learning
symposium(7th) in both posters and also talks. Last year, the posters and
talks were much more interesting to me. That being said, I could not
visit all of the posters so take my word with a grain of salt.  &lt;br /&gt;
The &lt;a href="http://www.nyas.org/asset.axd?id=defb6b86-f7ad-4a8d-af2a-3a5ef979c143&amp;amp;t=635314462602300000"&gt;abstracts in pdf&lt;/a&gt; in here. &lt;/p&gt;
&lt;h2&gt;Machine Learning for Powers of Good&lt;/h2&gt;
&lt;p&gt;by &lt;a href="http://www.rayidghani.com/"&gt;Rayid Ghani&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Probability of optimization of limited resources for a campaign. &lt;/li&gt;
&lt;li&gt;Another important thing: to influence the behavior of the voter and
  how to make them engage with the campaign.&lt;/li&gt;
&lt;li&gt;Because prediction itself is not good enough. &lt;/li&gt;
&lt;li&gt;Resource allocation based on who are likely to be influenced. Who are
  likely to change their mind? Definitely not Texas.&lt;/li&gt;
&lt;li&gt;Not voting to Romney and not going to vote, too much work. Do not try
  to event attempt to do anything.&lt;/li&gt;
&lt;li&gt;Focus on the ones either who are indecisive but likely to vote or
  indecisive about voting but weakly support Obama.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dssg.io/"&gt;Data Science for Social Good&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Following Spotlight Talks&lt;/h2&gt;
&lt;h3&gt;Graph-Based Posterior Regularization for Semi-Supervised Structured Prediction:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Posterior labels for part of speech tags. Graph-based approach,
  using Laplacian of Graph&lt;/li&gt;
&lt;li&gt;Structured Prediction =&amp;gt;  CRF =&amp;gt; local scope, features&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Graph-propagation and CRF estimation =&amp;gt; Joint objective, then to
optimize and look at the KL divergence as well for both world parameters.&lt;/p&gt;
&lt;p&gt;Relevant work is &lt;a href="http://jmlr.org/papers/volume11/ganchev10a/ganchev10a.pdf"&gt;Posterior Regularization(PR) Linear Ganchev&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;EM like algorithm =&amp;gt; to converge to the local optimum &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;She showed that it performs better than both CRF and Graph based approaches in her poster, but she did not compare speed of this approach with CRF or Graph based approaches. It is likely the method is slower than CRF but, I am not very familiar Graph based approaches and joint objective could be quite hard to optimize. So, the speed is could be much worse more than 2 times than CRF.&lt;/p&gt;
&lt;h3&gt;Learning from Label Proportions(LLP):&lt;/h3&gt;
&lt;p&gt;It attacks Binary learning problem with an extension of bag approach
where bags represent the ratio of the labels that are known but
individual labels are unknown. They try to solve the problem in a large
margin framework trying to model the instances belonging to a particular
label and try to increase margin with the other label(smv-like).
- Extension of supervised learning objective with Bag Proportion Loss
  with model parameters with a proportion loss.&lt;/p&gt;
&lt;h5&gt;Generalization Error of LLP&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Sample complexity of learning is proportional to bag proportion&lt;/li&gt;
&lt;li&gt;Instance label prediction error =&amp;gt; again depends on the prediction
  error&lt;/li&gt;
&lt;li&gt;Not only the supervised learning objective but also the bag
  proportions for the labeling matters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Generative Image Models For Visual Phenotype Modeling&lt;/h3&gt;
&lt;p&gt;They have genome types of fish and they have features of the fish. In
order to learn which genome type has effect on which fish trait, they
propose an admixture model which tries to correlate the traits and
genome. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Admixture model to correlate the shape variance between geneology of the
fishes.   &lt;/li&gt;
&lt;li&gt;Annotated genome from the shape variance of the fish.&lt;/li&gt;
&lt;li&gt;Genome annotates the features of the shape variance of the fish.&lt;/li&gt;
&lt;li&gt;Unsupervised learning of the features and joint generative model from
  fish variance and genome change. =&amp;gt; Seems quite novel.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Large Scale Learning - Scaling Graph-based semi supervised-learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Replace label vectors =&amp;gt; count-min-sketch? is a data
  structure(randomized) stores the counts of items. &lt;/li&gt;
&lt;li&gt;MAD exact vs Mad-Sketch =&amp;gt; comparison&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/parthatalukdar/junto"&gt;Junto Toolkit @ Github&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Structured Classification Criteria for Deep Learning for Speech Recognition (Second Keynote)&lt;/h2&gt;
&lt;p&gt;Before this talk, I knew that IBM is strong in deep learning(if I recall
correctly, they had a poster last year for speech recognition)  but
I did not know that they published also strong papers for speech
recognition last year. Google and Facebook get a lot of coverage for deep learning 
and that maybe rightly so, but IBM is also a strong player in the area.&lt;/p&gt;
&lt;h3&gt;Talk Structure&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Acoustic Modeling for speech recoognition:&lt;/li&gt;
&lt;li&gt;Structured loss function: we do not care about the loss function per
  se but how audible it is in the speech. Therefore, the loss function
should be structured around audibility of the speech.&lt;/li&gt;
&lt;li&gt;Optimization&lt;/li&gt;
&lt;li&gt;Speeding for training&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Bayesian Modeling for Speech Recognition&lt;/h4&gt;
&lt;p&gt;Sequence of phones are nice because  if have a word to
classify and you did not have that sample in the training set, you could "guess" the word from the sequence of phones.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Context affects the acoustic realization of a phone in the speech.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Context-dependent modeling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Condition on adjacent phones, infer the xcontext.&lt;/li&gt;
&lt;li&gt;Parameter sharing needed to mitigate data sparsity
 sahring must generalize to unseen contexts.&lt;/li&gt;
&lt;li&gt;Decision tree to get the AA-b, nasal, retroflex, fricative?, too much
  hand-engineered features.&lt;/li&gt;
&lt;li&gt;Basic speech sounds =&amp;gt; 1000 to 64K&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Structured Loss Functions&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Cross-entropy, for training criterion.&lt;/li&gt;
&lt;li&gt;A neural network with training error with the cross-entropy.&lt;/li&gt;
&lt;li&gt;We do not care about individual phones error but word error.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/Cross_entropy"&gt;Cross-Entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://mi.eng.cam.ac.uk/~wjb31/ppubs/hlt04_mbr_smt.pdf"&gt;Bayes-Risk Losses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hamming distance is a &lt;strong&gt;forgiving&lt;/strong&gt; distance measure for error between
  HMM sequences.&lt;/li&gt;
&lt;li&gt;To represent reference space, lattices generated via constrained
  recognition.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Stochastic Gradient Optimization&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Training GMM with is the fundamental idea.&lt;a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=4960445&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4960445"&gt;Stochastic Gradient Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Second Order optimization =&amp;gt; well researched&lt;/li&gt;
&lt;li&gt;Linear conjugate gradient minimizes a quadratic, which can be
  described by only matrix-vector products. Only linear time and memory are
necessary. &lt;/li&gt;
&lt;li&gt;CG is not necessary, truncated Newton is good enough.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.researchgate.net/publication/228619939_The_IBM_Attila_speech_recognition_toolkit"&gt;Hessian Free Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://research.microsoft.com/pubs/209355/NOW-Book-Revised-Feb2014-online.pdf"&gt;It has reference in this book,too.&lt;/a&gt; =&amp;gt; most probably, this may explain better.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Speeding Training&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester1201314/Sainath2013_lrdnn.pdf"&gt;Low-rank factorization of output weights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Word error rate generally drops as we increase the number of output
  targets =&amp;gt; factorization to reduce dimension?&lt;/li&gt;
&lt;li&gt;It gets faster.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Preconditioning in Sampling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/pdf/1309.1508v3.pdf"&gt;Accelerating Hessian Free Optimization Implicit Preconditioning and Sampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Geometric optimization reference is also in the above link.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Take-Home Mesages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Structured loss function instead of cross-entropy&lt;/li&gt;
&lt;li&gt;Stochastic gradient on a GPU is faster but distributed Hessian free
  optimization produces better models.&lt;/li&gt;
&lt;li&gt;Low rank factorization of the output weights&lt;/li&gt;
&lt;li&gt;Preconditioning and sampling&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_Martens10.pdf"&gt;Hessian-Free Optimization&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Learning Guarantees of the Optimization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Convex surrogate did not work&lt;/li&gt;
&lt;li&gt;You could learn an interesting loss function.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.google.com/presentation/d/1fZjE2aNMEoXprlvju_CrJETPylKrWZ5GzBYCj-GFSQc/edit#slide=id.g40599a40022e2fc114"&gt;Related
  Slides&lt;/a&gt;,
I could not catch the presenter in the poster.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Large Scale Machine Learning(Accelerated)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Machine learning as an optimization problem.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://yaroslavvb.blogspot.com/2014/03/stochastic-gradient-methods-2014.html"&gt;Stochastic Gradient Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://mrtz.org/blog/the-zen-of-gradient-descent/"&gt;Gradient Descent Method Blog Post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Fast Scalable Comment Moderation on NYT&lt;/h3&gt;
&lt;h4&gt;Active Learning at the New York Times&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Comment is split into two: metadata and n-grams.&lt;/li&gt;
&lt;li&gt;Hash the n-grams, score of the comment for human editor if it will be
  shown to her. Human moderators will work on whatever the algorithm
scores on high.&lt;/li&gt;
&lt;li&gt;20% =&amp;gt; workload reduction according to the plan&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Role of Optimization in Machine Learning&lt;/h2&gt;
&lt;h3&gt;Key Observations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stochastic Gradient Descent in the origin&lt;/li&gt;
&lt;li&gt;Batch Gradient Descent in one direction (between semi-stochatic
  approaches)&lt;/li&gt;
&lt;li&gt;Stochastic Newton Method in the other direction (between second-order
  methods)&lt;/li&gt;
&lt;li&gt;Other fields in Newton method&lt;/li&gt;
&lt;li&gt;Coordinate descent is towards simpler methods&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Optimal rate is achieved also for testing cos as long as each data
point is seen only once.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;To learn More about Stochastic Gradient Descent&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.eecs.berkeley.edu/~brecht/cs294docs/week1/09.Nemirovski.pdf"&gt;Nice paper for stochastic gradient methods&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stochastic Quasi-Newton Method performs quite well.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sorry about other spotlight talks, I am sure they were as interesting as
the ones above but I was only able to take notes and follow this much. &lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Fri, 28 Mar 2014 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2014-03-28:work/notes/2014-03-28/nyas-machine-learning-symposium-2014/</guid></item><item><title>Data Products Made Easy</title><link>http://bugra.github.io/work/notes/2014-03-21/data-products-made-easy/</link><description>&lt;p&gt;I read &lt;a href="http://www.oreilly.com/data/free/data-jujitsu.csp"&gt;Data Jujitsu&lt;/a&gt;
recently and enjoyed it a lot. DJ Patil presents a nice set of hard
learned things that he experienced in (mainly) Linkedin. I like this
type of &lt;strong&gt;real&lt;/strong&gt; experiences rather than a set of rules that advocates
a too ideal world and behave according to that world.  &lt;/p&gt;
&lt;p&gt;A recurring theme of one the short book is that divide and attack the
problems you have. The "Jujitsu" term comes from Japanese martial art
and suggests doing small attacks and trying to use the opponent's strength
towards her because the opponent (problem) is much stronger than you.
This naming actually reflects quite well what the short book is about.
Developing solutions to hard problems with limited human power and time.
What I found surprising a little bit is that, how consumer-oriented he
is throughout the book. He looks at the problem solving approximately in the following: if problems are not what customers want(enhancement in the product)
or what customers are unhappy about(bugs), then there must be some other
incentive(increase in engagement, better sales and so on) must exist so
that that problem would become &lt;strong&gt;worthy&lt;/strong&gt; to solve. So, solving a
problem for the sake of solving is not beneficial, neither for the
customer nor for the business. &lt;/p&gt;
&lt;h2&gt;Critical Question&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Does anyone want or need your product?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This question is so critical that I cannot stress how important it is.
Generally, the startups or businesses in general do not fail because 
they could not solve a problem technically but either they cannot create
enough incentive for people to buy the product or simply it is
irrelevant to the market. If you cannot answer this question with
confidence, however hard problem that you solved will not matter.
Eventually, if the product will be used, then someone needs or wants
your product.&lt;/p&gt;
&lt;h3&gt;Minimum Viable Product&lt;/h3&gt;
&lt;p&gt;If you answered the question above, then you are eligible to build your
minimum viable product. This does not have to be your fully functional
product but it should be a simple product that needs to prove that there is a
need and it should be &lt;strong&gt;good enough&lt;/strong&gt; so that you could determine if you
want to improve it or not. When you want to improve, ask these two
questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;"Does the customer care? Is there a market fit? If there isn't, there
   is no sense in building an application."&lt;/li&gt;
&lt;li&gt;"How long do we have to learn the answer to Question A?"&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When you prototype and measure the engagement of a user in the step 1,
you have a better sense of what you should build. Further, this also
ensures that you have a validity check that you are not building
something nobody wants it.&lt;/p&gt;
&lt;h2&gt;Definition&lt;/h2&gt;
&lt;p&gt;Go back for a second, what is a "data product" to begin with? He gives a
concrete definitiion for this question: &lt;br /&gt;
"Data product is a product that facilitates an end goal through the use
of data."  &lt;/p&gt;
&lt;h3&gt;Design of Product&lt;/h3&gt;
&lt;p&gt;Considering how much development, funding have been given towards big
data technologies or data analysis tools in general, data products would
have been easy, right? Not really.  &lt;/p&gt;
&lt;p&gt;Data is messy especially if you are getting data from a variety of
sources which do not have common interfaces. Data is messy if you are
collecting from input text fields that customers fill in. How do you
make sure that the data is in the right form? With product design. You
would provide feedback as Google search does ahead of the user, you
would prompt "did you mean ..." to help the user, you will arrange your
dropdown menu based on the input from the customer. This not only
provides much better experience for the user but also you get a much
better, structured data(think about dropdowns, support type-ahead for a
second) in your back-end. Patil presents this fact as: 
"I've found that trying to solve a problem on the back-end is
100-1000 times more expensive than on the front end".&lt;/p&gt;
&lt;h4&gt;Use humans when you have to, use technical solutions when you could&lt;/h4&gt;
&lt;p&gt;Generally, engineering seeks for technical solutions which are scalable.
This ensures that the solution will be profitable for high number of
users. However, when you try to be relevant in the market or try to see
if there is a market, you need to use humans. There is a similar problem 
&lt;a href="http://en.wikipedia.org/wiki/Cold_start"&gt;cold start&lt;/a&gt; which corresponds a
significant problem in the recommender system. Think about a user who
just signed your ecommerce startup, you want to recommmend things to him
but you do not have any history. Not only that, but you just launched
your product so you do not have any prior knowledge what people (in
general) like. If you had beta users or mechanical turks before you
launched your product, you are in luck. If the product is
consumer-facing, you should at least some data about your users from to
have a head start. Technical solutions are and will be more efficient,
cheaper and scale better in the long run, but if you cannot afford the
time and effort to build a technical solution to the problem, then do
not hesitate humans.&lt;/p&gt;
&lt;h4&gt;Always be opportunistic&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;If you are able to do data analysis to make the product better,
  increase the sales, just do it!&lt;/li&gt;
&lt;li&gt;If you cannot do some operation because you do not have resources or
  technical expertise, try do divide the operation and try to offer
simple version of it instead of providing nothing!&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Give the data back!&lt;/h4&gt;
&lt;p&gt;If your product is data-centric, you must be creating some value around
it and you should already providing the data to the customer in some
way. You should give it more! To both increase engagement and revenue,
give the data in an undertandable, clean and maybe even interactive way.
Let users play with the data. If your data is timely and
actioanable(think twitter for a sec), then it becomes addictive. Instead
of hoarding, share it. Only through sharing and giving back, you could
create more value around it.&lt;/p&gt;
&lt;h4&gt;But do not give it too much!&lt;/h4&gt;
&lt;p&gt;What Patil calls "Data Vomit" is that if you give too much data without
considering if it makes sense or valuable to the customer, there is a
good chance to overwhelm the user. So, confusion and frustration
replaces the engagement from the day one. There is a sweet spot where
more data generates more engagement and after that sweet spot, more data
will cause less interaction and engagement.&lt;/p&gt;
&lt;h4&gt;Consider non-ideal cases&lt;/h4&gt;
&lt;p&gt;If you are building a product, think about the extreme and edge cases as
well. Showing spanish pages to a tourist visiting Mexica just because
she is in Mexica may not make sense and especially if she repeatedly changes the browser language from spanish to english!&lt;/p&gt;
&lt;h3&gt;Precision and Recall&lt;/h3&gt;
&lt;p&gt;If you are building a retrieval system, first learn these concepts.
Then, find out what you want to compromise as these two generally work
against to each other. For a search engine, precision might be the
single most important metric whereas if you claim to be one of the most
comprehensive news source, you need to also increase your recall to be
consistent with your claim. Rule of thumb is that if the data is
exposed, first try to have a high precision because first page may be
the only page that your customer sees. If the precision is not good, it
may well be the last page.&lt;/p&gt;
&lt;h3&gt;Social system for the win&lt;/h3&gt;
&lt;p&gt;If your recommender system is terrible giving recommendations and
customers are unhappy about it, use collaborative filtering first and
blame the preferences of other users if customer is still not happy
about the recommendation. It is something that customer blames the product for
terrible recommendations, and it is completely different thing that
people are buying two incoherent things, so I am recommended that
product.&lt;/p&gt;
&lt;h3&gt;Get more data&lt;/h3&gt;
&lt;p&gt;Even if your domain is not advertising, knowing more about your users
always pay off. As you know more about her, you could recommend better,
you could personalize better, you could sell better, you could serve
better. Asking data if it is done correctly, it could be another way to
engage the customer as well. After you get the data, only goal would be
better product. Do not abuse it!&lt;/p&gt;
&lt;h3&gt;User is the most important&lt;/h3&gt;
&lt;p&gt;Features fail, products fail, nearly everyting in the universe at some
point fails. Get used to it, but try to preserve as much as user
experience you can in the process. Data products generally empower the
user in some way and there is a high chance that the experience will not
be constant through her time. Try not to decrease it too much. If the
ads that you show may be offensive, give an option to user so that when
she visits the website, she could just remove those type of ads(similar
to Facebook). If the people you recommend are not very relevant, provide
a way to user so that he could give negative feedback to people whom she
does not know so that she will not see them again(similar to Linkedin).
This not only gives control and value to user, in the process you could
learn user preferences and build a better product.&lt;/p&gt;
&lt;h3&gt;Three Fundamental Questions that you should ask to yourself&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;What do you want the user to take away from this product?&lt;/li&gt;
&lt;li&gt;What action do you want the user to take because of the product?&lt;/li&gt;
&lt;li&gt;How should the user feel during and after using your product?&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;If the core of your product gets it right the core and fundamentals
from the day one, you have a lot of time to improve it. Use Jujitsu! &lt;/p&gt;
&lt;/blockquote&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Fri, 21 Mar 2014 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2014-03-21:work/notes/2014-03-21/data-products-made-easy/</guid></item><item><title>Short Notes on Thinking with Data</title><link>http://bugra.github.io/work/notes/2014-03-18/thinking-with-data-short-notes/</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Arguments are how we convince ourselves that something is true.&lt;/li&gt;
&lt;li&gt;Argument theory provides a useful set of mental models for
  understanding arguments.&lt;/li&gt;
&lt;li&gt;Data work is held together by arguments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Data are just (sadly) observations.&lt;/li&gt;
&lt;li&gt;What we want is knowledge and representation, maybe even understanding
  and insight.&lt;/li&gt;
&lt;li&gt;By doing so, arguments move from what is unknown/not agreed upon yet.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Arguments are how we turn observations into knowledge.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Arguments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Claim&lt;/strong&gt; =&amp;gt; Audience does not believe it(yet).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prior Knowledge&lt;/strong&gt; =&amp;gt; Things you/your audience believes already
  before the case is made.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt; =&amp;gt; Where data enters into an argument. It becomes part of
  the argument, not just "data" anymore. It gains context in that sense.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Justification&lt;/strong&gt; =&amp;gt; Reasoning why evidence should cause vs to believe
  the claim.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rebuttal&lt;/strong&gt; =&amp;gt; Any of the reasons why the justification might not
  hold in this particular case.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Patterns of Arguments&lt;/h2&gt;
&lt;h3&gt;Template for Claims&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Categories of Dispute&lt;/li&gt;
&lt;li&gt;Causal Analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Causal Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Large set of patterns are necessary.&lt;/li&gt;
&lt;li&gt;Goal is to claim to have reasonably accounted for alternative
  explanations.&lt;/li&gt;
&lt;li&gt;People will interpret our work causally, we should try our best to
  make it reasonable to do so.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Conclusions&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Mental models make our lives and our work better.&lt;/li&gt;
&lt;li&gt;Arguments are a deep part of working with data.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Tue, 18 Mar 2014 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2014-03-18:work/notes/2014-03-18/thinking-with-data-short-notes/</guid></item><item><title>Document Visualization with JS Divergence Matrix and Multi Dimensional Scaling</title><link>http://bugra.github.io/work/notes/2014-03-16/jensen-shannon-divergence-matrix-multi-dimensional-scaling/</link><description>&lt;h3&gt;Bag of Words (BoW)&lt;/h3&gt;
&lt;p&gt;In text search and classification of text, word order contributes 
less to the search result or document classification unless it is part of 
a phrase. Therefore, it is a common practice to use the frequency
of occurrence of words sacrificing the word order which is known as "bag
of words". In this document representation method, document is converted
to vectors by simply counting number of occurrence of words. For
example, the following two sentences would have the same vector
representation:
"I am who I become" and "I become who I am"
as the frequency of occurrence of words are same even though the word
order is different. The rationality behind of this document
representation is that, the presence and count of words do matter more
than the word sequence in a sentence for classification. In practice,
this representation is "lingua franca" along with tf-idf representation.&lt;/p&gt;
&lt;h4&gt;Corpus&lt;/h4&gt;
&lt;p&gt;In BoW setup, corpus corresponds to the distinct number of words that covers all of the documents that we have in our dataset.&lt;/p&gt;
&lt;h4&gt;Stop Words&lt;/h4&gt;
&lt;p&gt;Words that do not contribute to the distinctiveness of either class.
They could occur quite a lot like (e.g. I, am, you, they) or occur
infrequently. The ones that occur a lot may prevent the vectors to
weight to the words that are distinct for particular class. The ones
that occur infrequently prevents us to represent the documents
efficiently and compactly as they make quite long corpus vectors.
Therefore, for text classification, we want to remove both the words
that occur infrequently and the words that occur a lot but do not
contribute to the distinctiveness of either class. A very useful
heuristics for these words are the most common (5% of the words that
occur most) and the least common(5-10% of the words that occur least in
the entire corpus). After removing these words, we would have a better
corpus and hopefully better vector representation for the documents for
each class. Note that, for a given dataset, due to nature of natural
language, the distribution of a typical corpus follows long tail
distribution. Therefore, even if the least common words would contribute
to the distinctiveness of the classes, there may be trade-off where we
gain some "information" incorporating the least common words and lose
the compactness of vector representation.(This brings &lt;strong&gt;curse of
dimensionality&lt;/strong&gt; problem which we would try to resolve by bringing a
dimensionality reduction method to the table.)&lt;/p&gt;
&lt;p&gt;After explanation of terms, let us give some concrete examples to
explain how this actually works:
consider following three sentences:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I am who I become&lt;/li&gt;
&lt;li&gt;I become who I am&lt;/li&gt;
&lt;li&gt;Who am I?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ignoring the punctuation and uppercase, we have the following corpus for
these three sentences:
['am', 'become', 'i', 'who'] &lt;br /&gt;
(note that the words are sorted by alphabetical order) Quite a large corpus!
Then the bag of words representation for the sentences are the
following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;[1, 1, 2, 1]&lt;/li&gt;
&lt;li&gt;[1, 1, 2, 1]&lt;/li&gt;
&lt;li&gt;[1, 0, 1, 1]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As previously mentioned, the first and second sentences would result in
the same bag of words representation even if the sentences are
different. One thing to note is that since the corpus is unrealistically
small, we did not stop any words but normally the contribution of
auxiliarry verbs and subject pronouns  is minimal, so it is common
practice to remove them alltogether. The default keyword list generally
includes these type of words independent from the domain for the
documents.&lt;/p&gt;
&lt;p&gt;It is easy to see that all of the documents would have the length of the
corpus where the word "distribution" of the sentences would differ if
the sentences are different(ignoring word order). 
This is all good but as we have previously mentioned, this brings dimensionality problems and lose the vector representation's efficiency and accuracy. 
This is due to the nature of bag of words representation, as all of the word frequency counts toward the vector, some of the not very informative words may overcome to the words that provides distinct character to the documents in a
classification perspective. In order to prevent this, we may want to use
a dimensionality reduction method. If the method preserves the relative
distance of individual vectors in euclidean space, then we not only gain
in terms of efficiency of the representation but also prevent curse of
dimensionality and a better document representation! But taking a step
back, how do measure the "distance" between two documents? Let's revisit
an information theory measure for comparison of two probability
distribution, namely K-L Divergence.&lt;/p&gt;
&lt;h2&gt;Kullback-Leibler Divergence&lt;/h2&gt;
&lt;p&gt;K-L divergence is a measure of the difference of two probability distributions, which is a special case of Bregman Divergence. For formal definition, if we have $p$ and $q$ discrete probability distributions, then the k-l divergence of these two distributions is defined as such:
$$ D_{KL}(p|q) = \displaystyle\sum_i ln(\frac{p(i)}{q(i)}) \hspace{2 mm} p(i) $$ 
Needless to say, this measure is not symmetric and is defined only $p$
and $q$ sum to 1 for all possible $i$ values. Moreover, for a particular
$i$, being $q(i)=0$ implies $p(i)$ to be $0$ as well, otherwise the whole
term would be undefined. Adopting this measure into our bag of words
representation to measure sentence difference brings us two problems.
First, we only have the counts of the words that occur in sentence, not
the distribution. Second, as sentences are only a small part of corpus,
there will be lots of $0$'s in the vector representation(would be a sparse
vector). In order to mitigate the first problem, we could divide the
counts of words in the vector by the total sum of the word count. By
doing so, we also make sure that we have two vectors whose values sum up
to 1. But second one is the hard one, how do we fill the 0's? Luckily,
this problem also comes up with in different domains and we have a
solution: &lt;em&gt;smoothing&lt;/em&gt;. There are a number of smoothing algorithms, but the
one that I used is Laplace Smoothing(also called Additive Smoothing) which I will explain in the next section. &lt;/p&gt;
&lt;p&gt;As mentioned before, this measure is not symmetric, but note that it is not
related to bag of words representation per se, so we will provide
another remedy for that after introducing the smoothing.&lt;/p&gt;
&lt;h3&gt;Laplace Smoothing&lt;/h3&gt;
&lt;p&gt;Behind of smoothing the distributions is to remove the $0$'s from the
distribution by introducing small amount of factor so that they will not 
be $0$ but also they will not have a large share in overall distribution. 
By doing so, we would mitigate the implausibility of $0$'s with very
small cost. Formally Laplace Smoothing is applied to distribution $p$ as
such:
$$ \hat{p_i} = \frac{p_i + \alpha}{1 + \alpha N} \hspace{5 mm} (i=1, \ldots, N) $$ where $\alpha$ is the smoothing parameter and $N$ is the number of discrete terms in the distribution.  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that without $\alpha$, $\hat{p_i}$ is $p_i$ as expected and
adding $\alpha$ for every parameter would not change the sum of the
distribution as in the denominator, we divide by $1 + \alpha N$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Very well, we removed our $0$'s and have "smooth" distributions. However, 
what kind of measure is this K-L divergence if it is not symmetric? 
Think about it one second, difference between sentence A and sentence B 
would be different than difference between sentence B and sentence A. How
ridiculuous is that! Luckily, we have a solution based on K-L
divergence; Jensen Shannon Divergence which will be explained in the
next section:&lt;/p&gt;
&lt;h2&gt;Jensen-Shannon Divergence&lt;/h2&gt;
&lt;p&gt;J-S Divergence builds itself on top of K-L divergence promising it is
symmetric; which is nice and quite popular measure of two probability
distributions. Formally it is defined for $p$ and $q$ distributions as
such:
$$ D_{JS}(p|q) = D_{JS}(q|p) = \frac{1}{2} D_{KL}(p|r) + \frac{1}{2} D_{KL}(q|r) $$
where $r = \frac{1}{2} (p+q)$.&lt;br /&gt;
Instead of going through getting K-L divergence of individual
distribution, J-S does this using a mixture distribution of $r$. Quite
smart, huh? By doing so, it ensures that it is symmetric and nice measure for probability distribution differences.  &lt;/p&gt;
&lt;h4&gt;J-S Matrix&lt;/h4&gt;
&lt;p&gt;Say we have $N$ documents and we constructed our J-S matrix using J-S
Divergence for each pair of smoothed bag of words of documents. This
would result in $NxM$ matrix where the $M$ is the size of our corpus as
every document is represented $1xM$ vectors. This matrix representation
would not solve our curse of dimensionality problem. So, let us
introduce a dimensionality reduction method(among many others);
Multidimensional Scaling.&lt;/p&gt;
&lt;h2&gt;Multidimensional Scaling (MDS)&lt;/h2&gt;
&lt;p&gt;Over-simplified idea behind MDS is that if we could find an embedding
which has a significantly lower dimension for high dimension space and
preserve the distance between observation pairs, we do not lose much in
relative sense since we are keeping the distance between the pairs.
Further, we both reduce dimensionality quite a lot and preserve the relative
distance to each other. This is somehow different than traditional
dimensionality reduction methods where they they generally scale
individual observations or all of the observations altogether. MDS seeks
to fit a lower embedding for observation pairs. Since the J-S divergence 
deals with document pairs, J-S Divergence matrix could be considered as a
dissimilarity matrix for the documents in that sense. Therefore, it is 
a perfect fit for MDS as MDS also tries to reduce the dimensionality of 
J-S measure between two documents. We want to find 2-dimension lower 
embedding in order to visualize the documents in a scatter plot but 
1-dimension works as well as any number of dimension could be used for MDS. 
Formally, loss function of MDS is defined as following:
$$ Loss_{x_1, \ldots, x_n} = (\displaystyle\sum_{i\neq j=1, \ldots, N} (D_{i,j} - \lVert x_i - x_j \rVert)^2)^{\frac{1}{2}}  $$ where $D$ is the dissimilarity matrix $Nxk$ where $N$ is the number of observations and $k$ is the dimension of the original space. We are not going to show how the minimization works, but for a lower embedding(say 2), the minimization function could be optimized through gradient descent.  &lt;/p&gt;
&lt;h3&gt;Result&lt;/h3&gt;
&lt;p&gt;After we apply MDS to our J-S divergence matrix, we get $x$-$y$ pair of
coordinates of each document and it looks like the following(for a
selected documents):
&lt;img alt="J-S Matrix of Documents with MDS" src="/images/work/notes/2014/3/16/documents_mds.png" title="Document Visualization on J-S Divergence Matrix and MultiDimensional Scaling" /&gt;
&lt;a href="/images/work/notes/2014/3/16/documents_mds.png"&gt;Original Image(Larger version)&lt;/a&gt;&lt;br /&gt;
Data is from Yelp's reviews of restaurantts and first three sentences are negative and mention about how disappointing their experience was with restaurant. The rest of them are generally positive and even if their experience may include some negativity, their overall experience is quite positive.&lt;br /&gt;
As you could see from the scatter plot, the grouping of these two
different class are somehow separate even though the documents are quite
short and overall corpus is only 8 sentences.  &lt;/p&gt;
&lt;h3&gt;Possible Improvement&lt;/h3&gt;
&lt;p&gt;We talked about stop words but curating stop words is manual and
laborious work. Further, it generally evolves to be domain specific as
you incorporate more and more stop words. Instead of using stopwords,
one may choose feature selection methods to choose the words from either
classification accuracy or some information theory measure from the
training dataset. This not only eliminates the stop words curation step
alltogether but also increase the efficiency and compactness of the bag
of words representation. As a result of this, when some dimensionality
reduction method is applied to the vectors, they would position
themselves better. &lt;/p&gt;
&lt;h3&gt;What is next?&lt;/h3&gt;
&lt;p&gt;An interactive app that allows user to type text and then visualize
every sentence in a scatter plot. I have a working version of this, but
it needs some polishing. &lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Sun, 16 Mar 2014 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2014-03-16:work/notes/2014-03-16/jensen-shannon-divergence-matrix-multi-dimensional-scaling/</guid></item><item><title>Pig Not So Foreign Language Paper Notes</title><link>http://bugra.github.io/work/notes/2014-02-09/pig-not-so-foreign-language-paper-notes/</link><description>&lt;p&gt;These are notes that I took from the &lt;a href="http://infolab.stanford.edu/~usriv/papers/pig-latin.pdf"&gt;paper&lt;/a&gt;, where the authors
explain the design principles and some theoretical aspects of Pig the
programming language. I gave a basic overview in &lt;a href="/work/notes/2014-02-08/pig-advantages-and-disadvantages/"&gt;Pig Advantages and
Disadvantages&lt;/a&gt;. &lt;/p&gt;
&lt;h2&gt;Before Pig&lt;/h2&gt;
&lt;p&gt;Before Pig and Hadoop, there was mighty Map-Reduce paradigm for
parallellization and data processin. The overview of data
processing before Pig as follows:&lt;/p&gt;
&lt;h3&gt;Map-Reduce Advantages&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Scale&lt;ul&gt;
&lt;li&gt;Scalable due to simpler design&lt;/li&gt;
&lt;li&gt;Only parallelizable operations&lt;/li&gt;
&lt;li&gt;No transactions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Runs on cheap commodity(cost) hardware&lt;/li&gt;
&lt;li&gt;Procedural control - a processing "pipe"&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Disadvantages&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Extremely rigid data flow(Map-Reduce)&lt;/li&gt;
&lt;li&gt;Common operations must be coded by hand(Join, filter, projection, aggregation)&lt;/li&gt;
&lt;li&gt;Semantics are hidden inside map-reduce functions&lt;ul&gt;
&lt;li&gt;Difficult to maintain, extend and optimize&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Need a high-level, general data-flow language.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Automatic query optimization is hard.&lt;/li&gt;
&lt;li&gt;Pig Latin does not preclude optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Big demand for parallel data processing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Emerging tools that do not look like SQL DBMS.&lt;/li&gt;
&lt;li&gt;Programmers like &lt;strong&gt;dataflow pipes&lt;/strong&gt; over static files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Building a High-Level Dataflow System on Top of MapReduce&lt;/h2&gt;
&lt;h3&gt;What is Pig&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Procedural dataflow language (Pig Latin) for Map-Reduce&lt;/li&gt;
&lt;li&gt;Provides standard relational transforms(group, join, filter, sort)&lt;/li&gt;
&lt;li&gt;Schemas are optional, if used, can be part of data or specified at run time.&lt;/li&gt;
&lt;li&gt;User defined functions are first class of citizens of the language.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Join implementations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Default is symmetric, hash join&lt;/li&gt;
&lt;li&gt;Fragment-replicate for joining large and small inputs.&lt;/li&gt;
&lt;li&gt;Merge join for joining inputs sorted on join key.&lt;/li&gt;
&lt;li&gt;Skew join for handling inputs with significant skew in the join key.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Hbase&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Solves appending problem in HDFS&lt;ul&gt;
&lt;li&gt;Low-latency query API&lt;/li&gt;
&lt;li&gt;rich, BigTable-style data model based on column families&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Advantages of Pig&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;High Level language&lt;/li&gt;
&lt;li&gt;Transformations on set of records&lt;/li&gt;
&lt;li&gt;Process data one step at a time&lt;/li&gt;
&lt;li&gt;UDF's are first class citizens.&lt;/li&gt;
&lt;li&gt;Easier than SQL.   &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Needs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Innovation at internet companies critically depends on being able to analyze terabytes of data collected every day.&lt;/li&gt;
&lt;li&gt;SQL could be unnatural, and hard to follow as it declarative.&lt;/li&gt;
&lt;li&gt;Map-Reduce paradigm is too low-level and rigid, and leads to a great deal of custom user code that is hard to maintain, and reuse.&lt;/li&gt;
&lt;li&gt;Engineers who develop search engine ranking algoriths spend much of their time analyzing search logs looking for exploitable trends.&lt;/li&gt;
&lt;li&gt;Map-Reduce: its one input and two-stage data flow is extremely rigid. &lt;/li&gt;
&lt;li&gt;Pig-Latin program is a sequence of steps, much of liek in a programming language, each of which carries out a single data transformation.&lt;/li&gt;
&lt;li&gt;The use of such high-level primitives renders low-level manipulations (as required in map-reduce) unnecessary.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;First, &lt;strong&gt;Pig Latin&lt;/strong&gt; is the programming language and &lt;strong&gt;Pig&lt;/strong&gt; is the data processing environment on top of Hadoop.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Pig Latin as a DataFlow Language&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;User specifies a sequence of steps where each step specifies only a single, high-level data transformation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Nested Data Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Programmers often think in terms of nested data structures.&lt;/li&gt;
&lt;li&gt;Databases on the other hand, allows only flat tables, i.e. only atomic fields as columns.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Why nested model is better?&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;A nested data model is closer to how programmers think, and consequently much more natural to them than normalization.&lt;/li&gt;
&lt;li&gt;Data is often stored on disk in an inherently nested fashion.&lt;/li&gt;
&lt;li&gt;A nested data model also allows us to fulfill our goal of having an algebraic language where each step carries out only a single data transformation.&lt;/li&gt;
&lt;li&gt;A nested data model allows programmers to easily write a rich set of user-defined functions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;UDFs&lt;/h3&gt;
&lt;p&gt;All aspects of processing in Pig Latin can be customized through User Defined Functions.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Sun, 09 Feb 2014 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2014-02-09:work/notes/2014-02-09/pig-not-so-foreign-language-paper-notes/</guid></item><item><title>Pig Advantages and Disadvantages</title><link>http://bugra.github.io/work/notes/2014-02-08/pig-advantages-and-disadvantages/</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Apache Pig is a dataflow language that is built on top of Hadoop to make
it easier to process, clean and analyze "big data" without having to write
vanilla map-reduce jobs in Hadoop.&lt;br /&gt;
It has also a lot of relational database features. Good old &lt;code&gt;join&lt;/code&gt;s, &lt;code&gt;distinct&lt;/code&gt;, &lt;code&gt;union&lt;/code&gt; and many more commands are already in the language. So what exactly Pig solves different than relational database is its applicability to "big data" where it can crunch large files with ease and it does not need a structured data. 
Contrarily, Pig could be used for ETL(Extraction Transformation Load)
tasks naturally as it can handle unstructured data. It is one of the
reasons why it exists to tell the truth. &lt;br /&gt;
But let's ask the fundamental question: &lt;strong&gt;Why does data analysis matter&lt;/strong&gt; ?&lt;/p&gt;
&lt;h3&gt;Data Analysis Matters&lt;/h3&gt;
&lt;p&gt;Data analysis matters because as &lt;a href="http://infolab.stanford.edu/~usriv/papers/pig-latin.pdf"&gt;original paper&lt;/a&gt; very good puts it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Data analysis is "inner loop" of product innovation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Companies which have data and "big data" want to automate some of their
processes, they want to make better products for their users, want to
create new products and platforms. If you do not happen to be Steve Jobs 
or someone who has natural insights of what users and consumers want
from the product or see new features, then you are dependent on data.
Feedback of users, their usage, log files of the website and metrics are
all things that make you run faster. They are not what you run with(it
is the product itself) but how you run faster. (So much for the analogy)   &lt;/p&gt;
&lt;p&gt;Pig paper also introduces the basic motivation for Pig why it is useful
and how does it fit into the analytics and data processing in Hadoop.
Moreover, as you read the paper you realize that the processing pipeline 
is actually Directed Acyclic Graph and paper goes a little more in depth
in theoretical aspects of Pig(the programming language).&lt;/p&gt;
&lt;p&gt;So, what does Pig bring to the table and what it is missing?  &lt;/p&gt;
&lt;h2&gt;Advantages&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Decrease in development time. This is the biggest advantage especially
  considering vanilla map-reduce jobs' complexity, time-spent and
maintenance of the programs. &lt;/li&gt;
&lt;li&gt;Learning curve is not steep, anyone who does not know how to write
  vanilla map-reduce or SQL for that matter could pick up and can write
  map-reduce jobs; not easy to master, though.&lt;/li&gt;
&lt;li&gt;Procedural, not declarative unlike SQL, so easier to follow the
  commands and provides better expressiveness in the transformation of
  data every step. Comparing to vanilla map-reduce, it is much more like
  an english language. It is concise and unlike Java but more like
  Python. &lt;/li&gt;
&lt;li&gt;I really liked the idea of dataflow where everything is about data
  even though we sacrifice control structures like for loop or if
  structures. This enforces the developer to think about the data but
  nothing else. In Python or Java, you create the control structures(for
  loop and ifs) and get the data transformation as a side effect. In here,
  data and because of data, data transformation is a first class citizen. Without data, you cannot create for loops, you need to always transform and manipulate data. But if you are not transforming data, what are you doing in the very first place?&lt;/li&gt;
&lt;li&gt;Since it is procedural, you could control of the execution of every
  step. If you want to write your own UDF(User Defined Function) and
  inject in one specific part in the pipeline, it is straightforward.&lt;/li&gt;
&lt;li&gt;Speaking of UDFs, you could write your UDFs in Python thanks to
  Jython. How awesome is that!&lt;/li&gt;
&lt;li&gt;Lazy evaluation: unless you do not produce an output file or does not
  output any message, it does not get evaluated. This has an advantage in the logical plan, it could optimize the program beginning to end and optimizer could produce an efficient plan to execute.&lt;/li&gt;
&lt;li&gt;Enjoys everything that Hadoop offers, parallelization,
  fault-tolerancy with many relational database features.&lt;/li&gt;
&lt;li&gt;It is quite effective for unstructured and messy large datasets.
  Actually, Pig is one of the best tool to make the large unstructured
data to structured.&lt;/li&gt;
&lt;li&gt;You have UDFs which you want to parallellize and utilize for large
  amounts of data, then you are in luck. Use Pig as a base pipeline
where it does the hard work and you just apply your UDF in the step that
you want. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Doge Pig" src="http://cdn.memegenerator.net/instances/500x/45784485.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;Disadvantages&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Especially the errors that Pig produces due to UDFS(Python) are not helpful at
  all. When something goes wrong, it just gives exec error in udf even
  if problem is related to syntax or type error, let alone a logical one.
  This is a big one. At least, as a user, I should get different error
messages when I have a syntax error, type error or a runtime error.&lt;/li&gt;
&lt;li&gt;Not mature. Even if it has been around for quite some time, it is
  still in the development.  (only recently they introduced a native datetime
  structure which is quite fundamental for a language like Pig especially      considering how an important component of datetime for time-series data.&lt;/li&gt;
&lt;li&gt;Support: Stackoverflow and Google generally does not lead good
  solutions for the problems. &lt;/li&gt;
&lt;li&gt;Data Schema is not enforced explicitly but implicitly. I think this is
  big one, too. The debugging of pig scripts in my experience is %90 of
time schema and since it does not enforce an explicit schema, sometimes
one data structure goes bytearray, which is a “raw” data type and unless
you coerce the fields even the strings, they turn bytearray without
notice. This may propagate for other steps of the data processing. &lt;/li&gt;
&lt;li&gt;Minor one: There is not a good ide or plugin for Vim  which provides more functionality than syntax completion to write the pig scripts. &lt;/li&gt;
&lt;li&gt;The commands are not executed unless either you dump or store an
  intermediate or final result. This increases the iteration between
debug and resolving the issue. &lt;/li&gt;
&lt;li&gt;Hive and Pig are not the same thing and the things that Pig does quite
  well Hive may not and vice versa. However, someone who knows SQL could
write Hive queries(most of SQL queries do already work in Hive) where she 
cannot do that in Pig. She needs to learn Pig syntax.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Some Pointers&lt;/h3&gt;
&lt;p&gt;If you want to do apply some statistics to your dataset(who does not
nowadays in order to get good analytics), then you should check out
&lt;a href="http://datafu.incubator.apache.org/"&gt;DataFu&lt;/a&gt;. Originally DataFu began in Linkedin but now it is incubator
Apache project, has a lot of good tools for statistics and utility UDFs
in general. Last month, Netflix released an interesting project named
&lt;a href="http://techblog.netflix.com/2014/01/introducing-pigpen-map-reduce-for.html"&gt;PigPen&lt;/a&gt; which aims to bring Clojure awesomeness to write Pig jobs.
It is an open source project, do not forget to check out the
&lt;a href="https://github.com/Netflix/PigPen"&gt;source code&lt;/a&gt;. I have not had chance to use it but functional
programming paradigm fits quite naturally to &lt;strong&gt;pipeline&lt;/strong&gt; processes, so
I expect it to be quite successful.(apart from Clojure's own
awesomeness)&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Sat, 08 Feb 2014 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2014-02-08:work/notes/2014-02-08/pig-advantages-and-disadvantages/</guid></item><item><title>Cultural Data Project Part 2</title><link>http://bugra.github.io/work/notes/2014-02-04/cultural-data-project-part-2/</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="/work/notes/2014-02-01/cultural-data-project-part-1/"&gt;First Part&lt;/a&gt; shows some basic statistics on &lt;a href="http://www.culturaldata.org/"&gt;Cultural Data Project&lt;/a&gt;. In order to get a better insight how do these companies make their money and different ways to monetize their services, I looked at the correlation of their revenue with other metrics. This could be quite important if the companies knew which type of marketing channels matter more for generating revenue, they could both allocate their efforts and also spendings more efficiently to increase their revenue and hopefully their profits.&lt;/p&gt;
&lt;h3&gt;Advertising vs. Revenue&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Advertisin Revenue Scatter Plot" src="/images/work/notes/2014/2/4/total_advertising_total_revenue_correlation.png" title="Advertising vs. Revenue" /&gt;
As you could see from the above graph, there is a correlation between
advertising and revenue. However, there are a lot of data points that
have zero for advertising. This could be due to
organizations do not spend &lt;strong&gt;any&lt;/strong&gt; money for advertisement or incomple
data. The latter seems more reasonable especially when we consider the values close
to zero are not present in the dataset. This shows a strong correlation if we
exclude the incomple data points. Let's look at the correlation more
closely, like doing a linear regression.&lt;/p&gt;
&lt;h3&gt;Linear Regression on Advertising vs. Revenue&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Linear Regression for Advertising and Total Revenue" src="/images/work/notes/2014/2/1/total_advertising_total_revenue_correlation.png" title="Linear Regression for Advertising and Total Revenue" /&gt;
Now, that is much better. We could see the correlation much better now
where the correlation coefficient is Pearson correlation coefficient.
But data have more than total revenue; earned revenue and contributed
revenue. Earned revenue suggests an income of selling good and services
of the organization where the contributed revenue is how much an
organization collects money from contributors. Let's see first earned
revenue:&lt;/p&gt;
&lt;h3&gt;Linear Regression on Advertising vs. Earned Revenue&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Linear Regression on advertising and earned revenue" src="/images/work/notes/2014/2/4/total_advertising_total_earned_revenue_correlation.png" title="Linear Regression on advertising and earned revenue" /&gt;&lt;/p&gt;
&lt;h3&gt;Linear Regression on Advertising vs. Contributed Revenue&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Linear Regression on advertising and contributed revenue" src="/images/work/notes/2014/2/4/total_advertising_total_contributed_revenue_correlation.png" title="Web Income mean grouped by Institution" /&gt;
To my surprise, the correlation between advertising and contributed
revenue is actually higher than earned revenue. Does this imply
advertising attracts more contributor? Maybe, but not necessarily. This
is most probably due to most of the organizations are highly dependent
on the contributions as their earned revenues are quite modest which
could be seen from the graphs above. &lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Tue, 04 Feb 2014 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2014-02-04:work/notes/2014-02-04/cultural-data-project-part-2/</guid></item><item><title>Cultural Data Project Part 1</title><link>http://bugra.github.io/work/notes/2014-02-01/cultural-data-project-part-1/</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Data is from &lt;a href="http://www.culturaldata.org/"&gt;Cultural Data Project&lt;/a&gt;. In order to get a better
understanding, I seggregated the data by organization type. There are
total 48 different organization types. Data follows &lt;a href="http://www.culturaldata.org/wp-content/themes/cdp/pdf/CDP-BlankProfile.pdf"&gt;data profile&lt;/a&gt; structure
and I grouped organizations into 6 different subsections;
employment statistics, their activities, pricing, website activity,
attendance and finally number of contributors. Before looking at the
distribution of the above metrics, let's look at when organizations are
founded. The first cultural organization dates go back to 1636 according
to data and youngest one seems to be 2012.
&lt;img alt="Foundation Year of Organizations" src="/images/work/notes/2014/2/1/year_founded.png" title="Foundation Year" /&gt;
Mean year is approximately 1975 and median year is 1984. The width of
the plot shows the number of organizations that are founded in a year
shown in the y axis. Most of the organizations are founded between 1950
and 2000 where around 2000, it reaches its peak. This could also be
observed from the difference in median and mean year.  &lt;/p&gt;
&lt;p&gt;Unless, it is stated otherwise, every subsection will give the mean,
median and sum statistics of the variables. The reason why three graphs
are provided is because some of the organizations are quite large in
terms of revenue, human resources and other metrics. As you will see
later, these companies' metrics dominate other companies; sum and mean
of those sections mostly reflect their statistics ignoring other
organizations. Median provides a nice solution for this type of
statistics as it is a metric which is quite robust to &lt;strong&gt;outliers&lt;/strong&gt;. Mean
and sum statistics are provided as they are also very useful to
summarize the data when there are not outliers or small number of
outliers.&lt;/p&gt;
&lt;p&gt;Organization types are sorted by alphabetical order so the &lt;em&gt;None of the
Above&lt;/em&gt; should change &lt;em&gt;None of anything else&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Employment&lt;/h2&gt;
&lt;h3&gt;Mean&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Employment Mean grouped by Institution" src="/images/work/notes/2014/2/1/employmen_groupby_institution_type_mean.png" title="Employment Mean grouped by Institution" /&gt;&lt;/p&gt;
&lt;h3&gt;Median&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Employment Median grouped by Institution" src="/images/work/notes/2014/2/1/employmen_groupby_institution_type_median.png" title="Employment Median grouped by Institution" /&gt;&lt;/p&gt;
&lt;h3&gt;Sum&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Employment Sum grouped by Institution" src="/images/work/notes/2014/2/1/employmen_groupby_institution_type_sum.png" title="Employment Sum grouped by Institution" /&gt;&lt;/p&gt;
&lt;h2&gt;Activity&lt;/h2&gt;
&lt;h3&gt;Mean&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Activity Types Mean grouped by Institution" src="/images/work/notes/2014/2/1/activity_groupby_institution_type_mean.png" title="Activity Types Mean grouped by Institution" /&gt;&lt;/p&gt;
&lt;h3&gt;Median&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Activity Types Median grouped by Institution" src="/images/work/notes/2014/2/1/activity_groupby_institution_type_median.png" title="Activity Types Median grouped by Institution" /&gt;&lt;/p&gt;
&lt;h3&gt;Sum&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Activity Types Sum grouped by Institution" src="/images/work/notes/2014/2/1/activity_groupby_institution_type_sum.png" title="Activity Types Sum grouped by Institution" /&gt;&lt;/p&gt;
&lt;h2&gt;Pricing&lt;/h2&gt;
&lt;h3&gt;Mean&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Pricing Mean grouped by Institution" src="/images/work/notes/2014/2/1/pricing_groupby_institution_type_mean.png" title="Pricing Mean grouped by Institution" /&gt;&lt;/p&gt;
&lt;h3&gt;Median&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Pricing Median grouped by Institution" src="/images/work/notes/2014/2/1/pricing_groupby_institution_type_median.png" title="Pricing Median grouped by Institution" /&gt;&lt;/p&gt;
&lt;h3&gt;Sum&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Pricing Sum grouped by Institution" src="/images/work/notes/2014/2/1/pricing_groupby_institution_type_sum.png" title="Pricing sum grouped by Institution" /&gt;&lt;/p&gt;
&lt;h2&gt;Web Activity&lt;/h2&gt;
&lt;h3&gt;Mean&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Website Activity Mean grouped by Institution" src="/images/work/notes/2014/2/1/web_activity_groupby_institution_type_mean.png" title="Web Activity Mean grouped by Institution" /&gt;&lt;/p&gt;
&lt;h3&gt;Median&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Website Activity Median grouped by Institution" src="/images/work/notes/2014/2/1/web_activity_groupby_institution_type_median.png" title="Web Activity sum grouped by Institution" /&gt;&lt;/p&gt;
&lt;h3&gt;Sum&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Website Activity Sum grouped by Institution" src="/images/work/notes/2014/2/1/web_activity_groupby_institution_type_sum.png" title="Web Activity sum grouped by Institution" /&gt;&lt;/p&gt;
&lt;h2&gt;Web Income&lt;/h2&gt;
&lt;h3&gt;Mean&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Website Income mean grouped by Institution" src="/images/work/notes/2014/2/1/web_income_groupby_institution_type_mean.png" title="Web Income mean grouped by Institution" /&gt;&lt;/p&gt;
&lt;h3&gt;Median&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Website Income Median grouped by Institution" src="/images/work/notes/2014/2/1/web_income_groupby_institution_type_median.png" title="Web Income median grouped by Institution" /&gt;&lt;/p&gt;
&lt;h3&gt;Sum&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Website Income Sum grouped by Institution" src="/images/work/notes/2014/2/1/web_income_groupby_institution_type_sum.png" title="Web Income sum grouped by Institution" /&gt;&lt;/p&gt;
&lt;h2&gt;Attendance&lt;/h2&gt;
&lt;h3&gt;Mean&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Attendance Mean grouped by Institution" src="/images/work/notes/2014/2/1/attendance_groupby_institution_type_mean.png" title="Attendance mean grouped by Institution" /&gt;&lt;/p&gt;
&lt;h3&gt;Median&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Attendance Median grouped by Institution" src="/images/work/notes/2014/2/1/attendance_groupby_institution_type_median.png" title="Attendance median grouped by Institution" /&gt;&lt;/p&gt;
&lt;h3&gt;Sum&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Attendance Sum grouped by Institution" src="/images/work/notes/2014/2/1/attendance_groupby_institution_type_sum.png" title="Attendance sum grouped by Institution" /&gt;&lt;/p&gt;
&lt;h2&gt;Number of Contributors&lt;/h2&gt;
&lt;h3&gt;Mean&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Contributor Mean grouped by Institution" src="/images/work/notes/2014/2/1/contributor_groupby_institution_type_mean.png" title="Contributor mean grouped by Institution" /&gt;&lt;/p&gt;
&lt;h3&gt;Median&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Contributor Median grouped by Institution" src="/images/work/notes/2014/2/1/contributor_groupby_institution_type_median.png" title="Contributor median grouped by Institution" /&gt;&lt;/p&gt;
&lt;h3&gt;Sum&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Contributor Sum grouped by Institution" src="/images/work/notes/2014/2/1/contributor_groupby_institution_type_sum.png" title="Contributor sum grouped by Institution" /&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Sat, 01 Feb 2014 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2014-02-01:work/notes/2014-02-01/cultural-data-project-part-1/</guid></item><item><title>Unreasonable Effectiveness of Metadata</title><link>http://bugra.github.io/work/notes/2014-01-30/unreasonable-effectiveness-of-metadata/</link><description>&lt;h2&gt;Metadata&lt;/h2&gt;
&lt;p&gt;Metadata could be translated as data about data if we want to translate
'mot à mot'. Generally, it defines what the data is about 
and gives some descriptive information around that. Still quite 
abstract huh? If the content of email is data, then sender,
receiver, date and time could be considered as metadata. 
Metadata could be quite important when data is not available or
unobservable due to various reasons. &lt;br /&gt;
One of the reasons why they could be collected quite freely and 
and abundant is because apparently it does not have to go through 
the same &lt;a href="http://www.slate.com/articles/news_and_politics/war_stories/2014/01/obama_s_nsa_reforms_the_president_s_proposals_for_metadata_and_the_fisa.html"&gt;privacy concerns&lt;/a&gt; unlike data goes.
Moreover, it is likely that its digital footprint is much smaller than
data and this leads to efficient storage, computation, analysis and
inference.
As its name suggests, it is an &lt;strong&gt;abstraction&lt;/strong&gt; of the data it sits on
top of the data; it does not necessarily tell the story, but that does not mean 
it does not reveal anything about data itself. Contrarily, it may expose a lot of 
interesting patterns and even not just about data.&lt;/p&gt;
&lt;h2&gt;Gmail Outage&lt;/h2&gt;
&lt;p&gt;Last week Gmail had an &lt;a href="http://googleblog.blogspot.in/2014/01/todays-outage-for-several-google.html"&gt;outage&lt;/a&gt; and users could not use their Gmail
accounts for almost one hour. &lt;br /&gt;
Consider this scenario, we cannot observe the outage of Gmail(after all 
it just prompts an error, temporary error 502), but we have all of the 
metadata of users who use Gmail. It would be quite easy to infer that
there is something wrong with Gmail infrastructure and this could be 
done pretty quickly and efficiently assuming we have all of the 
statistics and email metada information of users. Looking at the data of
the users has another important advantage; you actually do not care if
the infrastructure works but if &lt;strong&gt;your users&lt;/strong&gt; could actually use your
service. But is there a pattern to begin with? &lt;/p&gt;
&lt;h2&gt;Gmail Usage Analysis&lt;/h2&gt;
&lt;p&gt;So, I used my work gmail account that I used from beginning of June 2013
up to January 2014. I looked at received email and sent email times. The
results was quite interesting. Although there are a lot of third party
applications which shows some usage statistics, I found none of them
secure and reliable. So, I pulled the data from Gmail using standard
library &lt;a href="https://nest.com/"&gt;imaplib&lt;/a&gt;. Apart from easiness, you could have control of all
of the folders in your email for arbitrary time intervals whereas third 
party applications generally either gives predefined(inbox, sent) or put restrictions in the time interval. Python has a batteries-included 
approach which works quite convenient at the times  when you want 
to do implement something that is already implemented in the standard
library. &lt;/p&gt;
&lt;h2&gt;Received Emails&lt;/h2&gt;
&lt;h3&gt;Emails Per Day&lt;/h3&gt;
&lt;p&gt;For daily counts of emails, Monday and Thursday has two days less than
other days this is due to the federal holidays.
But I must say most of the received emails are from project management
web app that we were using. When I looked at my received emails, I see
mostly notifications rather than "emails". 
&lt;img alt="Received Email By Day" src="/images/work/notes/2014/1/27/received_email_by_day_bar_plot.png" title="Received Email By Day" /&gt;&lt;/p&gt;
&lt;p&gt;What do we infer from the above graph? For some reason, we have less
number of emails in Wednesday. Could it be we were working less in
Wednesday? Actually, yes. We used to have company meetings in Wednesday.
This may not change a lot for one person, but if you consider all of the
people in the company spent their times on the meeting, then total time
spent becomes 'meeting-hour * number-of-people' which apparently has an
effect on the total number of emails.  &lt;/p&gt;
&lt;h4&gt;What!, you were receiving emails on Sunday?&lt;/h4&gt;
&lt;p&gt;So, we were a distributed team and Israeli team took vacations
on Friday and worked on Sunday. Those emails are mostly due to the
activities and emails from Israeli team. This would also lead to high
number of sent emails on Monday as you will see later. What I found
interesting is that, more or less most of the days have same number of
emails. Even though it &lt;strong&gt;feels&lt;/strong&gt; like beginning of the weeks are
stressful and hectic, apparently if we sum the number of emails per day,
we get similar number of emails for each day.&lt;/p&gt;
&lt;h3&gt;Emails Per Hour&lt;/h3&gt;
&lt;p&gt;The following graph actually explains what type of email activity shows
a distributed and a team which has quite different timezones(USA,
Europe, Israel). 
&lt;img alt="Received Email By Day" src="/images/work/notes/2014/1/27/received_email_by_hour_histogram.png" title="Received Email By Hour" /&gt;&lt;/p&gt;
&lt;h3&gt;Emails Per Hour Over Time&lt;/h3&gt;
&lt;p&gt;There is a "inactive" time zone which corresponds to between 3 am and 6 am EST.
However, as different people have are in different time zones, generally
the inbox email flow seems to be continuous except inactive time zone. 
&lt;img alt="Received Email By Day" src="/images/work/notes/2014/1/27/received_email_by_hour_month_scatter_plot.png" title="Received Email By Hour Scatter Plot" /&gt;&lt;/p&gt;
&lt;p&gt;As I have mentioned, the emails are mostly project updates,
notifications and lastly and in a small percentage emails. What is more
interesting is actually &lt;strong&gt;sent&lt;/strong&gt; folder as it will reveal my email and 
for some extent working behaviour. &lt;/p&gt;
&lt;h2&gt;Sent Emails&lt;/h2&gt;
&lt;h3&gt;Emails Per Day&lt;/h3&gt;
&lt;p&gt;Remember, when I said the beginnings of the week was stressful and
hectic, it actually was. Generally, every week either task that I
was assigned in the previous week was completed or it the
method/algorithm/approach was not working, then we switch to other
approaches. Therefore, first two days of the week are mostly spent of 
trying to understand the task and try to figure out what is actually 
needed. This process brings a lot of questions and leads to high number 
of emails and communication. And, yes I was answering emails on Sunday 
time to time. 
&lt;img alt="Sent Email By Day" src="/images/work/notes/2014/1/27/sent_email_by_day_bar_plot.png" title="Sent Email By Day" /&gt;&lt;/p&gt;
&lt;h3&gt;Emails Per Hour&lt;/h3&gt;
&lt;p&gt;The following graph actually explains a lot of my daily activity. Can
you see when I have lunch? Or when I commute to work or when I leave
from the work? Such daily behavior probably also affects my reactions and
responses to advertisements that I see on web. My probability of
responding to a restaurant or a meal ad around noon peaks whereas after
1 hour, I would lose all of my interest to food. Similarly, Uber ads
around 9 am and 6 pm would may mean a lot to me whereas I am quite
unlikely to respond to the ads at other times. &lt;/p&gt;
&lt;p&gt;I also have two other accounts of in gmail which I use for my personal 
needs. With combination of those data, more or less my daily activity 
are actually could be inferred quite accurately from my gmail usage. And,
do not forget we did not yet go through my emails but only metada, 
timestamps of emails. Yet, even this much of data reveals a lot of my 
daily activity.
&lt;img alt="Sent Email By Day" src="/images/work/notes/2014/1/27/sent_email_by_hour_histogram.png" title="Sent Email By Hour" /&gt;&lt;/p&gt;
&lt;h3&gt;Emails Per Hour Over Time&lt;/h3&gt;
&lt;p&gt;From the following graph, could you see when I took vacation or switched to
another job so that my daily activity becomes almost zero? Could you
also see the activity in my transition and how it gets moved to
non-working hours (mostly).&lt;br /&gt;
&lt;img alt="Sent Email By Day" src="/images/work/notes/2014/1/27/sent_email_by_hour_month_scatter_plot.png" title="Sent Email By Hour Scatter Plot" /&gt;&lt;/p&gt;
&lt;p&gt;What is more is of course whom I contacted, what tasks or projects that
I worked, what type of emails I received and sent. However, even without context,
even without using &lt;em&gt;actual&lt;/em&gt; data, I could go this far. With recent acquisition of 
&lt;a href="https://nest.com/"&gt;Nest&lt;/a&gt;, Google not only has our online activity(search, email) but 
also physical activities which it is fundamental to complete 
&lt;strong&gt;the cycle&lt;/strong&gt;.  &lt;/p&gt;
&lt;h4&gt;What is next?&lt;/h4&gt;
&lt;p&gt;What is next is to infer the activities from data and predict 
what we will do next. In order to do that, Google needs abundant data
which it already has and machine learning algorithms that are effective
for unlabeled and large amounts of data, which it acqui-hires top-notch
researchers &lt;a href="http://www.cifar.ca/Google-DDNresearch-Geoffrey%20Hinton"&gt;Geoffrey Hinton&lt;/a&gt;, &lt;a href="http://www.technologyreview.com/news/524026/is-google-cornering-the-market-on-deep-learning/"&gt;DeepMind&lt;/a&gt; in order to enable just that.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Thu, 30 Jan 2014 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2014-01-30:work/notes/2014-01-30/unreasonable-effectiveness-of-metadata/</guid></item><item><title>PCA, EigenFace and All That</title><link>http://bugra.github.io/work/notes/2013-07-27/PCA-EigenFace-And-All-That/</link><description>&lt;p&gt;PCA(Principal Component Analysis) is one of the most commonly used unsupervised learning algorithm to compress, extract features for data and even for dimensionality reduction purposes. It has quite a lof of applications as follows: &lt;/p&gt;
&lt;h3&gt;Applications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Useful for compression and classfication of data.&lt;/li&gt;
&lt;li&gt;Aim is to reduce the number of variables, but at the same time, preserve most of information (variation), which may not necessarily hold true in general.&lt;/li&gt;
&lt;li&gt;New variables called principal components(PC) are uncorrelated, are ordered by fraction of total information each retains.&lt;/li&gt;
&lt;li&gt;PC's are a series of linear least squares fits to a sample, each orthogonal to all previous.&lt;/li&gt;
&lt;li&gt;Identify how different variables work together to create the dynamics of the system.&lt;/li&gt;
&lt;li&gt;Reduce the dimensionality of the data.&lt;/li&gt;
&lt;li&gt;Decrease the redundancy of the data.&lt;/li&gt;
&lt;li&gt;Filter some of the noise in the data.&lt;/li&gt;
&lt;li&gt;Compress the data.&lt;/li&gt;
&lt;li&gt;Prepare the data for further analysis using other techniques.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What does it do in the first place?&lt;/p&gt;
&lt;h3&gt;Functions of PCA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Is to reduce dimensionality by extracting the smallest number components that account for most of the variation in the original data. By doing so, we'd get get rid of the redundancy and preserve the variance in a smaller number of coefficients.&lt;/li&gt;
&lt;li&gt;PCA finds lines(2-d), planes(3-d) in a higher dimensional spaces that approximate the data in least squares($l_2$ norm).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Why do we choose PCA over other transformations which turn the original variables into a representation which depend on orthogonal bases, say Fourier Transform?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using a set of fixed set of components will give a good reconstruction of the same data(at least in least square sense, $l_2$ norm). However, Fourier transform does not guarantee such a premise.&lt;/li&gt;
&lt;li&gt;If the data has a lot of correlation among its variables(redundancy), then PCA could exploit this redundancy by uncorrelating the variables whereas Fourier transform cannot exploit this redundancy(at least explicitly).&lt;/li&gt;
&lt;li&gt;When PCA is used for dimensionality reduction, it is quite good at preserving the distance between the observations in the projection space.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;What about the disadvantages?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The components are not independent but uncorrelated. It would be even better if we have a representation which are independent to each other. It is called unsurprisingly &lt;em&gt;Independent Component Analysis&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;PCA seeks for linear combinations of the original variables. The nonlinear combination may even yield better representation. PCA has an extension for doing this type of analysis, &lt;em&gt;Nonlinear PCA&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Instead of $l_2$ norm, it may be advantageous to use $l_1$ norm. Especially, if the signal that we want to represent is sparse or has a sparse representation in some other space. PCA is extended for this specific problem as well, which is called &lt;em&gt;Sparse PCA&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Assumptions that PCA does:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Assumption 1: In general, high correlation between variables could be a sign of high redundancy.&lt;/li&gt;
&lt;li&gt;Assumption 2: The most important dynamics are the ones with the largest variance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Eigenvalues and Eigenvectors&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Eigenvalues measure the amount of variation(information) explaiend by each principal components and will be largest for the first PC and smaller for the subsequent PCs.&lt;/li&gt;
&lt;li&gt;An eigenvalue greater than 1 indicates that principal component accounts for more variance than accounted by one of the original variables in standardized data. This could be used to threshold to determine the number of eigenvectors.&lt;/li&gt;
&lt;li&gt;Eigenvectors provide the weights to compute the uncorrelated principal components, which are the linear combination of the original variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;What does PCA do (in a nutshell)?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PCA transforms the observations into uncorrelated components, which are nothing but linear combination of observations. &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Caveats&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;PCA is sensitive to scaling.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;By modifying the variance of the variables(scaling), it is possible to attribute different importance to them. By doing so, the prior information or the belief on the importance of the attributes can be preserved even in the PCA stage.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Definition of Principal Components&lt;/h2&gt;
&lt;p&gt;Given a sample $n$ observations on a vector $p$ variables:&lt;br /&gt;
$$x = (x_1, x_2, \ldots, x_p)$$
define the principal component of the sample by a linear transformation which is given as following:&lt;br /&gt;
$$ z = a^T x = \displaystyle\sum_{i=1}^p a_i x_i $$
where $a$ is
$$ a = (a_1, a_2, \ldots, a_p) $$
which is chosen to maximize the variance of $z$ and subject to 
$$ cov[z_k, z_l] = 0, k \gt l \geq 1 $$
and to 
$$ a^T a = 1 $$&lt;/p&gt;
&lt;h3&gt;How to derive the coefficients $a$ ?&lt;/h3&gt;
&lt;p&gt;The variance is:&lt;br /&gt;
$$ var[z] = \lt z^2\gt - \lt z \gt^2$$
$$ = \displaystyle \sum_{i,j = 1}^p a_i a_j \lt x_i x_j \gt - \displaystyle \sum_{i,j=1}^p a_i a_j \lt x_i \gt \lt x_j \gt $$
$$ = \displaystyle\sum_{i,j = 1}^p a_i a_j S_{ij}  $$
where $S_{ij} = \lt x_i x_j \gt - \lt x_i \gt \lt x_j \gt$ is the covariance matrix for $x$.&lt;br /&gt;
To find $a$ which maximizes $var[z]$ subject to $a^T a = 1$, let us introduce a lagrange multiplier $\lambda$. Then, the maximization equation becomes
$$ a^T S a - \lambda (a^T a - 1) $$
If we take the derivative with respect to $a$, then the equation becomes
$$ S a - \lambda a = 0 $$
$$ (S - \lambda I) a = 0 $$
Therefore, $a$ is an eigenvector of $S$ which has the corresponding value of $\lambda$.  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In short, we are interested in representing the most of the variation in the data by transforming the original variables into principal components. These components are orthogonal and ordered by magnitude so that the first few of them could explain most of the variation in the original observation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;EigenFace&lt;/h2&gt;
&lt;p&gt;PCA has a very good application which is in the computer vision domain, called &lt;a href="http://www.cs.ucsb.edu/~mturk/Papers/jcn.pdf"&gt;EigenFace&lt;/a&gt;, &lt;a href="http://www.cs.ucsb.edu/~mturk/Papers/mturk-CVPR91.pdf"&gt;short version&lt;/a&gt;. Eigenface is a name for eigenvectors which are the components of the face itself. It has been used for face recognition where the most variations considered as important. It was quite successful as well for some 20 years ago although it was replaced then by other methods. In this implementation, I used a particular subset of &lt;a href="http://cvc.yale.edu/projects/yalefaces/yalefaces.html"&gt;Yale Face Database&lt;/a&gt;.&lt;br /&gt;
The images that I used are given below:
&lt;img alt="Alt text" src="https://raw.github.com/bugra/EigenFace/master/img/images.png" title="Images" /&gt;&lt;/p&gt;
&lt;p&gt;If we average the face used for PCA, we get the following face:
&lt;img alt="Alt text" src="https://raw.github.com/bugra/EigenFace/master/img/average_face.png" title="Average Face" /&gt; &lt;/p&gt;
&lt;p&gt;The eigenfaces that we generated out of 11 faces are given below.
&lt;img alt="Alt text" src="https://raw.github.com/bugra/EigenFace/master/img/eigen_faces.png" title="EigenFaces" /&gt;  &lt;/p&gt;
&lt;p&gt;The eigenface which has the most variation(almost half of it) is given below(note the illumination variation)
&lt;img alt="Alt text" src="https://raw.github.com/bugra/EigenFace/master/img/first_eigen_face.png" title="First EigenFace" /&gt; &lt;/p&gt;
&lt;p&gt;Cumulative sum of first 10 eigenvalues is given below.
&lt;img alt="Alt text" src="https://raw.github.com/bugra/EigenFace/master/img/eigen_values.png" title="Cumulative Sum of Eigenvalues" /&gt;&lt;/p&gt;
&lt;p&gt;As it could be seen from &lt;a href="http://nbviewer.ipython.org/6099547"&gt;here&lt;/a&gt;(the last line), the top 4 eigenfaces can explain 95% variance of the faces.&lt;/p&gt;
&lt;p&gt;The program that I used to generate the images in &lt;a href="https://github.com/bugra/EigenFace"&gt;here&lt;/a&gt; and see the &lt;a href="http://nbviewer.ipython.org/6099547"&gt;Notebook&lt;/a&gt; for the flow of overall program.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Sat, 27 Jul 2013 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2013-07-27:work/notes/2013-07-27/PCA-EigenFace-And-All-That/</guid></item><item><title>Spectral Leakage</title><link>http://bugra.github.io/work/notes/2012-09-15/Spectral-Leakage/</link><description>&lt;p&gt;Have you ever analyzed(take FFT) a signal which has one or two main frequency components and found out that there are many more components than you expect even if there is no noise in the signal?  One of the reasons why it has more frequency components is spectral leakage, which is mainly due to windowing.&lt;br /&gt;
Say, we have a discrete signal of $x[n]$,&lt;br /&gt;
FFT of a sequence $x[n]$&lt;/p&gt;
&lt;p&gt;$$X[k] = \sum_{n=0}^{N-1} x[n] e^{\frac{-j 2 \pi n k}{N}}$$&lt;br /&gt;
where $w$ is sampled by $\frac{2\pi k}{N}$ for $k = 0,1, ..., N-1$.&lt;br /&gt;
FFT assumes the signal is periodic with period N and infinite duration. As the observation time of the signal is limited for a finite interval, infinite duration assumption cannot be satisfied. However, if the observation time is an integer multiple of the period of the signal, it would not cause a problem. This is because periodic signal and repeated signal results in the same signal.&lt;br /&gt;
On the other hand, if the observation time is not an integer multiple of the period, then frequency components of the signal change. There would be either discontinuties or overlaps in the signal when it is repeated over the observation time.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Notes/master/Spectral-Leakage/img/snapshot_twosignals.png" title="Discontinuity Signal" /&gt; 
The figure above shows the discontinuity in time domain and this results in spectral leakage in the frequency domain.&lt;br /&gt;
The problem is actually in this case and in general is "windowing". FFT assumes the signal is periodic and infinite, but when we observe and actually get the signal, we get its some part in a window.  This is called  non-coherent sampling, and shown below also.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Notes/master/Spectral-Leakage/img/spectral_leakage1.png" title="Non-coherent Sampling" /&gt;
That is the observed signal is actually convolution of "ideal"(infinite, periodic) signal with a windowing function. We also know that window results in $sinc$ function in the frequency domain and it has side lobes other than its main lobe. Those side lobes are not desired and they represent spectral leakage in frequency domain.&lt;br /&gt;
This results in unwanted frequency bins next to the main bin in the Fourier domain as shown below. It may be easy to remember this &lt;em&gt;phenomena&lt;/em&gt; if you think a "leakage" from the main bin to the other bins.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Notes/master/Spectral-Leakage/img/spectral_leakage2.png" title="Spectral Leakage in Frequency Bins" /&gt;
If the sampling rate is the multiple integer of the period of the input signal, this sampling is called &lt;strong&gt;coherent sampling&lt;/strong&gt;. However, it is not very common to know the input signal's frequency exactly. Therefore, we need windows in order to analyze signals.  Coherent sampling is shown in successive frames.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Notes/master/Spectral-Leakage/img/coherent_sampling1.png" title="Coherent Sampling in Time Domain" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Notes/master/Spectral-Leakage/img/coherent_sampling2.png" title="Coherent Sampling in Frequency Domain" /&gt;
Instead of rectangular signals, we may want to choose non-rectangular windows in order to decrease the spectral leakage. I do not want to go into detail, but in terms of choosing non-rectangular windows, there is also a trade-off. It is between frequency resolution and spectral leakage. If you want more frequency resolution in the signal, you need to sacrifice in the spectral leakage or vice versa.&lt;/p&gt;
&lt;h3&gt;Code&lt;/h3&gt;
&lt;p&gt;All of the code and images used in this note is available in the &lt;a href="https://github.com/bugra/Spectral-Leakage" title="Spectral Leakage"&gt;here&lt;/a&gt;.  &lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Sat, 15 Sep 2012 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2012-09-15:work/notes/2012-09-15/Spectral-Leakage/</guid></item><item><title>Phase Detection in Digital Signals</title><link>http://bugra.github.io/work/notes/2012-09-01/Phase-Detection/</link><description>&lt;p&gt;Phase Detection is an important concept in radar signals. It could be used to determine the time delay between radar signals and this time delay could be used to infer the distance of the object from the radar, which is the main aim of the radar. Even if it is assumed that transmitted signal and received signal should be same, there occurs to be noise.
I was assigned to implement phase detection for FPGA's for my internship in Summer 2010 as they are used for radar signal processing. They can do complex and real-time processing, are cheaper than computers, portable, low power consumers.
However, I did not go into implementation directly. I need a simulation which would yield promising results. Then, I could implement in VHDL. I chose Matlab and implemented three different methods. (This would decrease to two in VHDL).
I will compare these methods as well, but it cannot be deduced that some method &lt;em&gt;always&lt;/em&gt; or even usually perform better than some other method based on the results provided. There are a numerous reasons, but I will just mention few of them. Firstly, this is just a &lt;strong&gt;simulation&lt;/strong&gt; after all. Secondly, the simulation takes into consideration only one of the signal is noisy(received), which may not hold true in general. Moreover, there are constant parameters which may favor some method over other ones.
Before giving details of the methods, I am urged to mention an important concept that two methods use, Schmitt Trigger.&lt;/p&gt;
&lt;h3&gt;Schmitt Trigger&lt;/h3&gt;
&lt;p&gt;Schmitt Trigger is a comparator with two different threshold values. When the signal goes over &lt;strong&gt;High Threshold&lt;/strong&gt;, it yields 1(HIGH). It remains 1 until signal goes below &lt;strong&gt;Low Threshold&lt;/strong&gt;, it yields 0. It remains 0, until signal goes over ...  as it could be seen the figure below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Phase-Detection/master/img/schmitt-trigger2.png" title="Schmitt Trigger" /&gt;&lt;/p&gt;
&lt;p&gt;In analog implementation of Schmitt Trigger, there is a feedback mechanism which uses hysteresis to adjust its threshold values. In this implementation, they are constant. Schmitt Trigger is used to prevent noise when analogous signal is transformed into a digital(binary) one. It prevents oscillation in the binary signal which is convenient and makes the binary signal robust to noise. Consider the figure below, A is a basic comparator. B is a Schmitt Trigger. Any fluctuation in the border of A resultss in oscillation in the digital signal. This is the general case for noisy signals. However, fluctuations in border B does not change digital signal since digital signal needs to wait for &lt;em&gt;high threshold&lt;/em&gt; value or &lt;em&gt;low threshold&lt;/em&gt; value to change.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Phase-Detection/master/img/schmitt-trigger.png" title="A is comparator, B is Schmitt Trigger" /&gt;&lt;/p&gt;
&lt;p&gt;It is important to note that if noise level is higher than the difference of two thresholds, schmitt trigger does not provide robustness to noise. It is still better than a basic comparator, though. Therefore, noise level should be taken into consideration when setting up the threshold values.&lt;/p&gt;
&lt;h2&gt;PulseLag Method&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Phase-Detection/master/img/pulselag_method1.png" title="Original and Delayed Signals" /&gt;&lt;/p&gt;
&lt;p&gt;In this method, we multiply both signals $ f_c $ with another sinusoid, but a lower frequency $ f_{down} $  Then, low-pass filter this signal in order to get low frequency part.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Phase-Detection/master/img/pulselag_method2.png" title="Filtered Signals" /&gt;&lt;/p&gt;
&lt;p&gt;After filtering the signal, we put it into Schmitt Trigger. Then, we get their difference which also gives time delay.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Phase-Detection/master/img/pulselag_method3.png" title="Schmitt Triggered Signals" /&gt;&lt;/p&gt;
&lt;p&gt;We need the ratio of the (number of times first signal is 1 and second signal is 0) over the total time. Then, we need to divide this ratio to $ f_c $ to find time delay.
I apply 0.5 nanosecond delay in the signal and it will be constant for the other methods as well, for noiseless case method measures  4.5714e-10. For noise coefficient = 0.1, method measures  5.1429e-10. For noise coefficient = 0.2, method measures  8.0000e-10, which is not even close.
Even if noiseless and when noise is small, this method performs well, for noisy cases, it is not robust to noise.&lt;/p&gt;
&lt;h2&gt;Multiplication Method&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Phase-Detection/master/img/multiplication_method1.png" title="Original Signals" /&gt;
If we multiply two signals with each other, we have a high pass term and a phase term if they have both the same frequency.(noiseless case)&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Phase-Detection/master/img/multiplication_method2.png" title="Filtered Multiplied Signal" /&gt;
If we get inverse sin or cosine of the multiplied signal&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Phase-Detection/master/img/multiplication_method3.png" title=" Inverse of Filtered Multiplied Signal" /&gt;
and take mean of the signal(after some heuristics), divide by $2 * \pi * f_c$, then we get a time delay.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Phase-Detection/master/img/multiplication_method4.png" title="Filtered Multiplied Signal" /&gt;&lt;/p&gt;
&lt;p&gt;For noiseless case method measures 4.9639e-10 which is quite close. For noise coefficient = 0.1, method measures  4.6909e-10. For noise coefficient = 0.2, method measures 5.8330e-10, which is quite close for such a high noise.&lt;/p&gt;
&lt;h2&gt;FFT-Based Method&lt;/h2&gt;
&lt;p&gt;FFT can be written in a Discrete Fourier Transform:
$$ X[k] = \sum_{n=0}^{N-1} x[n] e^{\frac{-j 2 \pi n k}{N}} = A_k e^{j \phi_k} $$
and
$$ x[n] \leftrightarrow X[k]  $$
$$ x[n-D] \leftrightarrow e^{\frac{-j2 \pi k D}{N}}X[k] $$
It is important to note, in the magnitude response, the phase $e^{\frac{-j2 \pi k D}{N}}$ will be one. Therefore, it will not affect the magnitude response.
Since we send periodic signals, the maximum of frequency response will occur at the dominant frequency which is the period of the signal. As the magnitude response will be same, we could just take the phase response of the same index in FFT to find the phase information between the signals.
Original and delayed signal(0.5e-9) signals are given below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Phase-Detection/master/img/fft_method_fig1.png" title="Original and Delayed Signal" /&gt;&lt;/p&gt;
&lt;p&gt;Magnitude response of the signals are given below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Phase-Detection/master/img/fft_method_fig2.png" title="Magnitude Response of FFT of the signals" /&gt;&lt;/p&gt;
&lt;p&gt;Phase response of the signals:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="https://raw.github.com/bugra/Phase-Detection/master/img/fft_method_fig3.png" title="Phase Response of FFT" /&gt;&lt;/p&gt;
&lt;p&gt;When we take the phases of the both signals and subtract, we get the phase difference. Time delay is only the ratio of phase difference over $ 2 * pi * fc $.
For noiseless case method measures 4.9656e-10 which is quite close. For noise coefficient = 0.1, method measures  5.1161e-10. For noise coefficient = 0.2, method measures 3.9221e-10, which is not very close.&lt;/p&gt;
&lt;h3&gt;Result&lt;/h3&gt;
&lt;p&gt;For noiseless case, all of the methods perform quite well. For medium noise(n = 0.1), FFT-Based and PulseLag method; for high noise, Multiplication method performs better.&lt;/p&gt;
&lt;h3&gt;Code&lt;/h3&gt;
&lt;p&gt;All of the code and images used in this note is available in the &lt;a href="https://github.com/bugra/Phase-Detection" title="Phase-Detection-Matlab"&gt;here&lt;/a&gt;.
Moreover, implementation of Multiplication and FFT-Based method in VHDL is also available in &lt;a href="https://github.com/bugra/Phase-Detection-VHDL" title="Phase-Detection-VHDL"&gt;here&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Sat, 01 Sep 2012 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2012-09-01:work/notes/2012-09-01/Phase-Detection/</guid></item><item><title>MongoDB Notes</title><link>http://bugra.github.io/work/notes/2012-06-16/MongoDB-Notes/</link><description>&lt;p&gt;Some Properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MongoDB instances act as high-level container.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Collection&lt;/em&gt; is a synonym of table in sql.&lt;/li&gt;
&lt;li&gt;Collections are made by &lt;em&gt;documents&lt;/em&gt;. (document =&amp;gt; row)&lt;/li&gt;
&lt;li&gt;Document is made by &lt;em&gt;fields&lt;/em&gt;. (field =&amp;gt; column)&lt;/li&gt;
&lt;li&gt;Indices are similar to sql databases.&lt;/li&gt;
&lt;li&gt;Cursor can count or skip ahead without actually pulling down data.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;A collection does not have a schema to follow. Therefore, fields are tracked with each individual document.&lt;/p&gt;
&lt;p&gt;_id field is automatically generated by MongoDB, and every document must have a unique _id field.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Adding queries&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Agent Smith&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;gender&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;age&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and &lt;code&gt;db.matrix.findOne()&lt;/code&gt; results in;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcf8b5321f61bf6f63ddd78&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Agent Smith&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;gender&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;m&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;44&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we add another person:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Neo&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;gender&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;age&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and call  &lt;code&gt;db.matrix.find()&lt;/code&gt; then we get;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcf8b5321f61bf6f63ddd78&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Agent Smith&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gender&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;m&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;44&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcf8ce921f61bf6f63ddd79&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Neo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gender&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;m&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Similarly add two more people:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Trinity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;gender&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;f&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;age&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;35&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Morpheus&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;gender&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;age&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;45&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And we have total of four people in the database and we could print by using &lt;code&gt;find()&lt;/code&gt; method as  &lt;code&gt;db.matrix.find()&lt;/code&gt;  and return the list of &lt;em&gt;documents&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcf8b5321f61bf6f63ddd78&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Agent Smith&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gender&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;m&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;44&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcf8ce921f61bf6f63ddd79&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Neo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gender&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;m&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcf8df921f61bf6f63ddd7a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Trinity&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gender&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;f&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;35&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcf8e0821f61bf6f63ddd7b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Morpheus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gender&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;m&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;45&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Architecture seems to be male, but he is considered to be ageless. Therefore, when we insert the document in database, we &lt;em&gt;can&lt;/em&gt; simply ignore his age information as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Architecture&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;gender&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and when we return documents, mongo would not cause any problem at all.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcf8b5321f61bf6f63ddd78&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Agent Smith&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gender&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;m&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;44&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcf8ce921f61bf6f63ddd79&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Neo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gender&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;m&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcf8df921f61bf6f63ddd7a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Trinity&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gender&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;f&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;35&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcf8e0821f61bf6f63ddd7b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Morpheus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gender&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;m&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;45&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcfa18821f61bf6f63ddd7c&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Architecture&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gender&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;m&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;hr /&gt;
&lt;p&gt;After creating the database, we need to be able to select some queries based on their fields. In order to do it, we need to use &lt;em&gt;selectors&lt;/em&gt; which are very similart to &lt;strong&gt;where&lt;/strong&gt; clause of Sql statement.
simplest one is &lt;code&gt;{}&lt;/code&gt; which returns all documents in the collection. &lt;code&gt;null&lt;/code&gt; also does the same  thing as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({})&lt;/span&gt;
&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;returns all the documents in the collection.
&lt;code&gt;and&lt;/code&gt; statement is accomplished in the form of:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;field1&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;value1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;field2&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;value2&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which is very intuitive. In this example, if we want to return males whose ages are more or equal to 44, then we need to write a selector as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;gender&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;age&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;$gte&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;}})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Some of common operations are:
&lt;code&gt;\(lt&lt;/code&gt; =&amp;gt; less than
&lt;code&gt;\(lte&lt;/code&gt;=&amp;gt; less or equal
&lt;code&gt;\(gt&lt;/code&gt;=&amp;gt; greater than
&lt;code&gt;\(gte&lt;/code&gt;=&amp;gt; greater or equal
&lt;code&gt;\(ne&lt;/code&gt; =&amp;gt; not equal&lt;/p&gt;
&lt;p&gt;If we take harder example like a field defines an array, in a particular example, let it be sports which students like, create the database as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcfce79fd6230c28d817740&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;john&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;likes&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;basketball&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;football&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;baseball&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;swimming&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcfcf02fd6230c28d817742&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;mike&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;likes&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;basketball&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;football&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;tennis&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rugby&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcfcf08fd6230c28d817743&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;cassandra&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;likes&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;golf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;table-tennis&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;tennis&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcfcf11fd6230c28d817744&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;paul&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;likes&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;golf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;table-tennis&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;tennis&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rugby&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we would like to retrieve students who likes golf or tennis:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;students&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;$or&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt;&lt;span class="nx"&gt;likes&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tennis&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;},{&lt;/span&gt;&lt;span class="nx"&gt;likes&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;golf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}]})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then we get queries;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcfcf02fd6230c28d817742&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;mike&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;likes&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;basketball&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;football&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;tennis&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rugby&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcfcf08fd6230c28d817743&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;cassandra&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;likes&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;golf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;table-tennis&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;tennis&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fcfcf11fd6230c28d817744&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;paul&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;likes&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;golf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;table-tennis&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;tennis&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rugby&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;as expected.
In these array fields, it is very easy to combine some of the fields let alone one of the field query returning. It becomes extremely useful as time goes by.
One operation is &lt;code&gt;\(in&lt;/code&gt; which tries to determine whether the values are in the array.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;a&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;$in&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]}})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Another similar and useful operation is &lt;code&gt;exists&lt;/code&gt; operation which checks whether the value matches to any value in the database checking every field in the collection.
Id's of documents can also be selected using &lt;code&gt;_id&lt;/code&gt; field in the collection as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;_id&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fd0685b4e0fa619963db3b3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and it results in the respective document:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fd0685b4e0fa619963db3b3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Morpheus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gender&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;m&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;45&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we have document which have common fields and want to count them, we could do so by using &lt;code&gt;count&lt;/code&gt; operation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;count&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Morpheus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and it returns 2.(I added the same item twice). If it does not find it, it returns 0 as expected.&lt;/p&gt;
&lt;h2&gt;Removing Queries&lt;/h2&gt;
&lt;p&gt;We could also erase the documents based on their properties as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;remove&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Morpheus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we want to remove all the entries, we could simply do not give any field information or put &lt;code&gt;null&lt;/code&gt; in &lt;code&gt;remove&lt;/code&gt; operation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;remove&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For updates, let's first create a database in a different syntax:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;users&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;save&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;John&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;languages&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ruby&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;c&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;java&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;javascript&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]});&lt;/span&gt;
&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;users&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;save&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Sue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;languages&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;haskell&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;lisp&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;python&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;lush&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]});&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In order to update, we need to first select the document based on its one of the field, in this example it would be the names of people and using &lt;code&gt;update()&lt;/code&gt; operation, we could update as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;users&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;update&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;John&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Johnny&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;languages&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;scala&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;java&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;python&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]});&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Instead of updating the entire document, we could update only the fields:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;users&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;update&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Sue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;},{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;$set&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;age&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;}})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And when we try to print out the collection:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fd7d5aba46929bd0bbd56f7&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Johnny&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;languages&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;scala&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;java&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;python&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fd7d5ada46929bd0bbd56f8&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;age&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;languages&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;scala&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;lisp&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Sue&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In order to update array elements, we could use &lt;code&gt;\(pull&lt;/code&gt; and &lt;code&gt;\(push&lt;/code&gt; operations. For example, if we want to remove &lt;em&gt;haskell&lt;/em&gt; language from Sue's languages:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;users&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;update&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Sue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;$pull&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;languages&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;haskell&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="p"&gt;});&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and if we want to add a language say java:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;users&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;update&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Sue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;$push&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;languages&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;java&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="p"&gt;});&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Upsert(Update + Insert)&lt;/h3&gt;
&lt;p&gt;Mongo supports so called &lt;em&gt;upserts&lt;/em&gt; which is nothing more than a fancy combination of update and insert. That is, if item that we want to update is not in the collection, it automatically creates it. If it does exist in the collection, it updates by default. However, in order to enable this feature of Mongo, we need to enable the third parameter of &lt;code&gt;update&lt;/code&gt; operation as &lt;code&gt;true&lt;/code&gt;. Say, we need to create a website hit counter, and in order to do so we increment the number of hits every time the name of website is updated. If we do not have the website name in the collection, we do not have to create it beforehand. We could just use upsert as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;hits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;update&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;page&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;yahoo&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;},{&lt;/span&gt;&lt;span class="nx"&gt;$inc&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;hits&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;}},&lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we do not have the third parameter or set to &lt;code&gt;false&lt;/code&gt;, above statement does not change anything in the collection.&lt;/p&gt;
&lt;h4&gt;Multiple Updates&lt;/h4&gt;
&lt;p&gt;If we want to multiple updates in the collection, we need to enable the fourt parameter in the &lt;code&gt;update&lt;/code&gt; operation. For example, we want to reset the counter of websites as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;hits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;update&lt;/span&gt;&lt;span class="p"&gt;({},{&lt;/span&gt;&lt;span class="nx"&gt;$set&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;hits&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;}},&lt;/span&gt;&lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;By doing so, we update all the documents in the collection. However, if we do not enable the fourt parameter as &lt;code&gt;true&lt;/code&gt;, then &lt;em&gt;only the first element of the collection will be updated&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Deeper in &lt;code&gt;find()&lt;/code&gt; operation&lt;/h3&gt;
&lt;p&gt;If we want to retrieve specific fields of the documents we could use a second parameter in &lt;code&gt;find()&lt;/code&gt;,e.g. only the names of the webpages as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;hits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,{&lt;/span&gt;&lt;span class="nx"&gt;page&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;then, it results in only page fields of  documents, namely &lt;em&gt;google&lt;/em&gt; and &lt;em&gt;yahoo&lt;/em&gt;.&lt;/p&gt;
&lt;h4&gt;Ordering&lt;/h4&gt;
&lt;p&gt;Say, we have a database as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fd8cf5bbd6be5d371385b9a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hits&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;page&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;yahoo&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fd8d0fabd6be5d371385b9b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hits&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;page&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;google&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fd91925da57fdfbb68d7848&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;page&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;microsoft&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hits&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fd91930da57fdfbb68d7849&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;page&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;facebook&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hits&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4fd91938da57fdfbb68d784a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;page&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;apple&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hits&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and we want to order this collection by the number of hits in a descending order. We could do this by using &lt;code&gt;sort()&lt;/code&gt; operation as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;hits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;hits&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we want to also return some specific ranks in the sort, we could use &lt;code&gt;limit()&lt;/code&gt; and &lt;code&gt;skip()&lt;/code&gt; operations. For example, we want to return second and third queries only.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;hits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;hits&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;}).&lt;/span&gt;&lt;span class="nx"&gt;limit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;skip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Counting &lt;code&gt;count()&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;count()&lt;/code&gt; can be used itself as an independent operation similar to &lt;code&gt;update&lt;/code&gt; and &lt;code&gt;insert&lt;/code&gt;. However, its origin is to follow &lt;code&gt;find().count()&lt;/code&gt;. Therefore, &lt;code&gt;count()&lt;/code&gt; operation can be considered as a syntactic sugar for &lt;code&gt;find&lt;/code&gt; and &lt;code&gt;count&lt;/code&gt;. If we want to count the webpages which have higher than 9 hits;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;hits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;hits&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;$gt&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;}}).&lt;/span&gt;&lt;span class="nx"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;MongoDB Difference and Similarities to RDBMS&lt;/h3&gt;
&lt;p&gt;There is no &lt;code&gt;join&lt;/code&gt; opearation in Mongo contrary to RDBMS,but we could connect by using a foreign key in the documents of collections. In order to show the embedded and relational part of database, we start with employees example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4d85c7039ab0fd70a117d730&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Paul&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4d85c7039ab0fd70a117d731&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Duncan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;manager&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4d85c7039ab0fd70a117d730&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4d85c7039ab0fd70a117d732&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Moneo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;manager&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4d85c7039ab0fd70a117d730&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We set Paul as a manager of Duncan and Moneo's. In the collection, in order to find the manager, we could do as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;employees&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;manager&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4d85c7039ab0fd70a117d730&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we have two managers in the company, then we could add two managers as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;employees&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;_id&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4d85c7039ab0fd70a117d733&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Siona&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;manager&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4d85c7039ab0fd70a117d730&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4d85c7039ab0fd70a117d732&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We could again return the manager using above operation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;employees&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;manager&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4d85c7039ab0fd70a117d730&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we want to embed more documents into the collection like a nested document, we could do so as such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;employees&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="nx"&gt;_id&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4d85c7039ab0fd70a117d734&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Ghanima&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="nx"&gt;family&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;mother&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Chani&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;father&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Paul&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;brother&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;ObjectId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;4d85c7039ab0fd70a117d730&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)}})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We could return the respective field of the nested document by using &lt;em&gt;dot&lt;/em&gt; notation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;db&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;employees&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;find&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;family.mother&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Chani&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;MongoDB Repair in Ubuntu&lt;/h3&gt;
&lt;p&gt;First, try to repair.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;rm&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;mongodb&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;mongod&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lock&lt;/span&gt;
&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;chown&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="n"&gt;mongodb&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;mongodb&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;mongodb&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;
&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;u&lt;/span&gt; &lt;span class="n"&gt;mongodb&lt;/span&gt; &lt;span class="n"&gt;mongod&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;etc&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;mongodb&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;repair&lt;/span&gt;
&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;service&lt;/span&gt; &lt;span class="n"&gt;mongodb&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then, restart the database.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;service&lt;/span&gt; &lt;span class="n"&gt;mongod&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;
&lt;span class="n"&gt;mongo&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bugra Akyildiz</dc:creator><pubDate>Sat, 16 Jun 2012 00:00:00 +0000</pubDate><guid>tag:bugra.github.io,2012-06-16:work/notes/2012-06-16/MongoDB-Notes/</guid></item></channel></rss>