<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Bugra Akyildiz</title><link href="http://bugra.github.io/" rel="alternate"></link><link href="http://bugra.github.io/feeds/28.atom.xml" rel="self"></link><id>http://bugra.github.io/</id><updated>2014-03-28T00:00:00+00:00</updated><entry><title>8th NYAS Machine Learning Symposium 2014</title><link href="http://bugra.github.io/work/notes/2014-03-28/nyas-machine-learning-symposium-2014/" rel="alternate"></link><published>2014-03-28T00:00:00+00:00</published><updated>2014-03-28T00:00:00+00:00</updated><author><name>Bugra Akyildiz</name></author><id>tag:bugra.github.io,2014-03-28:work/notes/2014-03-28/nyas-machine-learning-symposium-2014/</id><summary type="html">&lt;p&gt;I attended to &lt;a href="http://www.nyas.org/Events/Detail.aspx?cid=2cc3521e-408a-460e-b159-e774734bcbea"&gt;NYAS 8th Machine Learning Symposium&lt;/a&gt; and here are the notes
that I took from the event. It may contain errors and mistakes. If you
find any, please let me  know.&lt;br /&gt;
On personal view, it was worse than the previous machine learning
symposium(7th) in both posters and also talks. Last year, the posters and
talks were much more interesting to me. That being said, I could not
visit all of the posters so take my word with a grain of salt.  &lt;br /&gt;
The &lt;a href="http://www.nyas.org/asset.axd?id=defb6b86-f7ad-4a8d-af2a-3a5ef979c143&amp;amp;t=635314462602300000"&gt;abstracts in pdf&lt;/a&gt; in here. &lt;/p&gt;
&lt;h2&gt;Machine Learning for Powers of Good&lt;/h2&gt;
&lt;p&gt;by &lt;a href="http://www.rayidghani.com/"&gt;Rayid Ghani&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Probability of optimization of limited resources for a campaign. &lt;/li&gt;
&lt;li&gt;Another important thing: to influence the behavior of the voter and
  how to make them engage with the campaign.&lt;/li&gt;
&lt;li&gt;Because prediction itself is not good enough. &lt;/li&gt;
&lt;li&gt;Resource allocation based on who are likely to be influenced. Who are
  likely to change their mind? Definitely not Texas.&lt;/li&gt;
&lt;li&gt;Not voting to Romney and not going to vote, too much work. Do not try
  to event attempt to do anything.&lt;/li&gt;
&lt;li&gt;Focus on the ones either who are indecisive but likely to vote or
  indecisive about voting but weakly support Obama.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dssg.io/"&gt;Data Science for Social Good&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Following Spotlight Talks&lt;/h2&gt;
&lt;h3&gt;Graph-Based Posterior Regularization for Semi-Supervised Structured Prediction:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Posterior labels for part of speech tags. Graph-based approach,
  using Laplacian of Graph&lt;/li&gt;
&lt;li&gt;Structured Prediction =&amp;gt;  CRF =&amp;gt; local scope, features&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Graph-propagation and CRF estimation =&amp;gt; Joint objective, then to
optimize and look at the KL divergence as well for both world parameters.&lt;/p&gt;
&lt;p&gt;Relevant work is &lt;a href="http://jmlr.org/papers/volume11/ganchev10a/ganchev10a.pdf"&gt;Posterior Regularization(PR) Linear Ganchev&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;EM like algorithm =&amp;gt; to converge to the local optimum &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;She showed that it performs better than both CRF and Graph based approaches in her poster, but she did not compare speed of this approach with CRF or Graph based approaches. It is likely the method is slower than CRF but, I am not very familiar Graph based approaches and joint objective could be quite hard to optimize. So, the speed is could be much worse more than 2 times than CRF.&lt;/p&gt;
&lt;h3&gt;Learning from Label Proportions(LLP):&lt;/h3&gt;
&lt;p&gt;It attacks Binary learning problem with an extension of bag approach
where bags represent the ratio of the labels that are known but
individual labels are unknown. They try to solve the problem in a large
margin framework trying to model the instances belonging to a particular
label and try to increase margin with the other label(smv-like).
- Extension of supervised learning objective with Bag Proportion Loss
  with model parameters with a proportion loss.&lt;/p&gt;
&lt;h5&gt;Generalization Error of LLP&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Sample complexity of learning is proportional to bag proportion&lt;/li&gt;
&lt;li&gt;Instance label prediction error =&amp;gt; again depends on the prediction
  error&lt;/li&gt;
&lt;li&gt;Not only the supervised learning objective but also the bag
  proportions for the labeling matters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Generative Image Models For Visual Phenotype Modeling&lt;/h3&gt;
&lt;p&gt;They have genome types of fish and they have features of the fish. In
order to learn which genome type has effect on which fish trait, they
propose an admixture model which tries to correlate the traits and
genome. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Admixture model to correlate the shape variance between geneology of the
fishes.   &lt;/li&gt;
&lt;li&gt;Annotated genome from the shape variance of the fish.&lt;/li&gt;
&lt;li&gt;Genome annotates the features of the shape variance of the fish.&lt;/li&gt;
&lt;li&gt;Unsupervised learning of the features and joint generative model from
  fish variance and genome change. =&amp;gt; Seems quite novel.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Large Scale Learning - Scaling Graph-based semi supervised-learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Replace label vectors =&amp;gt; count-min-sketch? is a data
  structure(randomized) stores the counts of items. &lt;/li&gt;
&lt;li&gt;MAD exact vs Mad-Sketch =&amp;gt; comparison&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/parthatalukdar/junto"&gt;Junto Toolkit @ Github&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Structured Classification Criteria for Deep Learning for Speech Recognition (Second Keynote)&lt;/h2&gt;
&lt;p&gt;Before this talk, I knew that IBM is strong in deep learning(if I recall
correctly, they had a poster last year for speech recognition)  but
I did not know that they published also strong papers for speech
recognition last year. Google and Facebook get a lot of coverage for deep learning 
and that maybe rightly so, but IBM is also a strong player in the area.&lt;/p&gt;
&lt;h3&gt;Talk Structure&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Acoustic Modeling for speech recoognition:&lt;/li&gt;
&lt;li&gt;Structured loss function: we do not care about the loss function per
  se but how audible it is in the speech. Therefore, the loss function
should be structured around audibility of the speech.&lt;/li&gt;
&lt;li&gt;Optimization&lt;/li&gt;
&lt;li&gt;Speeding for training&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Bayesian Modeling for Speech Recognition&lt;/h4&gt;
&lt;p&gt;Sequence of phones are nice because  if have a word to
classify and you did not have that sample in the training set, you could "guess" the word from the sequence of phones.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Context affects the acoustic realization of a phone in the speech.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Context-dependent modeling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Condition on adjacent phones, infer the xcontext.&lt;/li&gt;
&lt;li&gt;Parameter sharing needed to mitigate data sparsity
 sahring must generalize to unseen contexts.&lt;/li&gt;
&lt;li&gt;Decision tree to get the AA-b, nasal, retroflex, fricative?, too much
  hand-engineered features.&lt;/li&gt;
&lt;li&gt;Basic speech sounds =&amp;gt; 1000 to 64K&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Structured Loss Functions&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Cross-entropy, for training criterion.&lt;/li&gt;
&lt;li&gt;A neural network with training error with the cross-entropy.&lt;/li&gt;
&lt;li&gt;We do not care about individual phones error but word error.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/Cross_entropy"&gt;Cross-Entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://mi.eng.cam.ac.uk/~wjb31/ppubs/hlt04_mbr_smt.pdf"&gt;Bayes-Risk Losses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hamming distance is a &lt;strong&gt;forgiving&lt;/strong&gt; distance measure for error between
  HMM sequences.&lt;/li&gt;
&lt;li&gt;To represent reference space, lattices generated via constrained
  recognition.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Stochastic Gradient Optimization&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Training GMM with is the fundamental idea.&lt;a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=4960445&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4960445"&gt;Stochastic Gradient Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Second Order optimization =&amp;gt; well researched&lt;/li&gt;
&lt;li&gt;Linear conjugate gradient minimizes a quadratic, which can be
  described by only matrix-vector products. Only linear time and memory are
necessary. &lt;/li&gt;
&lt;li&gt;CG is not necessary, truncated Newton is good enough.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.researchgate.net/publication/228619939_The_IBM_Attila_speech_recognition_toolkit"&gt;Hessian Free Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://research.microsoft.com/pubs/209355/NOW-Book-Revised-Feb2014-online.pdf"&gt;It has reference in this book,too.&lt;/a&gt; =&amp;gt; most probably, this may explain better.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Speeding Training&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester1201314/Sainath2013_lrdnn.pdf"&gt;Low-rank factorization of output weights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Word error rate generally drops as we increase the number of output
  targets =&amp;gt; factorization to reduce dimension?&lt;/li&gt;
&lt;li&gt;It gets faster.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Preconditioning in Sampling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/pdf/1309.1508v3.pdf"&gt;Accelerating Hessian Free Optimization Implicit Preconditioning and Sampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Geometric optimization reference is also in the above link.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Take-Home Mesages&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Structured loss function instead of cross-entropy&lt;/li&gt;
&lt;li&gt;Stochastic gradient on a GPU is faster but distributed Hessian free
  optimization produces better models.&lt;/li&gt;
&lt;li&gt;Low rank factorization of the output weights&lt;/li&gt;
&lt;li&gt;Preconditioning and sampling&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_Martens10.pdf"&gt;Hessian-Free Optimization&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Learning Guarantees of the Optimization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Convex surrogate did not work&lt;/li&gt;
&lt;li&gt;You could learn an interesting loss function.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.google.com/presentation/d/1fZjE2aNMEoXprlvju_CrJETPylKrWZ5GzBYCj-GFSQc/edit#slide=id.g40599a40022e2fc114"&gt;Related
  Slides&lt;/a&gt;,
I could not catch the presenter in the poster.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Large Scale Machine Learning(Accelerated)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Machine learning as an optimization problem.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://yaroslavvb.blogspot.com/2014/03/stochastic-gradient-methods-2014.html"&gt;Stochastic Gradient Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://mrtz.org/blog/the-zen-of-gradient-descent/"&gt;Gradient Descent Method Blog Post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Fast Scalable Comment Moderation on NYT&lt;/h3&gt;
&lt;h4&gt;Active Learning at the New York Times&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Comment is split into two: metadata and n-grams.&lt;/li&gt;
&lt;li&gt;Hash the n-grams, score of the comment for human editor if it will be
  shown to her. Human moderators will work on whatever the algorithm
scores on high.&lt;/li&gt;
&lt;li&gt;20% =&amp;gt; workload reduction according to the plan&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Role of Optimization in Machine Learning&lt;/h2&gt;
&lt;h3&gt;Key Observations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stochastic Gradient Descent in the origin&lt;/li&gt;
&lt;li&gt;Batch Gradient Descent in one direction (between semi-stochatic
  approaches)&lt;/li&gt;
&lt;li&gt;Stochastic Newton Method in the other direction (between second-order
  methods)&lt;/li&gt;
&lt;li&gt;Other fields in Newton method&lt;/li&gt;
&lt;li&gt;Coordinate descent is towards simpler methods&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Optimal rate is achieved also for testing cos as long as each data
point is seen only once.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;To learn More about Stochastic Gradient Descent&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.eecs.berkeley.edu/~brecht/cs294docs/week1/09.Nemirovski.pdf"&gt;Nice paper for stochastic gradient methods&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stochastic Quasi-Newton Method performs quite well.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sorry about other spotlight talks, I am sure they were as interesting as
the ones above but I was only able to take notes and follow this much. &lt;/p&gt;</summary></entry></feed>