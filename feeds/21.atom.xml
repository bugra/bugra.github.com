<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Bugra Akyildiz</title><link href="http://bugra.github.io/" rel="alternate"></link><link href="http://bugra.github.io/feeds/21.atom.xml" rel="self"></link><id>http://bugra.github.io/</id><updated>2015-02-21T00:00:00+00:00</updated><entry><title>Topic Modeling for the Uninitiated</title><link href="http://bugra.github.io/work/notes/2015-02-21/topic-modeling-for-the-uninitiated/" rel="alternate"></link><updated>2015-02-21T00:00:00+00:00</updated><author><name>Bugra Akyildiz</name></author><id>tag:bugra.github.io,2015-02-21:work/notes/2015-02-21/topic-modeling-for-the-uninitiated/</id><summary type="html">&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;As more and more data are stored digitally and people have better and improved tools for publishing, we are witnessing more and more text data has been collected and published in various mediums.(e-books, blogs, newspaper websites, magazines and mobile applications) So-called big data era not only enable people to collect more and more data through different forms but also provide a set of tools to analyze, infer various structures in the data and interpret that knowledge in various forms. &lt;/p&gt;
&lt;p&gt;Search became more and more important to find the needle in this haystack. 
Especially, if you need a very specific document, the only option you have is to depend on the search engine’s capabilities and hope for the best. &lt;/p&gt;
&lt;p&gt;Search is good, it brings the exact document that has the specific keyword and maybe the most relevant document based on your history. It may even go further and do a link analysis in a link network if it really want the bring best in the class. However, it comes to short when you want to read a collection of documents that are about “Iraq War” or it comes to short when you want to analyze the documents that are about “literature in Renaissance”. One of the reasons why they came short is because they build indices on top of documents not themes or topics. Iraq War or literature in Renaissance implicitly suggests a theme rather than keyword or phrase. &lt;/p&gt;
&lt;p&gt;In order to overcome this problem, topic models provide a nice way to explore a collection of documents that share a single common theme so-called “topic”.&lt;/p&gt;
&lt;h2&gt;Definition&lt;/h2&gt;
&lt;p&gt;Probabilistic topic models aka Topic Models are probabilistic generative models which uncovers hidden thematic structures in large collection of documents.&lt;/p&gt;
&lt;p&gt;Its statistical nature does not need any information other than the text itself. It does not use metadata, labels or annotations to build up the topics from the text. One needs to define only the number of clusters(this is quite hard, albeit). There are various different inference algorithms that enables fast inference in the corpus and some of the them also work in an online fashion so that one does not have to load the data into the memory. &lt;/p&gt;
&lt;p&gt;As most of the unsupervised learning algorithms, since there is no definitive number of clusters, the topics in the corpus are quite hard to evaluate. One could define various metrics either inter-topic(distance between different topics) or intra-topic(topic coherence). However, it generally depends on manual inspection and humans to decide the exact number of clusters in the dataset. While increasing the number of topics in the dataset may provide more granular information in the dataset, it could divide a very coherent topic into two parts that may be very close to each other. &lt;/p&gt;
&lt;p&gt;Although Latent Dirichlet Allocation is one of the topic models, I somehow interchangeably used to mean topic model and vice versa.&lt;/p&gt;
&lt;h3&gt;Word Distribution&lt;/h3&gt;
&lt;p&gt;Since topic models treat corpus and documents as bag of words, occurrences rather than position of words play an important role. The probability distribution of a particular word not only changes the topic assignment of the topic assignment the document that it belongs to but also may change the word distribution in the topic as well. &lt;/p&gt;
&lt;p&gt;One thing to be noted before applying Topic Modeling is that you need to care a lot about word distribution in both corpus and also in the topics. Not only the words themselves affect the probability but also they will affect the co-occurrence probability. One word probability both plays a role in assignment of a topic for that particular word but also on other words that they are likely to occur.&lt;/p&gt;
&lt;p&gt;Due to that reason, one needs to remove very common words as they do not contribute the topic assignment and shadow the other words probabilities in the topics which results in incoherent topics. Similarly, if the word occurrences is very small in the corpus, those probabilities have very small probabilities and would likely to have small effect on the word distribution and word co-occurrences. They may not affect the topics as much as very common words, yet they may increase the convergence time in the Gibbs sampling.  Due to that reason, one should also remove the very infrequent words in the corpus as well.&lt;/p&gt;
&lt;h3&gt;Learning Topics&lt;/h3&gt;
&lt;p&gt;Using Gibbs Sampling, one could learn the topics in the following way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We first assign a random topic to each word in the document&lt;/li&gt;
&lt;li&gt;For each word in the document&lt;ul&gt;
&lt;li&gt;Compute the topic likelihood given document (1)&lt;/li&gt;
&lt;li&gt;Compute the word likelihood given a topic (2)&lt;/li&gt;
&lt;li&gt;Reassign the topic to the word which is a multiplicative of (1) and (2)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Topic distribution is drawn from a Dirichlet Distribution so the Dirichlet in Latent Dirichlet Allocation. The hidden structure among words in topics is latent so the Latent in LDA. &lt;/p&gt;
&lt;h3&gt;Experimentation&lt;/h3&gt;
&lt;p&gt;I generally run the topic models for a number of different clusters and determine the optimal number based on manual inspection. Although it is not easily quantified information, since the topics is plausible to a human being, it is easy to interpret. Especially, if you are doing exploratory analysis. This is also a good way to get a feeling in the dataset &lt;/p&gt;
&lt;h3&gt;Assumptions&lt;/h3&gt;
&lt;h4&gt;First One&lt;/h4&gt;
&lt;p&gt;As most of statical models, topic models also do a lot of assumptions in order to build its inference on top of the observations. One of the assumptions that it makes, there is a structure on the words that compose a topic gives its power. This hidden structure that the inference of topic models uncover is quite intuitive to the humans. By looking at the word distributions, one could immediately grasp the topic. &lt;/p&gt;
&lt;h4&gt;Second One&lt;/h4&gt;
&lt;p&gt;Second assumption is that all of the documents is composed of multiple topics and different documents share the topics in a different proportions. This is very much like a mixture model where every observation has mixed membership among many classes(in topic modeling, they are topics). This membership could also be used document retrieval as we could compute the maximum likelihood of documents given a topic: $\max P(*|t_0)$. By doing so, one could build a topic search engine as the most relevant documents are brought for a particular topic search. If one want to go further, she could assign topic memberships on individual words as the topics are word distributions themselves. &lt;/p&gt;
&lt;h3&gt;LDA Limitations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It does not use metadata or any related  information about the documents.&lt;/li&gt;
&lt;li&gt;It does not allow correlation among topics and does not have a good way  to model the correlations.&lt;/li&gt;
&lt;li&gt;You cannot incorporate temporal information(date of the publication) to measure the change occurring in the topic distribution. &lt;/li&gt;
&lt;li&gt;It assumes a bag of words model on top of words and phrases. You cannot use the word order information in the text.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;LDA Extensions&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Nearly all of the limitations of LDA are addressed in different variants of LDA.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cs.columbia.edu/~blei/papers/BleiLafferty2006a.pdf"&gt;Dynamic Topic Models&lt;/a&gt; use temporal information to detect changes in the topic frequency over time.&lt;/li&gt;
&lt;li&gt;There is &lt;a href="https://www.cs.princeton.edu/~blei/papers/BleiLafferty2006.pdf"&gt;Correlated Topic Model&lt;/a&gt; which allows correlation between different topics. &lt;/li&gt;
&lt;li&gt;&lt;a href="http://mimno.infosci.cornell.edu/info6150/readings/398.pdf"&gt;Author-Topic Models&lt;/a&gt; allows you to incorporate various author related information into the topic models.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cs.nyu.edu/~dsontag/papers/AroraEtAl_icml13.pdf"&gt;Online Topic Models&lt;/a&gt; allow you to infer the topics in an online manner without having to load the text data into the memory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Tools&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nlp.stanford.edu/software/tmt/tmt-0.4/"&gt;Stanford Topic Modeling Toolbox&lt;/a&gt; is quite good and if you want to look at the end results in Excel, it has a bunch of nice helper functions to use.  You could programmatically use Scala library as well.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://mallet.cs.umass.edu/"&gt;Mallet&lt;/a&gt; is somehow outdated, yet it is mature and has a bunch of nice evaluation methods and nice default parameters. It could be also used via terminal without using its Java interface as well.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://factorie.cs.umass.edu/"&gt;Factorie&lt;/a&gt; is not for topic modeling per se, it is much more comprehensive but has a suite of algorithms and implementation that could be used for topic modeling as well.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cran.r-project.org/web/packages/lda/"&gt;lda&lt;/a&gt; is for people who like R and has a number of variants of LDA as well(correlated topic model is one of them).&lt;/li&gt;
&lt;li&gt;Also check out the &lt;a href="http://www.cs.princeton.edu/~blei/topicmodeling.html"&gt;topic modeling page of David Blei&lt;/a&gt;. He is the main author of highly cited &lt;a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf"&gt;LDA paper&lt;/a&gt; as well.&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Data Products Made Easy</title><link href="http://bugra.github.io/work/notes/2014-03-21/data-products-made-easy/" rel="alternate"></link><updated>2014-03-21T00:00:00+00:00</updated><author><name>Bugra Akyildiz</name></author><id>tag:bugra.github.io,2014-03-21:work/notes/2014-03-21/data-products-made-easy/</id><summary type="html">&lt;p&gt;I read &lt;a href="http://www.oreilly.com/data/free/data-jujitsu.csp"&gt;Data Jujitsu&lt;/a&gt;
recently and enjoyed it a lot. DJ Patil presents a nice set of hard
learned things that he experienced in (mainly) Linkedin. I like this
type of &lt;strong&gt;real&lt;/strong&gt; experiences rather than a set of rules that advocates
a too ideal world and behave according to that world.  &lt;/p&gt;
&lt;p&gt;A recurring theme of one the short book is that divide and attack the
problems you have. The "Jujitsu" term comes from Japanese martial art
and suggests doing small attacks and trying to use the opponent's strength
towards her because the opponent (problem) is much stronger than you.
This naming actually reflects quite well what the short book is about.
Developing solutions to hard problems with limited human power and time.
What I found surprising a little bit is that, how consumer-oriented he
is throughout the book. He looks at the problem solving approximately in the following: if problems are not what customers want(enhancement in the product)
or what customers are unhappy about(bugs), then there must be some other
incentive(increase in engagement, better sales and so on) must exist so
that that problem would become &lt;strong&gt;worthy&lt;/strong&gt; to solve. So, solving a
problem for the sake of solving is not beneficial, neither for the
customer nor for the business. &lt;/p&gt;
&lt;h2&gt;Critical Question&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Does anyone want or need your product?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This question is so critical that I cannot stress how important it is.
Generally, the startups or businesses in general do not fail because 
they could not solve a problem technically but either they cannot create
enough incentive for people to buy the product or simply it is
irrelevant to the market. If you cannot answer this question with
confidence, however hard problem that you solved will not matter.
Eventually, if the product will be used, then someone needs or wants
your product.&lt;/p&gt;
&lt;h3&gt;Minimum Viable Product&lt;/h3&gt;
&lt;p&gt;If you answered the question above, then you are eligible to build your
minimum viable product. This does not have to be your fully functional
product but it should be a simple product that needs to prove that there is a
need and it should be &lt;strong&gt;good enough&lt;/strong&gt; so that you could determine if you
want to improve it or not. When you want to improve, ask these two
questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;"Does the customer care? Is there a market fit? If there isn't, there
   is no sense in building an application."&lt;/li&gt;
&lt;li&gt;"How long do we have to learn the answer to Question A?"&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When you prototype and measure the engagement of a user in the step 1,
you have a better sense of what you should build. Further, this also
ensures that you have a validity check that you are not building
something nobody wants it.&lt;/p&gt;
&lt;h2&gt;Definition&lt;/h2&gt;
&lt;p&gt;Go back for a second, what is a "data product" to begin with? He gives a
concrete definitiion for this question: &lt;br /&gt;
"Data product is a product that facilitates an end goal through the use
of data."  &lt;/p&gt;
&lt;h3&gt;Design of Product&lt;/h3&gt;
&lt;p&gt;Considering how much development, funding have been given towards big
data technologies or data analysis tools in general, data products would
have been easy, right? Not really.  &lt;/p&gt;
&lt;p&gt;Data is messy especially if you are getting data from a variety of
sources which do not have common interfaces. Data is messy if you are
collecting from input text fields that customers fill in. How do you
make sure that the data is in the right form? With product design. You
would provide feedback as Google search does ahead of the user, you
would prompt "did you mean ..." to help the user, you will arrange your
dropdown menu based on the input from the customer. This not only
provides much better experience for the user but also you get a much
better, structured data(think about dropdowns, support type-ahead for a
second) in your back-end. Patil presents this fact as: 
"I've found that trying to solve a problem on the back-end is
100-1000 times more expensive than on the front end".&lt;/p&gt;
&lt;h4&gt;Use humans when you have to, use technical solutions when you could&lt;/h4&gt;
&lt;p&gt;Generally, engineering seeks for technical solutions which are scalable.
This ensures that the solution will be profitable for high number of
users. However, when you try to be relevant in the market or try to see
if there is a market, you need to use humans. There is a similar problem 
&lt;a href="http://en.wikipedia.org/wiki/Cold_start"&gt;cold start&lt;/a&gt; which corresponds a
significant problem in the recommender system. Think about a user who
just signed your ecommerce startup, you want to recommmend things to him
but you do not have any history. Not only that, but you just launched
your product so you do not have any prior knowledge what people (in
general) like. If you had beta users or mechanical turks before you
launched your product, you are in luck. If the product is
consumer-facing, you should at least some data about your users from to
have a head start. Technical solutions are and will be more efficient,
cheaper and scale better in the long run, but if you cannot afford the
time and effort to build a technical solution to the problem, then do
not hesitate humans.&lt;/p&gt;
&lt;h4&gt;Always be opportunistic&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;If you are able to do data analysis to make the product better,
  increase the sales, just do it!&lt;/li&gt;
&lt;li&gt;If you cannot do some operation because you do not have resources or
  technical expertise, try do divide the operation and try to offer
simple version of it instead of providing nothing!&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Give the data back!&lt;/h4&gt;
&lt;p&gt;If your product is data-centric, you must be creating some value around
it and you should already providing the data to the customer in some
way. You should give it more! To both increase engagement and revenue,
give the data in an undertandable, clean and maybe even interactive way.
Let users play with the data. If your data is timely and
actioanable(think twitter for a sec), then it becomes addictive. Instead
of hoarding, share it. Only through sharing and giving back, you could
create more value around it.&lt;/p&gt;
&lt;h4&gt;But do not give it too much!&lt;/h4&gt;
&lt;p&gt;What Patil calls "Data Vomit" is that if you give too much data without
considering if it makes sense or valuable to the customer, there is a
good chance to overwhelm the user. So, confusion and frustration
replaces the engagement from the day one. There is a sweet spot where
more data generates more engagement and after that sweet spot, more data
will cause less interaction and engagement.&lt;/p&gt;
&lt;h4&gt;Consider non-ideal cases&lt;/h4&gt;
&lt;p&gt;If you are building a product, think about the extreme and edge cases as
well. Showing spanish pages to a tourist visiting Mexica just because
she is in Mexica may not make sense and especially if she repeatedly changes the browser language from spanish to english!&lt;/p&gt;
&lt;h3&gt;Precision and Recall&lt;/h3&gt;
&lt;p&gt;If you are building a retrieval system, first learn these concepts.
Then, find out what you want to compromise as these two generally work
against to each other. For a search engine, precision might be the
single most important metric whereas if you claim to be one of the most
comprehensive news source, you need to also increase your recall to be
consistent with your claim. Rule of thumb is that if the data is
exposed, first try to have a high precision because first page may be
the only page that your customer sees. If the precision is not good, it
may well be the last page.&lt;/p&gt;
&lt;h3&gt;Social system for the win&lt;/h3&gt;
&lt;p&gt;If your recommender system is terrible giving recommendations and
customers are unhappy about it, use collaborative filtering first and
blame the preferences of other users if customer is still not happy
about the recommendation. It is something that customer blames the product for
terrible recommendations, and it is completely different thing that
people are buying two incoherent things, so I am recommended that
product.&lt;/p&gt;
&lt;h3&gt;Get more data&lt;/h3&gt;
&lt;p&gt;Even if your domain is not advertising, knowing more about your users
always pay off. As you know more about her, you could recommend better,
you could personalize better, you could sell better, you could serve
better. Asking data if it is done correctly, it could be another way to
engage the customer as well. After you get the data, only goal would be
better product. Do not abuse it!&lt;/p&gt;
&lt;h3&gt;User is the most important&lt;/h3&gt;
&lt;p&gt;Features fail, products fail, nearly everyting in the universe at some
point fails. Get used to it, but try to preserve as much as user
experience you can in the process. Data products generally empower the
user in some way and there is a high chance that the experience will not
be constant through her time. Try not to decrease it too much. If the
ads that you show may be offensive, give an option to user so that when
she visits the website, she could just remove those type of ads(similar
to Facebook). If the people you recommend are not very relevant, provide
a way to user so that he could give negative feedback to people whom she
does not know so that she will not see them again(similar to Linkedin).
This not only gives control and value to user, in the process you could
learn user preferences and build a better product.&lt;/p&gt;
&lt;h3&gt;Three Fundamental Questions that you should ask to yourself&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;What do you want the user to take away from this product?&lt;/li&gt;
&lt;li&gt;What action do you want the user to take because of the product?&lt;/li&gt;
&lt;li&gt;How should the user feel during and after using your product?&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;If the core of your product gets it right the core and fundamentals
from the day one, you have a lot of time to improve it. Use Jujitsu! &lt;/p&gt;
&lt;/blockquote&gt;</summary></entry></feed>